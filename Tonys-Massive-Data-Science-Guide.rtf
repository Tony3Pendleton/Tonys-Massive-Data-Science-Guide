{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf500
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 LucidaGrande;\f2\fswiss\fcharset0 Helvetica-Oblique;
\f3\froman\fcharset0 Times-Roman;\f4\fswiss\fcharset0 Helvetica-Bold;\f5\fnil\fcharset0 STIXGeneral-Regular;
}
{\colortbl;\red255\green255\blue255;\red252\green87\blue8;\red0\green0\blue233;\red254\green255\blue10;
\red255\green255\blue255;\red35\green255\blue6;\red0\green0\blue0;\red0\green0\blue0;\red255\green255\blue11;
\red255\green255\blue255;\red255\green255\blue255;\red253\green139\blue9;\red251\green0\blue7;\red255\green255\blue255;
\red18\green0\blue233;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c100000\c43066\c0;\cssrgb\c0\c0\c93333;\cssrgb\c99555\c99475\c0;
\cssrgb\c100000\c100000\c100000;\cssrgb\c0\c100000\c0;\cssrgb\c0\c1\c1;\cssrgb\c0\c0\c0;\cssrgb\c100000\c100000\c0;
\cssrgb\c100000\c100000\c99985;\cssrgb\c100000\c100000\c99971;\cssrgb\c100000\c61456\c0;\cssrgb\c100000\c12195\c0;\cssrgb\c100000\c100000\c99985\c0;
\cssrgb\c10160\c10292\c93230;\csgray\c0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid104\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid105\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid106\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid304\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid305\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid306\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid307\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid403\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid404\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid405\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid406\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid407\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid408\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5760\lin5760 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid409\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li6480\lin6480 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid410\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li7200\lin7200 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid411\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li7920\lin7920 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid503\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid601\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid603\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid604\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid605\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid606\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid704\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid705\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid706\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid707\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid9}
{\list\listtemplateid10\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid10}
{\list\listtemplateid11\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid11}
{\list\listtemplateid12\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1104\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1105\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid12}
{\list\listtemplateid13\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid13}
{\list\listtemplateid14\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1304\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1305\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid14}
{\list\listtemplateid15\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1403\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1404\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid15}
{\list\listtemplateid16\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1503\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid16}
{\list\listtemplateid17\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1603\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid17}
{\list\listtemplateid18\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid18}
{\list\listtemplateid19\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid19}
{\list\listtemplateid20\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid20}
{\list\listtemplateid21\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid2001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2003\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2004\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2005\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2006\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listname ;}\listid21}
{\list\listtemplateid22\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid2101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid2102\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2104\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2105\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2106\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2107\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid22}
{\list\listtemplateid23\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid2201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid2202\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2203\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2204\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2205\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid23}
{\list\listtemplateid24\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid2301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2304\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid24}
{\list\listtemplateid25\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid2401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2403\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2404\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid25}
{\list\listtemplateid26\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid2501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2503\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2504\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid26}
{\list\listtemplateid27\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid2601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2603\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid27}
{\list\listtemplateid28\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid2701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2704\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid28}
{\list\listtemplateid29\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid2801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2803\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2804\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2805\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2806\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listname ;}\listid29}
{\list\listtemplateid30\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2901\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2903\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2904\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid30}
{\list\listtemplateid31\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid3001\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid31}
{\list\listtemplateid32\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid3101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid32}
{\list\listtemplateid33\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid3201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3203\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3204\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3205\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3206\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listname ;}\listid33}
{\list\listtemplateid34\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid3301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3304\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3305\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid34}
{\list\listtemplateid35\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid3401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid35}
{\list\listtemplateid36\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3501\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid36}
{\list\listtemplateid37\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3601\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3603\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3604\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid37}
{\list\listtemplateid38\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid3701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid38}
{\list\listtemplateid39\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid3801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3803\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3804\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid39}
{\list\listtemplateid40\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid3901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid40}
{\list\listtemplateid41\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid4001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid41}
{\list\listtemplateid42\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid4101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4104\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid42}
{\list\listtemplateid43\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4201\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4203\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid43}
{\list\listtemplateid44\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid4301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4304\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid44}
{\list\listtemplateid45\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid4401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid45}
{\list\listtemplateid46\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid4501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid46}
{\list\listtemplateid47\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid4601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid47}
{\list\listtemplateid48\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid4701\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid48}
{\list\listtemplateid49\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid4801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid49}
{\list\listtemplateid50\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid4901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid50}
{\list\listtemplateid51\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid5001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid51}
{\list\listtemplateid52\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid5101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid52}
{\list\listtemplateid53\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5201\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid53}
{\list\listtemplateid54\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid5301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid54}
{\list\listtemplateid55\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid5401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid5402\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5403\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid55}
{\list\listtemplateid56\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5501\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5503\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5504\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid56}
{\list\listtemplateid57\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid5601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid57}
{\list\listtemplateid58\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid5701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5704\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid58}
{\list\listtemplateid59\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid5801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5803\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5804\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid59}
{\list\listtemplateid60\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid5901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5903\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid60}
{\list\listtemplateid61\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6003\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6004\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6005\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid61}
{\list\listtemplateid62\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid62}
{\list\listtemplateid63\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6203\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid63}
{\list\listtemplateid64\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid64}
{\list\listtemplateid65\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6403\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid65}
{\list\listtemplateid66\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid66}
{\list\listtemplateid67\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid67}
{\list\listtemplateid68\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid68}
{\list\listtemplateid69\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid69}
{\list\listtemplateid70\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6903\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid70}
{\list\listtemplateid71\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7003\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7004\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid71}
{\list\listtemplateid72\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7104\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7105\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7106\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7107\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7108\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5760\lin5760 }{\listname ;}\listid72}
{\list\listtemplateid73\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7203\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7204\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid73}
{\list\listtemplateid74\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid74}
{\list\listtemplateid75\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid75}
{\list\listtemplateid76\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7503\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7504\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7505\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid76}
{\list\listtemplateid77\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7603\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid77}
{\list\listtemplateid78\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid78}
{\list\listtemplateid79\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid79}
{\list\listtemplateid80\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7903\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid80}
{\list\listtemplateid81\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid8001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid81}
{\list\listtemplateid82\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid8101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid82}
{\list\listtemplateid83\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid8201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid83}
{\list\listtemplateid84\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid8301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8304\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid84}
{\list\listtemplateid85\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid8401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid85}
{\list\listtemplateid86\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid8501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid86}
{\list\listtemplateid87\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid8601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid87}
{\list\listtemplateid88\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid8701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8704\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8705\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid88}
{\list\listtemplateid89\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid8801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8803\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8804\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8805\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid89}
{\list\listtemplateid90\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid8901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8903\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8904\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8905\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8906\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8907\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid90}
{\list\listtemplateid91\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid9001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9003\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9004\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid91}
{\list\listtemplateid92\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid9101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9104\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid92}
{\list\listtemplateid93\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid9201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9203\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9204\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid93}
{\list\listtemplateid94\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid9301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9304\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid94}
{\list\listtemplateid95\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid9401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9403\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9404\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid95}
{\list\listtemplateid96\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid9501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9503\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9504\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid96}
{\list\listtemplateid97\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid9601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9603\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9604\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid97}
{\list\listtemplateid98\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid9701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9704\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid98}
{\list\listtemplateid99\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid9801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9803\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid99}
{\list\listtemplateid100\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid9901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid9903\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid100}
{\list\listtemplateid101\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid10001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10003\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10004\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10005\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid101}
{\list\listtemplateid102\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid10101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10104\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10105\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10106\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10107\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid102}
{\list\listtemplateid103\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid10201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10203\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10204\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10205\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10206\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10207\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid103}
{\list\listtemplateid104\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid10301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10304\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10305\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10306\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10307\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid104}
{\list\listtemplateid105\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid10401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10403\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10404\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10405\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10406\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10407\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid105}
{\list\listtemplateid106\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid10501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10503\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10504\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10505\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10506\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10507\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid106}
{\list\listtemplateid107\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid10601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10603\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10604\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10605\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10606\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10607\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid107}
{\list\listtemplateid108\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid10701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10704\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10705\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10706\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10707\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid108}
{\list\listtemplateid109\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid10801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10803\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10804\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10805\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid109}
{\list\listtemplateid110\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid10901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10903\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10904\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid10905\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid110}
{\list\listtemplateid111\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid11001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11003\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11004\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11005\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid111}
{\list\listtemplateid112\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid11101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11104\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11105\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid112}
{\list\listtemplateid113\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid11201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11203\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11204\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11205\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid113}
{\list\listtemplateid114\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid11301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11304\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11305\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid114}
{\list\listtemplateid115\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid11401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11403\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11404\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11405\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid115}
{\list\listtemplateid116\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid11501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11503\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11504\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11505\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid116}
{\list\listtemplateid117\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid11601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11603\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11604\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11605\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid117}
{\list\listtemplateid118\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid11701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11704\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11705\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid118}
{\list\listtemplateid119\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid11801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11803\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11804\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11805\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid119}
{\list\listtemplateid120\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid11901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11903\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11904\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid11905\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid120}
{\list\listtemplateid121\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid12001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12003\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12004\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12005\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12006\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12007\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12008\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5760\lin5760 }{\listname ;}\listid121}
{\list\listtemplateid122\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid12101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12104\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid122}
{\list\listtemplateid123\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid12201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12203\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12204\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12205\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12206\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12207\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid123}
{\list\listtemplateid124\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid12301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12304\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid124}
{\list\listtemplateid125\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid12401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12403\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid125}
{\list\listtemplateid126\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid12501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12503\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12504\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12505\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12506\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12507\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12508\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5760\lin5760 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12509\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li6480\lin6480 }{\listname ;}\listid126}
{\list\listtemplateid127\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid12601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12603\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid127}
{\list\listtemplateid128\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid12701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid128}
{\list\listtemplateid129\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid12801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12803\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid12804\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid129}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}{\listoverride\listid10\listoverridecount0\ls10}{\listoverride\listid11\listoverridecount0\ls11}{\listoverride\listid12\listoverridecount0\ls12}{\listoverride\listid13\listoverridecount0\ls13}{\listoverride\listid14\listoverridecount0\ls14}{\listoverride\listid15\listoverridecount0\ls15}{\listoverride\listid16\listoverridecount0\ls16}{\listoverride\listid17\listoverridecount0\ls17}{\listoverride\listid18\listoverridecount0\ls18}{\listoverride\listid19\listoverridecount0\ls19}{\listoverride\listid20\listoverridecount0\ls20}{\listoverride\listid21\listoverridecount0\ls21}{\listoverride\listid22\listoverridecount0\ls22}{\listoverride\listid23\listoverridecount0\ls23}{\listoverride\listid24\listoverridecount0\ls24}{\listoverride\listid25\listoverridecount0\ls25}{\listoverride\listid26\listoverridecount0\ls26}{\listoverride\listid27\listoverridecount0\ls27}{\listoverride\listid28\listoverridecount0\ls28}{\listoverride\listid29\listoverridecount0\ls29}{\listoverride\listid30\listoverridecount0\ls30}{\listoverride\listid31\listoverridecount0\ls31}{\listoverride\listid32\listoverridecount0\ls32}{\listoverride\listid33\listoverridecount0\ls33}{\listoverride\listid34\listoverridecount0\ls34}{\listoverride\listid35\listoverridecount0\ls35}{\listoverride\listid36\listoverridecount0\ls36}{\listoverride\listid37\listoverridecount0\ls37}{\listoverride\listid38\listoverridecount0\ls38}{\listoverride\listid39\listoverridecount0\ls39}{\listoverride\listid40\listoverridecount0\ls40}{\listoverride\listid41\listoverridecount0\ls41}{\listoverride\listid42\listoverridecount0\ls42}{\listoverride\listid43\listoverridecount0\ls43}{\listoverride\listid44\listoverridecount0\ls44}{\listoverride\listid45\listoverridecount0\ls45}{\listoverride\listid46\listoverridecount0\ls46}{\listoverride\listid47\listoverridecount0\ls47}{\listoverride\listid48\listoverridecount0\ls48}{\listoverride\listid49\listoverridecount0\ls49}{\listoverride\listid50\listoverridecount0\ls50}{\listoverride\listid51\listoverridecount0\ls51}{\listoverride\listid52\listoverridecount0\ls52}{\listoverride\listid53\listoverridecount0\ls53}{\listoverride\listid54\listoverridecount0\ls54}{\listoverride\listid55\listoverridecount0\ls55}{\listoverride\listid56\listoverridecount0\ls56}{\listoverride\listid57\listoverridecount0\ls57}{\listoverride\listid58\listoverridecount0\ls58}{\listoverride\listid59\listoverridecount0\ls59}{\listoverride\listid60\listoverridecount0\ls60}{\listoverride\listid61\listoverridecount0\ls61}{\listoverride\listid62\listoverridecount0\ls62}{\listoverride\listid63\listoverridecount0\ls63}{\listoverride\listid64\listoverridecount0\ls64}{\listoverride\listid65\listoverridecount0\ls65}{\listoverride\listid66\listoverridecount0\ls66}{\listoverride\listid67\listoverridecount0\ls67}{\listoverride\listid68\listoverridecount0\ls68}{\listoverride\listid69\listoverridecount0\ls69}{\listoverride\listid70\listoverridecount0\ls70}{\listoverride\listid71\listoverridecount0\ls71}{\listoverride\listid72\listoverridecount0\ls72}{\listoverride\listid73\listoverridecount0\ls73}{\listoverride\listid74\listoverridecount0\ls74}{\listoverride\listid75\listoverridecount0\ls75}{\listoverride\listid76\listoverridecount0\ls76}{\listoverride\listid77\listoverridecount0\ls77}{\listoverride\listid78\listoverridecount0\ls78}{\listoverride\listid79\listoverridecount0\ls79}{\listoverride\listid80\listoverridecount0\ls80}{\listoverride\listid81\listoverridecount0\ls81}{\listoverride\listid82\listoverridecount0\ls82}{\listoverride\listid83\listoverridecount0\ls83}{\listoverride\listid84\listoverridecount0\ls84}{\listoverride\listid85\listoverridecount0\ls85}{\listoverride\listid86\listoverridecount0\ls86}{\listoverride\listid87\listoverridecount0\ls87}{\listoverride\listid88\listoverridecount0\ls88}{\listoverride\listid89\listoverridecount0\ls89}{\listoverride\listid90\listoverridecount0\ls90}{\listoverride\listid91\listoverridecount0\ls91}{\listoverride\listid92\listoverridecount0\ls92}{\listoverride\listid93\listoverridecount0\ls93}{\listoverride\listid94\listoverridecount0\ls94}{\listoverride\listid95\listoverridecount0\ls95}{\listoverride\listid96\listoverridecount0\ls96}{\listoverride\listid97\listoverridecount0\ls97}{\listoverride\listid98\listoverridecount0\ls98}{\listoverride\listid99\listoverridecount0\ls99}{\listoverride\listid100\listoverridecount0\ls100}{\listoverride\listid101\listoverridecount0\ls101}{\listoverride\listid102\listoverridecount0\ls102}{\listoverride\listid103\listoverridecount0\ls103}{\listoverride\listid104\listoverridecount0\ls104}{\listoverride\listid105\listoverridecount0\ls105}{\listoverride\listid106\listoverridecount0\ls106}{\listoverride\listid107\listoverridecount0\ls107}{\listoverride\listid108\listoverridecount0\ls108}{\listoverride\listid109\listoverridecount0\ls109}{\listoverride\listid110\listoverridecount0\ls110}{\listoverride\listid111\listoverridecount0\ls111}{\listoverride\listid112\listoverridecount0\ls112}{\listoverride\listid113\listoverridecount0\ls113}{\listoverride\listid114\listoverridecount0\ls114}{\listoverride\listid115\listoverridecount0\ls115}{\listoverride\listid116\listoverridecount0\ls116}{\listoverride\listid117\listoverridecount0\ls117}{\listoverride\listid118\listoverridecount0\ls118}{\listoverride\listid119\listoverridecount0\ls119}{\listoverride\listid120\listoverridecount0\ls120}{\listoverride\listid121\listoverridecount0\ls121}{\listoverride\listid122\listoverridecount0\ls122}{\listoverride\listid123\listoverridecount0\ls123}{\listoverride\listid124\listoverridecount0\ls124}{\listoverride\listid125\listoverridecount0\ls125}{\listoverride\listid126\listoverridecount0\ls126}{\listoverride\listid127\listoverridecount0\ls127}{\listoverride\listid128\listoverridecount0\ls128}{\listoverride\listid129\listoverridecount0\ls129}}
\margl1440\margr1440\vieww12300\viewh14280\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \'97\'97\'97\'97\'97\
READ ME\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls1\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}How to use this guide\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls1\ilvl1\cf0 {\listtext	\uc0\u8259 	}Currently the best way to search through this guide is to press CMD+F (if on Mac) or Ctrl+F (if on Windows/Linux) and to search for the following keywords\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls1\ilvl2\cf0 {\listtext	\uc0\u8259 	}general snippets, data wrangling snippets, multiindex, level, .size, .value_counts, .groupby, examples, python, plt, sns, statistics, terminology alert, sparsity, ETL, read_csv, get_dummies, probability, keras, tflearn, impute, onehotencod, dummy v, PCA, Hive,  CNN, ensemble, validation, memory, usage, Hadoop, MAE, RSME, confusion, grid, function, matplotlib, plot, seaborn, facet, violin, histogram, bar, box, anomaly, count, label, title, libraries, sklearn. , keras. , tensor, from import, subplotting, regression, regressor, linear, logistic, SVM, clustering, classification, KNN,  k-m, boosting, xgb, decision tree, random forest, bagging, deep, learning, convolution, dense, bias, variance, skew, heat, drop, probability, pandas, preprocessing, skew, correction, subsetting, dataframe, for i, apply, column, row, data engineering,  concatenate, aggregate, terminology, important, feature, index, unique, value, naive bay, scaling, AB, A/B, predict, classifier, model, X_train, X, y_val, y_test, error, deviation, accurate, metric, melt, loc, iloc, .column, NaN, missing, lambda, np.expm1, np.log1p, split, CART, gradient, Series, Index, list, array, convert, lightgbm, xgboost, catboost, reinforcement, rule, selection, gaussian, numba, timeit, dendogram, t test, z test, f test, matched pair test, anova, edge, bootstrap, ANOVA, F1 score, AWS, standard error, standard dev, degrees of freedom, SSIS, PostgreSQL, MySQL, Visual Studio, Management Studio\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\'97\'97\'97\'97\'97\
STATISTICS \
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls2\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}\cf2 Terminology Alert\cf0 \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls2\ilvl1\cf0 {\listtext	\uc0\u8259 	}X_bar is commonly used for 
\f2\i sample
\f0\i0  mean, whereas \uc0\u956  is commonly used to represent the true population mean\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls2\ilvl2\cf0 {\listtext	\uc0\u8259 	}Variance and Std Deviation | Why divide by n-1? by zedstatistics {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=wpY9o_OyxoQ"}}{\fldrslt https://www.youtube.com/watch?v=wpY9o_OyxoQ}} (great video)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls2\ilvl3\cf0 {\listtext	\uc0\u8259 	}Note, to calculate variance if we have an entire population and no average\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls2\ilvl4\cf0 {\listtext	\uc0\u8259 	}Population Variance\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls2\ilvl5\cf0 {\listtext	\uc0\u8259 	}\uc0\u963 ^2 = summation( (X - \u956 )^2 ) / (N)          # note population mean \u956  = summation(X)/n\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls2\ilvl3\cf0 {\listtext	\uc0\u8259 	}Whereas, if we DO NOT have the population mean \uc0\u956 , we have to rely on using X_bar (a sample mean, typically given in problem statement) and use \
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls2\ilvl4\cf0 {\listtext	\uc0\u8259 	}Sample Variance\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls2\ilvl5\cf0 {\listtext	\uc0\u8259 	}s^2 = summation( (X - X_bar)^2 ) / (n - 1)\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls3\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Probability\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls4\ilvl1\cf0 {\listtext	\uc0\u8259 	}Probability of rolling three dice without getting a six\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls4\ilvl2\cf0 {\listtext	\uc0\u8259 	}To obtain the probability that at least one 6 is rolled in the three tosses\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls4\ilvl3\cf0 {\listtext	\uc0\u8259 	}1- (5/6)^3   Note, the probability of rolling a 6 with one dice on one roll is 1/6.\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls4\ilvl1\cf0 {\listtext	\uc0\u8259 	}Probability of rolling double sixes twice in a roll (rolling two dices at a time, two times)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls4\ilvl2\cf0 {\listtext	\uc0\u8259 	}Since each dice is being rolled independently\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls4\ilvl3\cf0 {\listtext	\uc0\u8259 	}(1/6)^1 * (1/6)^1 * 1/6)^1 * (1/6)^1 = (1/6)^4 = 0.00077\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls4\ilvl1\cf0 {\listtext	\uc0\u8259 	}Probability of rolling one dice twice and getting a six both times\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls4\ilvl3\cf0 {\listtext	\uc0\u8259 	}(1/6)^1 * (1/6)^1 = (1/6)^2 = 1/36\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls4\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://sciencing.com/calculate-dice-probabilities-5858157.html"}}{\fldrslt https://sciencing.com/calculate-dice-probabilities-5858157.html}}\
{\listtext	\uc0\u8259 	}Probability Distribution {\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Probability_distribution"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \ulc3 https://en.wikipedia.org/wiki/Probability_distribution }}(great read)
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \ulc3 \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls4\ilvl2
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}Probability distributions are generally divided into two classes. A \cf4 \cb5 discrete probability distribution\cf0 \cb1  (applicable to the scenarios where the set of possible outcomes is discrete, such as a coin toss or a roll of dice) can be encoded by a discrete list of the probabilities of the outcomes, known as a \cf4 \cb5 probability mass function\cf0 \cb1 ({\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Probability_mass_function"}}{\fldrslt https://en.wikipedia.org/wiki/Probability_mass_function}}). On the other hand, a \cf4 \cb5 continuous probability distribution\cf0 \cb1  (applicable to the scenarios where the set of possible outcomes can take on values in a continuous range (e.g. real numbers), such as the temperature on a given day) is typically described by \cf4 \cb5 probability density functions\cf0 \cb1  ({\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Probability_density_function"}}{\fldrslt https://en.wikipedia.org/wiki/Probability_density_function}}, with the probability of any individual outcome actually being 0)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls4\ilvl1\cf0 {\listtext	\uc0\u8259 	}The Central Limit Theorem\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls4\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Central_limit_theorem"}}{\fldrslt https://en.wikipedia.org/wiki/Central_limit_theorem}}\
{\listtext	\uc0\u8259 	}The Central Limit Theorem by StatQuest with Josh Starmer {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=YAlJCEDH2uY&t=35s"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=YAlJCEDH2uY&t=35s}}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls4\ilvl3\cf0 {\listtext	\uc0\u8259 	}Note, for this StatQuest, you should be familiar with The Normal Distribution. If you aren\'92t please see \'93StatQuest: The Normal Distribution, Clearly Explained!!!\'94 by StatQuest with Josh Starmer {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=rzFX5NWojp0"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=rzFX5NWojp0 }}.\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls4\ilvl4\cf0 {\listtext	\uc0\u8259 	}aka bell shaped curve, x-axis is usually some sort of measurement (eg, height), and y-axis represents the relative probability (less likely to more likely)\
{\listtext	\uc0\u8259 	}Normal distributions are always centered on the average value\
{\listtext	\uc0\u8259 	}Normal distributions are kind of \'93magical\'94 / a phenomenon because we wee it a lot in nature. But there\'92s a reason for that, and that reason makes it super useful for statistics as well. It\'92s called The Central Limit Theorem.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls4\ilvl3\cf0 {\listtext	\uc0\u8259 	}Note, for this StatQuest, you should also be familiar with Sampling a Distribution. If you aren\'92t please see \'93StatQuest: Sampling a Distribution\'94 by StatQuest with Josh Starmer {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=XLCWeSVzHUU"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=XLCWeSVzHUU}}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=rzFX5NWojp0"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul  }}. Great short video.\
{\listtext	\uc0\u8259 	}The Central Limit Theorem is the basis for a lot of statistics and the good news is that it is a pretty simple concept.\
{\listtext	\uc0\u8259 	}Here is why it is important. Like most things in statistics, it is best explained by looking at some examples:\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls4\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1:\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls4\ilvl5\cf0 {\listtext	\uc0\u8259 	}If we take a bunch of samples from a uniform distribution and get the mean of each sample. We will get a bunch of means that are normally distributed.\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls4\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 2:\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls4\ilvl5\cf0 {\listtext	\uc0\u8259 	}If we take a bunch of samples from an exponential distribution (~exponential decay) and get  the mean of each sample. We will get a bunch of means that are normally distributed.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls4\ilvl3\cf0 {\listtext	\uc0\u8259 	}In summary, the Central Limit Theorem states that samples from a population will result in means that are normally distributed.\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls4\ilvl4\cf0 {\listtext	\uc0\u8259 	}But what are the practical implications of knowing that means are normally distributed?\
{\listtext	\uc0\u8259 	}When we do an experiment, we don\'92t always know what distribution our data comes from. To this, The Central Limit Theorem say\'92s \'93Who cares??\'94\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls4\ilvl5\cf0 {\listtext	\uc0\u8259 	}Because the sample means will be normally distributed\
{\listtext	\uc0\u8259 	}Since we know that the sample means will be normally distributed, we don\'92t need to worry too much about the distribution that the samples came from\
{\listtext	\uc0\u8259 	}We can use the mean\'92s normal distribution to make confidence intervals\'85..do t-tests, where we ask if there is a difference between the means from two samples\'85.and ANOVA, where we ask if there is a difference among the means from three or more samples\'85and pretty much any statistical test that uses the sample mean.\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls4\ilvl6\cf0 {\listtext	\uc0\u8259 	}\'93Note: Out there in the wild some folks say that in order for the Central Limit Theorem to be true, the sample size must be at least 30. This is just a rule of thumb and generally considered safe. However, as we can see here where I use a sample size of 20, the rule was meant to be broken\'94 - Josh\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls4\ilvl5\cf0 {\listtext	\uc0\u8259 	}There is one assumption the Central Value Theorem makes.\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls4\ilvl6\cf0 {\listtext	\uc0\u8259 	}We must be able to calculate a mean from our sample\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls5\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Statistics\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls5\ilvl1\cf0 {\listtext	\uc0\u8259 	}Statistics Fundamentals: Population Parameters by StatQuest ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=vikkiwjQqfU&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=4"}}{\fldrslt https://www.youtube.com/watch?v=vikkiwjQqfU&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=4}})\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls5\ilvl2\cf0 {\listtext	\uc0\u8259 	}Since we rarely, if ever, have enough time to measure every single thing in a population distribution\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}for example, lets imagine that instead of having a population distribution created with 240 billion cells of Gene X, we only have a population distribution of 5 cells\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}We will use these 5 measurements to estimate the population parameters. The reason why we want the population parameters is to ensure results drawn from our experiment are reproducible.\
{\listtext	\uc0\u8259 	}In other words, if someone else measures the gene in 5 different liver cells, then they will get 5 different measurements. However, the new measurements will come from the same population.\
{\listtext	\uc0\u8259 	}Insights can then be derived from the population. One example insight might be, what is the probability of observing more than 30 mRNA transcripts in a single cell? We will apply this question to both of the two experiments and future experiments.\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Note, on the screen, Josh is showing is showing a normal distribution with the number of mRNA transcripts from Gene X in 5 different liver cells\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Blue line (Gene X) ranging from 0-40 mRNA with 5 green dots representing our samples\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}This green curve represents the population of our data (eg, \ul one csv with data we are told to model\ulnone )\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}Instead of describing the first 5 measurements we made, we want to actually 
\f2\i estimate 
\f0\i0 the population parameters (eg, population mean, population std. deviation)  and use them as the basis for the results\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Note, in the onscreen example, Josh is showing the 
\f2\i true
\f0\i0  population mean is 20 and the 
\f2\i true
\f0\i0  population standard deviation is 10. Again, these parameters are the 
\f2\i true
\f0\i0  parameters of the dataset. Think of this as the data that we have been given to predict/model.\
{\listtext	\uc0\u8259 	}We see from the first 5 measurements we did the 
\f2\i estimated
\f0\i0  population mean is 17.6 and the 
\f2\i estimated
\f0\i0  population std deviation is 10.1\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}\cf2 Terminology alert:\cf0  This is the training set \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Now when we repeat the experiment the 
\f2\i estimated 
\f0\i0 population mean is 19.2 and the 
\f2\i estimated 
\f0\i0 standard deviation is 12.7\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}\cf2 Terminology alert:\cf0  This is the testing set \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Thus, each time we do the experiment we get different estimates of the population parameters.\
{\listtext	\uc0\u8259 	}And both sets of 
\f2\i estimates
\f0\i0  are different \
{\listtext	\uc0\u8259 	}Let\'92s say we only had 2 measurements in the training set instead of 5.\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The 
\f2\i estimated
\f0\i0  population mean is 11 and the 
\f2\i estimated
\f0\i0  population std deviation is 11.3\
{\listtext	\uc0\u8259 	}Note, the 
\f2\i true
\f0\i0  population mean is 20 and the 
\f2\i true
\f0\i0  population standard deviation is 10\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Let\'92s say we only had 3 measurements in the training set instead of 5.\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The 
\f2\i estimated
\f0\i0  population mean is 15.3 and the 
\f2\i estimated
\f0\i0  population std deviation is 11\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Let\'92s say we only had 10 measurements in the training set instead of 5.\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The 
\f2\i estimated
\f0\i0  population mean is 19 and the 
\f2\i estimated
\f0\i0  population std deviation is 10.5\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Therefore, this means that the more data we have, the more 
\f2\i confidence
\f0\i0  we can have in the accuracy of the estimates.\
{\listtext	\uc0\u8259 	}This is why it\'92s important to choose the appropriate \'93test_size\'94 for when we split our data for machine learning algorithms\
{\listtext	\uc0\u8259 	}One of the main goals in statistics is quantifying how much confidence we can have in population estimates.\
{\listtext	\uc0\u8259 	}Specifically, statisticians often calculate \cf6 p-values\cf0  and \cf6 confidence intervals\cf0  to quantify the confidence in the estimated parameters.\
{\listtext	\uc0\u8259 	}And like we just saw, generally speaking, the more data, the more confidence we have in the estimates\
{\listtext	\uc0\u8259 	}Going back to the two replicate experiments (training data vs testing data). Even though these two experiments resulted in different estimates for the population mean and population standard deviation, we can use statistics to quantify our confidence in how different they are.\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}\ul In this case, a \cf6 \ulc6 p-value\cf7 \ulc7 , or, alternatively, a \cf6 \ulc6 confidence interval\cf8 \ulc8 , would tell us that while the estimates are different, they are not 
\f2\i significantly
\f0\i0  different.\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 \ulnone {\listtext	\uc0\u8259 	}\ul That means, the results from the first experiment (training) should not be 
\f2\i significantly
\f0\i0  different from the results generated from the second experiment (testing).\
\ls5\ilvl6\ulnone {\listtext	\uc0\u8259 	}\ul In conclusion, we can generate results that are reproducible in future experiments.\ulnone  \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls5\ilvl1\cf0 {\listtext	\uc0\u8259 	}StatQuest: Confidence Intervals ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=TqOeMYtOc1w&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=19"}}{\fldrslt https://www.youtube.com/watch?v=TqOeMYtOc1w&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=19}})\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls5\ilvl2\cf0 {\listtext	\uc0\u8259 	}Many people misunderstand confidence intervals, but that\'92s only because they didn\'92t learn about bootstrapping first\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}Here is a bootstrap refresher\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}Imagine if we weighed a bunch of female mice\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}In the video, Josh is currently showing a 1D line ranging from 15-35. On the line, are 12 sample of mice who have been weighed. Note, we didn\'92t weigh every single female mouse on the planet, just 12.\
{\listtext	\uc0\u8259 	}We can calculate the mean of the 12 mice. Note, the sampled mean is not the mean of all the mice on the entire planet.\
{\listtext	\uc0\u8259 	}This is where \cf6 bootstrapping\cf0  comes in. We can use it to get a better estimate of the global mean of all the female mice on the entire planet.\
{\listtext	\uc0\u8259 	}Since we already have the mean from the first sample we collected we can implement bootstrapping.\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Step one \'97> We 
\f2\i randomly
\f0\i0  select 12 weights from the original sample (global mice population, duplicates are ok)\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}First. In the video, Josh is now showing another 1D line with 12 new 
\f2\i randomly
\f0\i0  chosen mouse samples directly underneath the first 1D line. However, in this sample there are a few duplicate mice weights, requiring us to stack the markers on top of one another.\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}Note, there is something called \cf6 sampling width replacement\cf0  that can happen here. In the second 1D line we se that there are 
\f2\i two
\f0\i0  furthest left samples fall directly beneath the 
\f2\i one
\f0\i0  furthest left sample on the first line. Also, to the right of the 
\f2\i one
\f0\i0  furthest left sample is another sample a few weights away. We then notice that there isn\'92t a weight down at this value on the second 1D. There is a gap. We call thing \cf6 sampling width replacement\cf0 .	\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}Step two \'97> We calculate the mean of the random sample (the second 1D line)\
{\listtext	\uc0\u8259 	}Repeat steps one and 2 until we have calculated a lot of means (> 10,000)\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Note, in the video Josh is now displaying a new 1D line with all of the means plotted on the number line. There is a heavily dense region right in the middle of the line around the middle, ~25. Appears to be normally distributed.\
{\listtext	\uc0\u8259 	}Thats all there is to bootstrapping\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Lets look at what a confidence interval is now (typically 95% confidence intervals are used)\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}A 95% confidence interval is just an interval that covers 95% of the means.\
{\listtext	\uc0\u8259 	}For example, 95% of the bootstrapped means fall in between 21 and 31.\
{\listtext	\uc0\u8259 	}That\'92s all a \cf6 confidence interval\cf0  is!\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Now that we know what a confidence interval is, let\'92s look at why they are useful\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Confidence intervals are useful because they are statistical tests that can be performed visually\
{\listtext	\uc0\u8259 	}For our example, because the interval covers 95% of the means, we know that anything outside of it occurs less than 5% of the time.\
{\listtext	\uc0\u8259 	}That is to say, the p-value of anything outside of the confidence interval < 0.05 (and thus, significantly different)\
{\listtext	\uc0\u8259 	}Here is an example of a visual statistic test\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}What is the p-value that the \'93true\'94 mean of all female mice, not just in our sample, weights are < 20?\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}Looking at our bootstrapped means, we see the highlighted region is outside of the 95% confidence interval which contains 95% of the means, we know that the probability that the \'93true\'94 mean is in this area has to be < 0.05. \
{\listtext	\uc0\u8259 	}The p-value is < 0.05. This is unlikely and we say there is a statistically significant difference between this sample relative to the other samples on the bootstrapped line/plot.\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Here is another example (comparing two samples)\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Female Mice vs Male Mice\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}In the video, Josh is still showing the female mice bootstrapped means on a 1D plot. Again, the bootstrapped means appear to be normally distributed around a mean of 25.\
{\listtext	\uc0\u8259 	}Directly underneath the female plot, Josh is showing a new plot with bootstrapped means of male mice. However, here the mice are normally distributed but the mean on the male mice plot falls on the upper-end value on the female mice plot\
{\listtext	\uc0\u8259 	}In other words, the right-tail of the female mice distribution crosses over the left tale of the male mice distribution.\
\pard\tx6700\tx7200\tx7920\tx8640\li7200\fi-7200\pardirnatural\partightenfactor0
\ls5\ilvl9\cf0 {\listtext	\uc0\u8259 	}IMPORTANT: Only the tails overlap here. The 95% confidence intervals on both plots do not overlap. Therefore, we know there is a statistically significant difference in the weights of female and male mice. We know the p-vale is < 0.05 just by looking at this picture!\
\pard\tx7420\tx7920\tx8640\li7920\fi-7920\pardirnatural\partightenfactor0
\ls5\ilvl10\cf0 {\listtext	\uc0\u8259 	}\ul HOWEVER\ulnone : There is one caveat. What if the confidence intervals overlap a little (some p-value > 0.05)? If they overlap, there is still a chance that the means are significantly different from each other, so, in this case, we still have to do our \cf6 t-test\cf0 ! When they don\'92t overlap, we don\'92t have to and can be rest assured that is a statistically significant difference between the two means.\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	} \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls5\ilvl1\cf0 {\listtext	\uc0\u8259 	}Covariance and Correlation Part 1: Covariance by StatQuest with Josh Starmer {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=qtaqvPAeEJY&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=6"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \ulc3 https://www.youtube.com/watch?v=qtaqvPAeEJY&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=6}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \ulc3 \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls5\ilvl2
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}The first main idea behind covariance is that it can classify three types of relations\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}Relationships with 
\f4\b positive trends
\f0\b0 \
{\listtext	\uc0\u8259 	}Relationships with 
\f4\b negative trends
\f0\b0 \
{\listtext	\uc0\u8259 	}Times where there is no relationship because there is 
\f4\b no trend
\f0\b0 \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls5\ilvl2\cf0 {\listtext	\uc0\u8259 	}However, covariance in and of itself, is not very interesting\
{\listtext	\uc0\u8259 	}The second main idea behind covariance is that covariance is only a computational stepping stone to something that is interesting, like correlation\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}Covariance is calculated by\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}summation( ( x - x_bar ) * ( y - y_bar ) ) / ( n - 1 )\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Note, for there to be a positive trend, the x and y value both have to fall equally below and above the mean on their respective axis.\
{\listtext	\uc0\u8259 	}See ~07:30 for better visual\
{\listtext	\uc0\u8259 	}Also, lets say we calculated a covariance value of 116 is 
\f2\i positive
\f0\i0 . This means that the slope of the relation between variable X and variable Y is 
\f2\i positive
\f0\i0 . In other words, when the covariance value is positive we classify the trend as positive. \
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Note, the covariance value itself isn\'92t very easy to interpret and depends on the context.\
{\listtext	\uc0\u8259 	}For example, the covariance value does not tell us if the slope of line representing the relationship is steep or not steep\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}It just tells us that the slope is positive\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}
\f4\b More importantly, the covariance value doesn\'92t tell us if the points are relatively close or relatively far to the dotted line\
\ls5\ilvl6
\f0\b0 {\listtext	\uc0\u8259 	}Note, we will talk about 
\f2\i why
\f0\i0  the covariance value is so hard to interpret later\
{\listtext	\uc0\u8259 	}Remember, even though covariance is hard to interpret, it is a computational stepping stone to more interesting things\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}For a scenario when there is no trend, lets look at the covariance when every value for variable X corresponds to the same value for variable Y\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The covariance value will be zero\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls5\ilvl1\cf0 {\listtext	\uc0\u8259 	}Statistics Fundamentals: The Mean, Variance and Standard Deviation by StatQuest with Josh Starmer {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=SzZ6GpcfoQY"}}{\fldrslt https://www.youtube.com/watch?v=SzZ6GpcfoQY}}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}ANOVA and related\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls5\ilvl2\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}ANOVA and related\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls5\ilvl2\cf0 {\listtext	\uc0\u8259 	}Hypothesis Testing (Critical Value Approach)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://newonlinecourses.science.psu.edu/statprogram/reviews/statistical-concepts/hypothesis-testing/critical-value-approach"}}{\fldrslt https://newonlinecourses.science.psu.edu/statprogram/reviews/statistical-concepts/hypothesis-testing/critical-value-approach}}(great read)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls5\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://keydifferences.com/difference-between-t-test-and-f-test.html#targetText=The%20difference%20between%20t%2Dtest%20and%20f%2Dtest%20can%20be,is%20small%20is%20t%2Dtest.&targetText=In%20contrast%2C%20f%2Dtest%20is,to%20compare%20two%20population%20variances"}}{\fldrslt https://keydifferences.com/difference-between-t-test-and-f-test.html#targetText=The%20difference%20between%20t%2Dtest%20and%20f%2Dtest%20can%20be,is%20small%20is%20t%2Dtest.&targetText=In%20contrast%2C%20f%2Dtest%20is,to%20compare%20two%20population%20variances}}  and    {\field{\*\fldinst{HYPERLINK "https://brandalyzer.blog/2010/12/05/difference-between-z-test-f-test-and-t-test/#targetText=A%20z%2Dtest%20is%20used,population%20standard%20deviation%20or%20not.&targetText=An%20F%2Dtest%20is%20used%20to%20compare%202%20populations'%20variances."}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://brandalyzer.blog/2010/12/05/difference-between-z-test-f-test-and-t-test/#targetText=A%20z%2Dtest%20is%20used,population%20standard%20deviation%20or%20not.&targetText=An%20F%2Dtest%20is%20used%20to%20compare%202%20populations'%20variances.}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul  
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone (great read)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}\cf9 Note, \ul the following methods \ulnone are generally used to validate our data. Just because we have a p-value lower than our acceptance criteria doesnt mean there is conclusive evidence that something significant is going on. \
{\listtext	\uc0\u8259 	}T test is usually performed after a p-value results in a number around our acceptance criteria.  \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf9 {\listtext	\uc0\u8259 	}For example, a p-value of 0.07 when our acceptance criteria is 0.05 (see StatQuest: Confidence Intervals by Josh Starmer (~4:00 {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=5Z9OIYA8He8&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=20"}}{\fldrslt https://www.youtube.com/watch?v=5Z9OIYA8He8&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=20}}) and  StatQuickie: Thresholds for Significance by Josh Starmer (\cf8 FANTASTIC short video\cf9  {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=KEofcJ1tfkI&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=21"}}{\fldrslt https://www.youtube.com/watch?v=KEofcJ1tfkI&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=21}})\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf9 {\listtext	\uc0\u8259 	}Also, note that just because we calculate  small p-value (eg, 0.03) under our acceptance criteria (eg, 0.05), it doesn\'92t mean that we have explained our data. We also generally want to have a good r-squared and use additional methods (eg, t-test, z-rest, etc) with our model that correlates and validates the data, respectively. We want to be able to explain the data.\
{\listtext	\uc0\u8259 	}Another big note\'85.extraordinary claims need extraordinary data! We don\'92t want a p-value of 0.047 with an acceptance criteria of 0.05. If we were to go publish an extraordinary claim our p-value better be crazy small.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf8 {\listtext	\uc0\u8259 	}T test is used to compare two related samples (good for small # of observations < ~30)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf8 {\listtext	\uc0\u8259 	}StatQuickie: Which t test to use ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=nnBJeb_I-q8&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=22"}}{\fldrslt https://www.youtube.com/watch?v=nnBJeb_I-q8&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=22}})\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf8 {\listtext	\uc0\u8259 	}There are two main type:\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Paired Data\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Are useful if we have \'93before and after\'94 measurement taken from the same test subject\
{\listtext	\uc0\u8259 	}For ex, if we have a group of people who are fixing to go in a clinical trial for blood pressure, we can measure their blood pressure before taking a certain blood pressure medication and then measure their blood pressure after they have taken the medication.\
{\listtext	\uc0\u8259 	}A pair of \'93before and after\'94 measurements for a test subject\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Unpaired Data\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Occurs when we have two different samples from a population.\
{\listtext	\uc0\u8259 	}For ex, if we measured height for one group of people (Group A), and measured the heights of another group of people (Group B)\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}There are two subcategories of unpaired t-tests\
\pard\tx6700\tx7200\tx7920\tx8640\li7200\fi-7200\pardirnatural\partightenfactor0
\ls5\ilvl9\cf0 {\listtext	\uc0\u8259 	}One assumes the variance of height measurements in Group A is equal to the variance of height measurements in Group B\
{\listtext	\uc0\u8259 	}The other one assumes the opposite, the variances differ (Josh Starmer recommends going with this one as it is a more conservative approach. Therefore, if the data can pass this more conservative t-test, it will mean the data is rock solid. This is my general recommendation.)\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Should you use a one-tail / one-sided t-test or a two-tail / two-sided t-test? \
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}A two-sided t-test\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Let\'92s go back to our example with Group A and Group B height measurements. A two-tail t-test for instance will test to see if Group A is significantly higher than Group B or if Group A is significantly lower than Group B. It is agnostic to the t-test, it doesn\'92t know if Group A should be higher or lower than Group B (same for vice-versa.  \
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}A one-sided t-test\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Is a lot less conservative. It requires the user (us) to know ahead of time which one is higher.\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}IMPORTANT: Generally, especially in academic journals, we always want the data to speak for itself. Therefore, the two-sided t-test is the better test to go with since it is slightly more conservative and lets the data speak for itself. \
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 \ul {\listtext	\uc0\u8259 	}T-test i\ulnone s a univariate hypothesis test, that is applied when standard deviation is not known and the sample size is small. T-statistic follows Student t-distribution, under null hypothesis.\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=pTmLQvMM-1M"}}{\fldrslt https://www.youtube.com/watch?v=pTmLQvMM-1M}} (great video on Student\'92s t-test)\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}t value = signal / noise = diff. Between group means / variability of groups\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}A t-test is used for testing the mean of one population against a standard or comparing the means of two populations if you do not know the populations\'92 standard deviation and when you have a limited sample (n < 30). If you know the populations\'92 standard deviation, you may use a z-test.\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Examples \
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Measuring the average diameter of shafts from a certain machine when you have a small sample.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}Z test (good for larger # of observations) \
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}A z-test is used for testing the mean of a population versus a standard, or comparing the means of two populations, with large (n \uc0\u8805  30) samples whether you know the population standard deviation or not. It is also used for testing the proportion of some characteristic versus a standard proportion, or comparing the proportions of two populations.\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Examples\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Comparing the average engineering salaries of men versus women.\
{\listtext	\uc0\u8259 	}Comparing the fraction defectives from 2 production lines.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}F test is used to test the equality of two populations\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}F-test is statistical test, that determines the equality of the variances of the two normal populations. F-statistic follows Snedecor f-distribution, under null hypothesis. Comparing two population variances.\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Examples\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Comparing the variability of bolt diameters from two machines.\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}Matched pair test\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}Matched pair test is used to compare the means before and after something is done to the samples. A t-test is often used because the samples are often small. However, a z-test is used when the samples are large. The variable is the difference between the before and after measurements.\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Examples\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The average weight of subjects before and after following a diet for 6 weeks\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls5\ilvl2\cf0 {\listtext	\uc0\u8259 	}F Statistic/Critical and F Value\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/f-statistic-value-test/#ANOVA"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/f-statistic-value-test/#ANOVA}} (great read)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}\'93An F statistic is a value you get when you run an ANOVA test or a regression analysis to find out if the means between two populations are significantly different. It\'92s similar to a T statistic from a T-Test; A-T test will tell you if a single variable is statistically significant and an F test will tell you if a group of variables are jointly significant.\'94\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls5\ilvl2\cf0 {\listtext	\uc0\u8259 	}F Distribution\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/f-statistic-value-test/#ANOVA"}}{\fldrslt https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/f-statistic-value-test/#ANOVA}}(great read, see graph in F distribution section)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}\'93The F Distribution is a probability distribution of the F Statistic. In other words, it\'92s a distribution of all possible values of the f statistic.\'94\
{\listtext	\uc0\u8259 	}\'93The F distribution is related to chi-square, because the f distribution is the ratio of two chi-square distributions with degrees of freedom \uc0\u957 1 and \u957 2 (note: each chi-square is first been divided by its degrees of freedom). Each curve depends on the degrees of freedom in the numerator (dfn) and the denominator (dfd). These depend upon your sample characteristics.\'94\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}\'93For example, in a simple one-way ANOVA between-groups,\'94\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Dfn = a \'96 1\
{\listtext	\uc0\u8259 	}dfd = N \'96 a\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}where\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}a =  the number of groups\
{\listtext	\uc0\u8259 	}n = the total number of subjects in the experiment\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}\cf2 Terminology alert:\cf0  The degrees of freedom in the denominator (dfd) is also referred to as the degrees of freedom error (dfe).\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls5\ilvl2\cf0 {\listtext	\uc0\u8259 	}Chi-squared \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/chi-square/"}}{\fldrslt https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/chi-square/}} (great read)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}What is a Chi-squared Test?\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Two types (both use \'93chi-square statistic\'94 and \'93chi-square distribution\'94 for different purposes)\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Chi-square goodness of fit test\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Determines if a sample data matches a population (see Goodness of Fit Test for more info {\field{\*\fldinst{HYPERLINK "https://www.statisticshowto.datasciencecentral.com/goodness-of-fit-test/"}}{\fldrslt https://www.statisticshowto.datasciencecentral.com/goodness-of-fit-test/}})\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Chi-square test for independence\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Compares two variables in a contingency table to see if they are related. In a more general sense, it tests to see whether distributions of categorical variables differ from each other.\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}A very small chi square test statistic means that our observed data fits our expected data extremely well. In other words, there is a relationship.\
{\listtext	\uc0\u8259 	}A very large chi square test statistic means that the data does not fit very well. In other words, there isn\'92t a relationship.\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}What is a chi-square statistic?\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}(chi_c)^2 = summation( (O_i - E_i)^2 / E_i )\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}where the subscript c are the degrees of freedom, O is our observed value (eg, markers on a scatter plot), and E is our expected value from our model, and i for every \'93ith\'94 position\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Note, it\'92s very rare that we\'92ll ever want to actually 
\f2\i use 
\f0\i0 this formula to find a chi-square by hand.\
{\listtext	\uc0\u8259 	}\cf8 \
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}A low value chi-square means there is a high correlation between our two sets of data. In theory, if our observed and expected values were equal (\'93no difference\'94) then chi-square would be zero\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Note,  deciding whether a chi-square test statistic is large enough to indicate a statistically significant difference isn\'92t as easy it seems.\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls5\ilvl2\cf0 {\listtext	\uc0\u8259 	}One Way ANOVA ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=q48uKU_KWas"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=q48uKU_KWas}})\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}The ultimate goal of the one-way ANOVA is to compare the means at at least three conditions \
{\listtext	\uc0\u8259 	}The first step is to always state the null hypotheses and the alpha level, all of this is done apriori \
{\listtext	\uc0\u8259 	}The null hypotheses states there are no significant relationships/correlations in the data. In other words, they all have same same mean.\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}\ul 123\ulnone  (column id\'92s)\
{\listtext	\uc0\u8259 	}122 \
{\listtext	\uc0\u8259 	}242\
{\listtext	\uc0\u8259 	}524\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}Step 1\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Null hypothesis (all means are equal, nothing significant in the data):   \uc0\u956 _1 = \u956 _2 = \u956 _3\
{\listtext	\uc0\u8259 	}Alternative hypothesis: We are going to hypothesize that there is at least one difference among the means.  \
{\listtext	\uc0\u8259 	}Alpha level = 0.05 # commonly used\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Step 2\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Determine degrees of freedom between groups (df_between) by taking the conditions/features in our study (k) and subtracting one\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}df_between = k - 1\
{\listtext	\uc0\u8259 	}df_between = 3 - 1 = 2\
{\listtext	\uc0\u8259 	}\cf2 Terminology alert: \cf7 \'94Degrees of freedom between\'94 is also referred to as the \'93degrees of freedom in the numerator\'94\
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Note, the between part means we have different subjects/cases/features in each group \
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Determine degrees of freedom within (df_within) by taking the total amount of values we have (N) and subtracting the number of conditions/features (k)\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}df_within = N - k \
{\listtext	\uc0\u8259 	}df_within = 9 - 3 = 6\
{\listtext	\uc0\u8259 	}\cf2 Terminology alert: \cf7 \'94Degrees of freedom within\'94 is also referred to as the \'93degrees of freedom in the denominator\'94\cf0 \
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Determine total degrees of freedom by adding df_between and df_within\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}df_total = 8 \
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Determine F_critical (aka F_stat)\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Use F distribution table or calculator to calculate F_critical\
{\listtext	\uc0\u8259 	}F_critical = 5.14\
{\listtext	\uc0\u8259 	}Note,\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/f-statistic-value-test/#ANOVA"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/f-statistic-value-test/#ANOVA}}\
\pard\tx6700\tx7200\tx7920\tx8640\li7200\fi-7200\pardirnatural\partightenfactor0
\ls5\ilvl9\cf0 {\listtext	\uc0\u8259 	}\'93A calculator will certainly give you a fast answer. But with many scenarios in statistics, you will look at a range of possibilities and a table is much better for visualizing a large number of probabilities at the same time."\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Step 3\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Analysis of Variance\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Calculate mean for each condition\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}\uc0\u956 _1 = (1+2+5)/3 = 2.67\
{\listtext	\uc0\u8259 	}\uc0\u956 _2 = (2+4+2)/3 = 2.67\
{\listtext	\uc0\u8259 	}\uc0\u956 _3 = (2+2+4)/3 = 3.00\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Calculate grand mean of data\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}\uc0\u956 _total = summation(3x3 matrix)/count(3x3 matrix) = 25/9 = 2.78\
\pard\tx6700\tx7200\tx7920\tx8640\li7200\fi-7200\pardirnatural\partightenfactor0
\ls5\ilvl9\cf0 {\listtext	\uc0\u8259 	}or\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}\uc0\u956 _total = (2.67+2.67+3.00)/3 = 25/9 = 2.78\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Calculate sum of squares total (SS_total)\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}SS_total = \uc0\u931 (\u956 _i - \u956 _total)^2 # where i is every value in dataset\
{\listtext	\uc0\u8259 	}               = (1 - 2.78)^2 + (2 - 2.78)^2 + (5 - 2.78)^2   +    etc\
{\listtext	\uc0\u8259 	}
\f4\b SS_total = 13.6
\f0\b0 \
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Calculate sum of squares within (SS_within)\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}SS_within =  (1 - 2.67)^2 + (2 - 2.67)^2 + (5 - 2.67)^2 +\
{\listtext	\uc0\u8259 	}                     (2 - 2.67)^2 + (4 - 2.67)^2 + (2 - 2.67)^2 +\
{\listtext	\uc0\u8259 	}                     (2 - 3.00)^2 + (2 - 3.00)^2 + (4 - 3.00)^2 \
{\listtext	\uc0\u8259 	}
\f4\b SS_within = 13.34
\f0\b0 \
{\listtext	\uc0\u8259 	}SS_between = SS_total - SS_within = 13.6 - 13.34\
{\listtext	\uc0\u8259 	}
\f4\b SS_between = 0.23
\f0\b0  \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Step 4\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Calculate the mean square between / aka the variance between\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}\uc0\u956 _square_between = SS_between / df_between = 0.23 / 2\
{\listtext	\uc0\u8259 	}
\f4\b \uc0\u956 _square_between = 0.12\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6
\f0\b0 \cf0 {\listtext	\uc0\u8259 	}Calculate the mean square within / aka the variance within\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}\uc0\u956 _square_within = SS_within / df_within = 13.34 / 6\
{\listtext	\uc0\u8259 	}
\f4\b \uc0\u956 _square_within = 2.22\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5
\f0\b0 \cf0 {\listtext	\uc0\u8259 	}Step 5\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Calculate F value\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}F_value = \uc0\u956 _square_between / \u956 _square_within = 0.12 / 2.22\
{\listtext	\uc0\u8259 	}
\f4\b F_value = 0.05
\f0\b0 \
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Now, let\'92s compare F_value with F_critical\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Here, the F_value (0.05) is smaller than F_critical (5.14)\
{\listtext	\uc0\u8259 	}So this supports the null hypothesis, meaning there is no significant difference between the three groups.\
{\listtext	\uc0\u8259 	}However, if the data set proved to be significant (aka we rejected the null hypothesis), we should also consider the p value. The p value is determined by the F statistic and is the probability the results could have happened by chance.\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}In other words, we would need to look into using the F critical/statistic in COMBINATION (not comparing) with the p-value to determine if ALL of the variables in the dataset are significant. The F critical/statistic is just comparing the joint effect of all the variables together.\
\pard\tx6700\tx7200\tx7920\tx8640\li7200\fi-7200\pardirnatural\partightenfactor0
\ls5\ilvl9\cf0 	\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls5\ilvl2\cf0 {\listtext	\uc0\u8259 	}What is a p value?\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.statisticshowto.datasciencecentral.com/p-value/"}}{\fldrslt https://www.statisticshowto.datasciencecentral.com/p-value/}} (great read)\
{\listtext	\uc0\u8259 	}Note, in the examples below we are calculating a two-sided p-value since we are considering equally or rarer possibilities (part 2 and part 3 below).\
{\listtext	\uc0\u8259 	}Example 1 (Simple)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}StatQuest: P Values, clearly explained \
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=5Z9OIYA8He8"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=5Z9OIYA8He8}} (see ~05:20)\
{\listtext	\uc0\u8259 	}What is the p-value of getting two heads in a row when flipping a coin?\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}HH, HT, TH, TT\
{\listtext	\uc0\u8259 	}Part 1\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The first part of a p-value is the probability that random chance generated the data\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}The probability of flipping a coin two times and getting heads on each toss \'97> (1/2)^1 * (1/2)^1 = (1/2)^2 = 0.25\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Part 2\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The second part of a p-value is to add anything else in the outcome that has equal probability. \
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}The probability of getting two tails on both coin flips is the same as getting two heads \'97> (1/2)^1 * (1/2)^1 = (1/2)^2 = 0.25\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Part 3\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The last part of a p-value is to add on anything that is rarer than what was observed\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}This part would be equal to zero since there aren\'92t any rarer outcomes than two heads or two tails \'97> 0\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Total p-value\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}herefore, the p-value for HH is 0.25+0.25+0 = 
\f4\b 0.50
\f0\b0 \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}To summarize this example\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The probability of getting HH is 0.25\
{\listtext	\uc0\u8259 	}The p-value for getting HH is 0.5\
{\listtext	\uc0\u8259 	}In this case, the probability and p-value are not equal\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 2 (Simple, ~same as Example 1)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=5Z9OIYA8He8"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=5Z9OIYA8He8}} (see ~07:00)\
{\listtext	\uc0\u8259 	}What is the p-value of getting five heads in a row when flipping a coin five times?\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}see video for all possible combinations and better visualization \
{\listtext	\uc0\u8259 	}Part 1\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The first part of a p-value is the probability that random chance generated the data\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}(1/2)^5 = 1/32 = 0.03125 \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Part 2\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The second part of a p-value is to add anything else in the outcome that has equal probability.\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}(1/2)^5 = 1/32 = 0.03125 \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Part 3\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The last part of a p-value is to add on anything that is rarer than what was observed\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}This part would be equal to zero since there aren\'92t any rarer outcomes than five heads or five tails \'97> 0\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Total p-value,\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Therefore, the p-value for HHHHH is 0.03125+0.03125+0 = 
\f4\b 0.0625
\f0\b0 \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}To summarize this example\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The probability of getting HHHHH is 0.03125\
{\listtext	\uc0\u8259 	}The p-value for getting HHHHH is 0.0625\
{\listtext	\uc0\u8259 	}In this case, the probability and p-value are not equal\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 3 (intermediate)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=5Z9OIYA8He8"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=5Z9OIYA8He8}} (see ~09:45)\
{\listtext	\uc0\u8259 	}What is the p-value of getting four heads and one tail when flipping a coin five times? 
\f4\b Order doesn\'92t matter.
\f0\b0 \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}see video for all possible combinations and better visualization \
{\listtext	\uc0\u8259 	}Part 1\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The first part of a p-value is the probability that random chance generated the data\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}since there are 5 possibilities of getting four heads\
{\listtext	\uc0\u8259 	}out of 32 total possible outcomes\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}5/32 = 0.15625\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Part 2\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The second part of a p-value is to add anything else in the outcome that has equal probability.\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}since there are 5 possibilities of getting four tails\
{\listtext	\uc0\u8259 	}out of 32 total possible outcomes\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}5/32 = 0.15625\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Part 3\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The last part of a p-value is to add on anything that is rarer than what was observed\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Here, there are two outcomes that are more rare than a combination of four heads and one tail. They are \'97> HHHHH and TTTTT\
{\listtext	\uc0\u8259 	}We then add the probability of both rarer outcomes \
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}1/32 + 1/32 = 0.0625\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Total p-value\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Therefore, the p-value for a combination of four heads and one tail is  is 0.15625 + 0.15625 + 0.0625 = 
\f4\b 0.375
\f0\b0 \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 4 (advanced)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=5Z9OIYA8He8"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=5Z9OIYA8He8}} (see ~11:00)\
{\listtext	\uc0\u8259 	}What about if we were measuring height instead? It was easy to write out all possible outcomes for coin tosses, but what if we wanted to calculate the probabilities for heights instead? We use the \'93density\'94 function instead.\
{\listtext	\uc0\u8259 	}Let\'92s look at the distribution/density of height measurements in women between 15 and 49 years old in 1996\
{\listtext	\uc0\u8259 	}The area under the curve indicates the probability that a person will have a height within a range of possible values.\
{\listtext	\uc0\u8259 	}In this example, 95% of the area under the curve is between 142 cm and 169 cm, indicating that most women are between those two values. \
{\listtext	\uc0\u8259 	}In other words, there is a 95% probability that each time we measure a woman\'92s height, their height will be between 142 cm and 169 cm.\
{\listtext	\uc0\u8259 	}Also, there is a 2.5% probability that each time we measure a woman\'92s height, their height be less than 142 cm.\
{\listtext	\uc0\u8259 	}There is also a 2.5% probability that each time we measure a woman\'92s height, their height be greater than 169 cm.\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}What is the p-value for someone who is 142 cm tall? (See ~12:00)\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Part 1\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The first part of a p-value is the probability that random chance generated the data\
{\listtext	\uc0\u8259 	}The area for people 142 cm or shorter  \'97> 2.5%\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Part 2\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The second part of a p-value is to add anything else in the outcome that has equal probability.\
{\listtext	\uc0\u8259 	}The area for people 169 cm or taller \'97> 2.5%\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Part 3 \
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The last part of a p-value is to add on anything that is rarer than what was observed\
{\listtext	\uc0\u8259 	}Here nothing is rarer \'97> 0%\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Total p-value\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}
\f4\b 5.0%\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4
\f0\b0 \cf0 {\listtext	\uc0\u8259 	}What\'92s the p-value for someone who is between 155.4 and 156 cm tall between someone is 142 cm tall and 169 cm tall (at the middle of the distribution)?  (See ~13:48)\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Note: The probability of someone being between 155.4 and 156 cm is only 0.04 or 4%. The area is pretty small!\
{\listtext	\uc0\u8259 	}Part 1\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Note: The first part of calculating a p-value is the probability of the event of interest\
{\listtext	\uc0\u8259 	}Therefore \'97> 
\f4\b 4.0 %
\f0\b0 \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Part 2\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The second part of a p-value is to add anything else in the outcome that has equal probability.\
{\listtext	\uc0\u8259 	}We see the left side of the distribution from 142 cm to 155.4 cm is 48%\
{\listtext	\uc0\u8259 	}We see the right side of the distribution from 156 cm and 169 cm is also 48%\
{\listtext	\uc0\u8259 	}Therefore, 48%+48% = 
\f4\b 96%
\f0\b0 \
{\listtext	\uc0\u8259 	}Part 3 \
{\listtext	\uc0\u8259 	}The last part of a p-value is to add on anything that is rarer than what was observed\
{\listtext	\uc0\u8259 	}Here nothing is rarer \'97> 
\f4\b 0%
\f0\b0 \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Total p-value\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}
\f4\b 100% or 1\
\ls5\ilvl6
\f0\b0 {\listtext	\uc0\u8259 	}In conclusion, this means that there is nothing special about measuring someone who has the average height even though that in particular is rare\
{\listtext	\uc0\u8259 	}Also, in this example, the probability of measuring someone between 155.4 and 156 cm tall is tiny (
\f4\b 0.04 or 4.0%
\f0\b0 ), but the p-value is huge (
\f4\b 1 or 100%
\f0\b0 )\
{\listtext	\uc0\u8259 	}In other words, this is also conclusive that there is nothing significant about measuring someones who has an average height.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls5\ilvl2\cf0 {\listtext	\uc0\u8259 	}StatQuest: One or Two Tailed P-Values by Josh Starmer ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=bsZGt-caXO4&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=23"}}{\fldrslt https://www.youtube.com/watch?v=bsZGt-caXO4&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=23}}) great \
{\listtext	\uc0\u8259 	}The Binomial Distribution and Test, Clearly Explained!!!  by Josh Starmer ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=J8jNoF-K8E8&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=24"}}{\fldrslt https://www.youtube.com/watch?v=J8jNoF-K8E8&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=24}}) fantastic video to see how we can relate this to real business problems\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}Usually when people talk about the binomial distribution, they talk about flipping a coin\
{\listtext	\uc0\u8259 	}But who really cares about flipping coins?\
{\listtext	\uc0\u8259 	}What folks really want to know is whether or not people like Orange Fanta more than Grape Fanta\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}Which flavor reigns supreme? Or are they both equally loved (null hypothesis)??\
{\listtext	\uc0\u8259 	}Scenario\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Let\'92s say we take a sample of 7 people. From the sample, 4 people like Orange Fanta and 3 people like Purple Fanta. Is this one sample enough to be confident that \'93most people like Orange Fanta\'94? Or could it be that people, in general, don\'92t have a preference and these results are just due to random chance (aka \cf6 null hypothesis\cf0 ) and a small sample size?\
{\listtext	\uc0\u8259 	}Let\'92s say we take another survey/sample of 7 people, and 3 people say they like Orange Fanta and 4 people say they like Purple Fanta.\
{\listtext	\uc0\u8259 	}To get to the bottom of this mystery, we need to get a sense of what to expect if there is no preference. Then we determine if our survey results fit those expectations. If not, then we can reject the idea that both Fanta are loved equally (aka we reject the \cf6 null hypothesis\cf0 )\
{\listtext	\uc0\u8259 	}The binomial distribution will tell us what to expect if there is no preference.\
{\listtext	\uc0\u8259 	}Note, the binomial distribution model follows the following formula of what to expect when there is no preference\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}pr(x | n, p) = ((n!)/((x!)*(n-x)!)) * (p^x) * (1-p)^(n-x)\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}If the model is a poor fit, we will reject the idea that both flavors are loved equally (aka we reject the null hypothesis)\
{\listtext	\uc0\u8259 	}Example\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Assume I asked 3 people if they liked Orange Fanta 
\f2\i more 
\f0\i0 than Grape Fanta \
{\listtext	\uc0\u8259 	}The first two say they like Orange Fanta and the last one says Grape Fanta.\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Note, there is a 50% chance a person will say either orange or grape\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Now, we can calculate the probability of the first two people randomly choosing orange and the third person randomly choosing grape\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Assuming there is no preference between the two\
{\listtext	\uc0\u8259 	}The probability of the first person selecting orange is\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}
\f4\b 0.5
\f0\b0 \
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}The probability of the first two people selecting orange is\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}0.5 * 0.5 = 
\f4\b 0.25\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7
\f0\b0 \cf0 {\listtext	\uc0\u8259 	}The probability of the first two people preferring orange and the third person preferring grape is\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}0.5 * 0.5 * 0.5 = 
\f4\b 0.125
\f0\b0 \
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}NOTE: 0.125 is the probability of the first two people saying they prefer orange and the third person saying they prefer grape.\
{\listtext	\uc0\u8259 	}
\f4\b IT IS NOT the probability that any 2 out of 3 people would prefer orange.\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6
\f0\b0 \cf0 {\listtext	\uc0\u8259 	}It could have just as easily been that the first person said they preferred grape.\
{\listtext	\uc0\u8259 	}Note, all of the following combinations are equally as likely\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}purple, orange, orange = 0.125\
{\listtext	\uc0\u8259 	}orange, purple, orange = 0.125\
{\listtext	\uc0\u8259 	}orange, orange, purple = 0.125\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}To get the probability that any 2 out of 3 people prefer Orange Fanta we have to sum up the probability of all possible combinations:\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}0.125 + 0.125 + 0.125 = 
\f4\b 0.375\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6
\f0\b0 \cf0 {\listtext	\uc0\u8259 	}Therefore, the probability of randomly selecting three people and two of them saying they prefer Orange Fanta and one saying Purple Fanta is 0.375\
{\listtext	\uc0\u8259 	}Alternatively, we could of done the math using this formula:\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}pr(x | n, p) = ((n!)/((x!)*(n-x)!)) * (p^x) * (1-p)^(n-x)\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls5\ilvl8\cf0 {\listtext	\uc0\u8259 	}x = number of people who preferred Orange Fanta = 2\
{\listtext	\uc0\u8259 	}n = total number of people we asked = 3\
{\listtext	\uc0\u8259 	}Note, \'93x | n\'94 also means \'93n - x\'94. Therefore, it also means the total number of people minus the number of people who preferred Orange Fanta equals the number of people who said they prefer Grape Fanta\
{\listtext	\uc0\u8259 	}p = probability that someone will pick Orange Fanta = 0.5\
{\listtext	\uc0\u8259 	}Note, the probability that someone might pick Grape Fanta is \'931 - p\'94\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Together, the expression says \'93The probability of x (the number of people who say they prefer Orange Fanta), given n, the number of people we asked, and p (the probability of picking Orange Fanta)\'85\'94\
{\listtext	\uc0\u8259 	}
\f4\b pr(x | n, p) = 0.375
\f0\b0 \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Lets go back to our original question. If 4 people say they like Orange Fanta and 3 people say they like Grape Fane, can we conclude that people in general prefer Orange Fanta?\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}pr(x | n, p) = ((n!)/((x!)*(n-x)!)) * (p^x) * (1-p)^(n-x)\
{\listtext	\uc0\u8259 	}where x = 4, n = 7, and p = 0.5\
{\listtext	\uc0\u8259 	}
\f4\b pr(x | n, p) = 0.273\
\ls5\ilvl6
\f0\b0 {\listtext	\uc0\u8259 	}In other words, 0.273 is the probability that 4 of 7 people would randomly prefer Orange Fanta\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Note, when you use a binomial distribution to calculate a p-value, it\'92s called a Binomial Test\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}So what\'92s the p-value for 4 of 7 people preferring Orange Fanta?\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls5\ilvl7\cf0 {\listtext	\uc0\u8259 	}Note, the p-value is the probability of the observed data (4 of 7 people prefer Orange Fanta), plus the probabilities of all other possibilities that are equally likely and rarer (see ~11:30 in the video, {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=J8jNoF-K8E8&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=24"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=J8jNoF-K8E8&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=24}})\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}The conclusion for the p-value saying that 4 out of 7 people stating that they prefer Orange Fanta is 1\
{\listtext	\uc0\u8259 	}Therefore, this means that we support the null-hypothesis. Both flavors are equally loved. For there to be strong evidence in saying one is preferred over the other, the p-value would need to be lower than some acceptance criteria (eg, \uc0\u945  = 0.05)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls5\ilvl2\cf0 {\listtext	\uc0\u8259 	}StatQuickie: Standard Deviation vs Standard Error by Josh Starmer ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=A82brFpdr9g&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=13"}}{\fldrslt https://www.youtube.com/watch?v=A82brFpdr9g&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=13}})\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}The standard deviation of the means is called the \cf6 standard error\cf0 .\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}For example, imagine we took 5 different samples  where there are 5 measurements/observations per sample. The sample mean and sample standard deviation can be calculated for each sample. If we were then to take the 5 means, and plot them on a number line and calculate the standard deviation. We would get the \cf6 standard error\cf0 .\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls5\ilvl1\cf0 {\listtext	\uc0\u8259 	}Alpha Value\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls5\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.statisticshowto.datasciencecentral.com/p-value/"}}{\fldrslt https://www.statisticshowto.datasciencecentral.com/p-value/}} (see P Value vs Alpha level section)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}\'93Alpha levels are controlled by the researcher and are related to confidence levels. You get an alpha level by subtracting your confidence level from 100%. For example, if you want to be 98 percent confident in your research, the alpha level would be 2% (100% \'96 98%). When you run the hypothesis test, the test will give you a value for p. Compare that value to your chosen alpha level. For example, let\'92s say you chose an alpha level of 5% (0.05). If the results from the test give you:\'94\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}\'93A small p (\uc0\u8804  0.05), reject the null hypothesis. This is strong evidence that the null hypothesis is invalid.\'94\
{\listtext	\uc0\u8259 	}\'93A large p (> 0.05) means the alternate hypothesis is weak, so you do not reject the null.\'94\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls6\ilvl1\cf0 {\listtext	\uc0\u8259 	}Multicollinearity (occurs when independent variables in a regression model are correlated.)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls6\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://towardsdatascience.com/multicollinearity-in-data-science-c5f6c0fe6edf"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://towardsdatascience.com/multicollinearity-in-data-science-c5f6c0fe6edf}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls6\ilvl1
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}Bias and Variance Tradeoff (Great)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls6\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=EuBBz3bI-aA"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=EuBBz3bI-aA}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone 	\uc0\u8259 	The first thing we do before start looking bias and variance is to split out data into a training and test set\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\cf0 	\uc0\u8259 	Example\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls7\ilvl5\cf0 {\listtext	\uc0\u8259 	}imagine viewing a scatter plot with data in a log10 shape and showing 80% of the markers/data as blue and the remaining 20% as green. The blue dots are the training set and the green dots are the testing set\
{\listtext	\uc0\u8259 	}\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\cf0 	
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls8\ilvl3
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}The inability for a machine learning method (like linear regression) to capture the true relationship is called 
\f4\b \cf6 bias
\f0\b0 \cf0 \
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls8\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls8\ilvl5\cf0 {\listtext	\uc0\u8259 	}Think about trying to fit a linear line to a training set with log10 type scatter data\
{\listtext	\uc0\u8259 	}Because the linear line can\'92t be curved like the \'93true\'94 relationship, we say it has a relatively large amount of \cf6 bias\cf0 . In other words, the inability for a machine learning method (like linear regression) to capture the true relationship is called 
\f4\b \cf6 bias\cf0 \
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls8\ilvl4
\f0\b0 \cf0 {\listtext	\uc0\u8259 	}Example\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls8\ilvl5\cf0 {\listtext	\uc0\u8259 	}Another machine learning method, may try to fit a squiggly line (zig-zag/decision tree type shape) to the training set. The squiggly line can handle to the arc of a true relationship with log10 type data so we can it would have a relatively low amount of 
\f4\b \cf6 bias
\f0\b0 \cf0  compared to trying trying to fit a straight line to log10 type data.\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls8\ilvl6\cf0 {\listtext	\uc0\u8259 	}To quantify this, we can compare the sum of squares between the squiggly line model and straight line model. But note, the squiggly line model fits the scatter plot so well, that the sum of squares will be zero! The squiggly line wins here at modeling the training set data.\
{\listtext	\uc0\u8259 	}Lets now look at the testing set and calculate the sum of squares\
{\listtext	\uc0\u8259 	}The straight line wins here! It\'92s sum of squares is lower than that of the squiggly line fine\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls8\ilvl3\cf0 {\listtext	\uc0\u8259 	}\cf6 Variance\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls8\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls8\ilvl5\cf0 {\listtext	\uc0\u8259 	}Since the squiggly line model fitted the training set so well, but failed to fit the testing set well, we say it has a relatively large amount of \cf6 variance\cf7 . In other words, the difference in fits between the data sets is called \cf6 variance\cf7 .\
\ls8\ilvl5\cf0 {\listtext	\uc0\u8259 	}In summary, the squiggly line had low \cf6 bias\cf0  with the training and test data since it is flexible and can adapt to the log10 shaped data. But\'85the squiggly line has \cf8 high variability/\cf6 variance\cf0  because it results in a vastly different sums of squares for different data sets. In other words, it\'92s hard to predict how well the squiggly line will perform with future data sets. It might do well sometimes, and other times it might do terribly.\
{\listtext	\uc0\u8259 	}In contrast, the straight line has relatively high balance when trying to capture the true relationship of the log10 shaped data. But\'85. The straight line has relatively low variability/variance because the sums of squares are very similar between different datasets.  In other words, the straight line might only give good predictions, and not 
\f2\i great 
\f0\i0 predictions. But they will be consistently good predictions\
{\listtext	\uc0\u8259 	}\cf2 Terminology alert:\cf0 \
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls8\ilvl6\cf0 {\listtext	\uc0\u8259 	}If the model fits the training set better than the test set we call this \cf6 overfitting\cf8 .\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls8\ilvl3\cf0 {\listtext	\uc0\u8259 	}Overall\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls8\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls8\ilvl5\cf0 {\listtext	\uc0\u8259 	}In machine learning, the ideal algorithm has low bias and can accurately model the true relationship (imagine a model fitting a training set that has log10 type shape data). Also, the ideal model will have low variability, by producing consistently predictions across different data sets (imagine a representation of the same model but now with testing data. The model will ideally fit it the same. The sums of squares are about equal between data sets if it has low variability.) An ideal model for your data to create is done by finding the sweet spot between a simple model (think about the straight line fitting the training set, ~high bias) and complex model (think about the squiggly line model fitting the training set perfectly, zero bias)\
{\listtext	\uc0\u8259 	}\cf2 Terminology alert\cf0 :\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls8\ilvl6\cf0 {\listtext	\uc0\u8259 	}Three commonly used methods for finding the sweet spot between simple and complicated models are: 
\f4\b regularization
\f0\b0 , 
\f4\b boosting
\f0\b0 , and 
\f4\b bagging
\f0\b0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls8\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Finance\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls8\ilvl1\cf0 {\listtext	\uc0\u8259 	}Black\'96Scholes model\
{\listtext	\uc0\u8259 	}Definitions:\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls8\ilvl2\cf0 {\listtext	\uc0\u8259 	}Risk\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls8\ilvl3\cf0 {\listtext	\uc0\u8259 	}How do you measure Risk\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls8\ilvl2\cf0 {\listtext	\uc0\u8259 	}Stocks\
{\listtext	\uc0\u8259 	}Portfolio\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls8\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Difference between linear and logistic regression\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls8\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://techdifferences.com/difference-between-linear-and-logistic-regression.html"}}{\fldrslt https://techdifferences.com/difference-between-linear-and-logistic-regression.html}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls8\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls9\ilvl0\cf0 	\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\
SLANG TERMS/TERMINOLOGY\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls10\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Long Data\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls10\ilvl1\cf0 {\listtext	\uc0\u8259 	}Bunch of columns, little rows\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls10\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Wide Data\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls10\ilvl1\cf0 {\listtext	\uc0\u8259 	}Bunch of rows, little columns\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\
MATHEMATICS\
\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls11\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Common Functions\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls11\ilvl1\cf0 {\listtext	\uc0\u8259 	}Sigmoid\
{\listtext	\uc0\u8259 	}Threshold\
{\listtext	\uc0\u8259 	}Rectifier\
{\listtext	\uc0\u8259 	}Hyperbolic tangent\
{\listtext	\uc0\u8259 	}Softmax Function\
{\listtext	\uc0\u8259 	}Cross-Entropy\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls11\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Common Distributions\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls11\ilvl1\cf0 {\listtext	\uc0\u8259 	}Gaussian\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls11\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}The Applications of Matrices \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls11\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=rowWM-MijXU"}}{\fldrslt https://www.youtube.com/watch?v=rowWM-MijXU}}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\'97\
LOOK UP / SORT LATER\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Python\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}Error Handing!\
{\listtext	\uc0\u8259 	}OpenCV\
{\listtext	\uc0\u8259 	}Looping through files in a directory\
{\listtext	\uc0\u8259 	}sparsity (can come up a lot in NLP, we want to avoid it)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://realpython.com/python-modules-packages/"}}{\fldrslt https://realpython.com/python-modules-packages/}}\
{\listtext	\uc0\u8259 	}Pandas\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls12\ilvl2\cf0 {\listtext	\uc0\u8259 	}Multiindex DataFrames\
{\listtext	\uc0\u8259 	}Levels in a data frame ({\field{\*\fldinst{HYPERLINK "https://stackoverflow.com/questions/48761486/pandas-unable-to-reset-index-because-name-exist"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://stackoverflow.com/questions/48761486/pandas-unable-to-reset-index-because-name-exist}})\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}Linked Lists\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Watch Later\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}YouTube\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls12\ilvl2\cf0 {\listtext	\uc0\u8259 	}Image Classifier using VGG16 Model (Great playlist to go through by deeplizard)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls12\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=oDHpqu52soI&list=PLZbbT5o_s2xrwRnXk_yCPtnqqo4_u2YGL&index=13"}}{\fldrslt https://www.youtube.com/watch?v=oDHpqu52soI&list=PLZbbT5o_s2xrwRnXk_yCPtnqqo4_u2YGL&index=13}} \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Big Data Tools\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}ipyparallel (formerly ipython cluster)\
{\listtext	\uc0\u8259 	}pyspark\
{\listtext	\uc0\u8259 	}spaCy \'97> NLP\
{\listtext	\uc0\u8259 	}Gensim \'97> topic modeling and NLP \
{\listtext	\uc0\u8259 	}Apache OpenNLP \'97> NLP\
{\listtext	\uc0\u8259 	}Hadoop\
{\listtext	\uc0\u8259 	}Spark\
{\listtext	\uc0\u8259 	}H2O\
{\listtext	\uc0\u8259 	}CDSW\
{\listtext	\uc0\u8259 	}Domino Labs\
{\listtext	\uc0\u8259 	}HIVE\
{\listtext	\uc0\u8259 	}PIG\
{\listtext	\uc0\u8259 	}and unstructured data\
{\listtext	\uc0\u8259 	}etc.\
{\listtext	\uc0\u8259 	}CPLEX, IBM-DOC & AnyLogic \
{\listtext	\uc0\u8259 	}Advanced SQL skills with Teradata, SQL Server, Hive and Spark experience\
{\listtext	\uc0\u8259 	}Experience with Airflow, Argo, Luigi, or similar orchestration tool\
{\listtext	\uc0\u8259 	}Experience performing root cause analysis on Spark jobs to identify areas for improvement\
{\listtext	\uc0\u8259 	}Experience with No-SQL databases such as HBase, Cassandra, or Redis.\
{\listtext	\uc0\u8259 	}Experience with streaming technologies such as Kafka, Flink, or Spark Streaming\
{\listtext	\uc0\u8259 	}Experience with DevOps principals and CI/CD\
{\listtext	\uc0\u8259 	}Experience with Containers and Kubernetes\
{\listtext	\uc0\u8259 	}Combinatorics (Mixed Integer Programming)\
{\listtext	\uc0\u8259 	}Developing and/or applying linear, mixed integer, stochastic programming to solve demand planning, supply chain, production scheduling \
{\listtext	\uc0\u8259 	}Discrete Event Simulation, Factor Analysis, Genetic Algorithms, Bayesian Probability Models, Hidden Markov Models and Sensitivity Analysis. (Cotiviti)\
{\listtext	\uc0\u8259 	}Amazon AWS machine learning technologies such as Amazon SageMaker, TensorFlow, PyTorch, Keras, Amazon Transcribe, Amazon Textract\
{\listtext	\uc0\u8259 	}statistical pattern recognition, graph theoretic approaches, high dimensional data visualization techniques, nonparametrics statistics as well as game theory fundamentals\
{\listtext	\uc0\u8259 	}Natural Language Processing & Text Mining, Experimental Design, Computer Vision & Image Processing, Bayesian Networks, Reinforcement Learning, Collaborative Filtering, Network/Graph Mining, Combinatorial Optimization, Linear & Mixed-Integer Programming, Discrete-Event & Stochastic Simulation\
{\listtext	\uc0\u8259 	}Experience with statistical methods such t-test of means, Tukey-HSD tests of means on groups, ANOVA, Proportion tests, data normalization and scaling, univariate and multivariate outlier detection\
{\listtext	\uc0\u8259 	}Survival Analysis\
{\listtext	\uc0\u8259 	}Time-Series Analysis\
{\listtext	\uc0\u8259 	}Anomaly Detection\
{\listtext	\uc0\u8259 	}years experience with predictive and forecasting techniques and tools including time-series analysis (Autocorrelation plots, ARIMA / ARIMAX, Vector Auto Regressions, Recurrent Neural Networks), machine learning model development (grid searching, cross validation, parameter tuning & optimization), and appropriate model evaluation (ROC analysis, confusion-matrices, error rate logging)\
{\listtext	\uc0\u8259 	}Experience with data mining processes (SEMMA, CRISP-DM), data preparation, consolidation, imputation, transformation, interaction, variable reduction, modeling, maintenance, and post-mortem analysis.\
{\listtext	\uc0\u8259 	}Blackbox and bespoke solutions\
{\listtext	\uc0\u8259 	}Test-driven development \
{\listtext	\uc0\u8259 	}Experience with statistical methods such t-test of means, Tukey-HSD tests of means on groups, ANOVA, Proportion tests, data normalization and scaling, univariate and multivariate outlier detection.\
{\listtext	\uc0\u8259 	}Experience with modeling techniques such as linear models, decision trees, neural networks, k-nearest-neighbor, support vector machines, cluster analyses, and ensembling methods.\
{\listtext	\uc0\u8259 	}Knime and RapidMiner \
{\listtext	\uc0\u8259 	}Experience with Big Data technologies including but not limited to: Sqoop, Hive, YARN, Spark, NiFi, Kafka, HDFS, and HBase.\
{\listtext	\uc0\u8259 	}Knowledge in JanusGraph, Apache Ranger, Apache Atlas, and graph-based solutions.\
{\listtext	\uc0\u8259 	}Experience with Agile methodologies (e.g. Scrum, Kanban, Dev/Ops)\
{\listtext	\uc0\u8259 	}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Business Intelligence Analyst\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}Intermediate level T-SQL (queries, stored procedures, functions, DDL, DML) \
{\listtext	\uc0\u8259 	}Intermediate level SSIS skills (or other ETL technology) \
{\listtext	\uc0\u8259 	}Intermediate level Excel skills \
{\listtext	\uc0\u8259 	}Strong proficiency with data modeling, ERDs, UML modeling, workflow diagrams\
{\listtext	\uc0\u8259 	}Understanding of data constructs such as appropriate data types, data normalization, data quality analysis and efficient data modeling\
{\listtext	\uc0\u8259 	}Experience with at least one data analysis tool such as PowerBI, Tableau, SSRS or Qlik\
{\listtext	\uc0\u8259 	}Familiarity with Microsoft Tabular models and the DAX language\
{\listtext	\uc0\u8259 	}Proficiency with common Microsoft Office tools: Word, Excel, PowerPoint, Outlook\
{\listtext	\uc0\u8259 	}Experience with Powershell or other scripting languages like Python, C#, VB is a plus\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Deep Learning\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}An alternative to deep learning can be Learning Vector Quantization (LVQ)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls12\ilvl2\cf0 {\listtext	\uc0\u8259 	}Signal Processing\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls12\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Signal_processing"}}{\fldrslt https://en.wikipedia.org/wiki/Signal_processing}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Matrices\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}scipy.sparse matrices vs numpy matrices?\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Feature Selection\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}Heatmaps\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Leaf-wise vs Level-wise Decision Tree Algorithmss\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}Leaf-Wise\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls12\ilvl2\cf0 {\listtext	\uc0\u8259 	}lightgbm\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Omnichannel\
{\listtext	
\f1 \uc0\u9642 
\f0 	}Deep Learning Libraries and their differences\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.guru99.com/deep-learning-libraries.html"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.guru99.com/deep-learning-libraries.html}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}SMOTE\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/smote"}}{\fldrslt https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/smote}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}How to Implement a Real World Usecase or Project for Data Science\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=Ur6tpb0eIkY"}}{\fldrslt https://www.youtube.com/watch?v=Ur6tpb0eIkY}}\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls12\ilvl2\cf0 {\listtext	\uc0\u8259 	}Do Kaggle Competitions\
{\listtext	\uc0\u8259 	}Also learn Django and Flask to actually be able to implement models into web applications\
{\listtext	\uc0\u8259 	}Learn deployment techniques\
{\listtext	\uc0\u8259 	}Pipelines\
{\listtext	\uc0\u8259 	}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}XGBoost, Lightgbm, and CatBoost\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1
\f3 \cf3 {\listtext	
\f1 \uc0\u8259 
\f3 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=V5158Oug4W8"}}{\fldrslt \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=V5158Oug4W8}}\expnd0\expndtw0\kerning0
\ul \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db"}}{\fldrslt https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db}}\
{\listtext	\uc0\u8259 	}these are are gradient boosting algorithms for decision trees\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls12\ilvl2\cf0 {\listtext	\uc0\u8259 	}xgboost\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls12\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://xgboost.readthedocs.io/en/latest/tutorials/index.html"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://xgboost.readthedocs.io/en/latest/tutorials/index.html}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\ls12\ilvl3
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}loss function term\
{\listtext	\uc0\u8259 	}regularization term\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls12\ilvl4\cf0 {\listtext	\uc0\u8259 	}xgboost has show to work very welling practice \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls12\ilvl2\cf0 {\listtext	\uc0\u8259 	}LightGBM\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls12\ilvl3\cf0 {\listtext	\uc0\u8259 	}written by Microsoft\
{\listtext	\uc0\u8259 	}much faster than xgboost\
{\listtext	\uc0\u8259 	}works about the same\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls12\ilvl2\cf0 {\listtext	\uc0\u8259 	}CatBoost \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls12\ilvl3\cf0 {\listtext	\uc0\u8259 	}great if you have a lot of categorical variables\
{\listtext	\uc0\u8259 	}uses something called mean target encoding ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=V5158Oug4W8"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=V5158Oug4W8}}, see around ~13:00)\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}L1 Regularization (Lasso Regression), L2 Regularization (Ridge Regression), Gaussian Prior\
{\listtext	
\f1 \uc0\u9642 
\f0 	}ROC AUC\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://stackabuse.com/understanding-roc-curves-with-python/#:~:targetText=AUC%E2%80%93ROC%20curve%20is%20the,bi%E2%80%93multi%20class%20classification%20problem.&targetText=A%20typical%20ROC%20curve%20has,This%20area%20covered%20is%20AUC."}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://stackabuse.com/understanding-roc-curves-with-python/#:~:targetText=AUC%E2%80%93ROC%20curve%20is%20the,bi%E2%80%93multi%20class%20classification%20problem.&targetText=A%20typical%20ROC%20curve%20has,This%20area%20covered%20is%20AUC.}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls12\ilvl2
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}One of the most commonly used metrics nowadays is AUC-ROC (Area Under Curve - Receiver Operating Characteristics) curve. ROC curves are pretty easy to understand and evaluate once there is a good understanding of confusion matrix and different kinds of errors.\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}To more intuitively understand why multicollinearity is a problem for estimating regression coefficients, this post provides the following explanation:\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Dendograms, Agglomerative Hierarchical Clustering\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.datanovia.com/en/lessons/agglomerative-hierarchical-clustering/#dendrogram"}}{\fldrslt https://www.datanovia.com/en/lessons/agglomerative-hierarchical-clustering/#dendrogram}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}fit_transform # ignores dependent var, {\field{\*\fldinst{HYPERLINK "https://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn"}}{\fldrslt https://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn}}\
{\listtext	
\f1 \uc0\u9642 
\f0 	}\
{\listtext	
\f1 \uc0\u9642 
\f0 	}GridSearchCV vs random search\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}learning rate \
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://medium.com/datadriveninvestor/an-introduction-to-grid-search-ff57adcc0998"}}{\fldrslt https://medium.com/datadriveninvestor/an-introduction-to-grid-search-ff57adcc0998}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Precision, Recall, and F1 Score\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/"}}{\fldrslt https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/}}\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"}}{\fldrslt https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html}} (great read)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls12\ilvl2\cf0 {\listtext	\uc0\u8259 	}Precision (P) is defined as the number of true positives (T_p) over the number of true positives plus the number of false positives (F_p)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls12\ilvl3\cf0 {\listtext	\uc0\u8259 	}P = T_p / (T_p + F_p)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls12\ilvl2\cf0 {\listtext	\uc0\u8259 	}Recall (R) is defined as the number of true positives (T_p) over the number of true positives plus the number of false negatives (F_n)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls12\ilvl3\cf0 {\listtext	\uc0\u8259 	}R = T_p / (T_p + F_n)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls12\ilvl2\cf0 {\listtext	\uc0\u8259 	}Precision and Recall are also related to the F1 score\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls12\ilvl3\cf0 {\listtext	\uc0\u8259 	}F1 = 2 * ( (P * R) / (P + R))\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls12\ilvl2\cf0 {\listtext	\uc0\u8259 	}Note that the precision may not decrease with recall. The definition of precision shows that lowering the threshold of a classifier may increase the denominator, by increasing the number of results returned. If the threshold was previously set too high, the new results may all be true positives, which will increase precision. \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls12\ilvl3\cf0 {\listtext	\uc0\u8259 	}For example,\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls12\ilvl4\cf0 {\listtext	\uc0\u8259 	}P = 10 / (10 + 1) = 10/11\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls12\ilvl2\cf0 {\listtext	\uc0\u8259 	}If the previous threshold was about right or too low, further lowering the threshold will introduce false positives, decreasing precision.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls12\ilvl3\cf0 {\listtext	\uc0\u8259 	}For example,\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls12\ilvl4\cf0 {\listtext	\uc0\u8259 	}P = 8 / (8 + 3) = 8/11\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls12\ilvl2\cf0 {\listtext	\uc0\u8259 	}Recall is defined as (see formula for R above), where T_p + F_n does not depend on the classifier threshold. This means that lowering the classifier threshold may increase recall, by increasing the number of true positive results. It is also possible that lowering the threshold may leave recall unchanged, while the precision fluctuates.\
{\listtext	\uc0\u8259 	}Precision-recall curves are typically used in binary classification to study the output of a classifier. In order to extend the precision-recall curve and average precision to multi-class or multi-label classification, it is necessary to binarize the output. One curve can be drawn per label, but one can also draw a precision-recall curve by considering each element of the label indicator matrix as a binary prediction (micro-averaging).\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}f1_score\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"}}{\fldrslt https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html}}\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/F1_score"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://en.wikipedia.org/wiki/F1_score}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}.ffill\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls12\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.geeksforgeeks.org/python-pandas-dataframe-ffill/"}}{\fldrslt https://www.geeksforgeeks.org/python-pandas-dataframe-ffill/}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}map in pandas\
{\listtext	
\f1 \uc0\u9642 
\f0 	}Neo4j\
{\listtext	
\f1 \uc0\u9642 
\f0 	}graphKnows\
{\listtext	
\f1 \uc0\u9642 
\f0 	}DecisionTreeRegressor \
{\listtext	
\f1 \uc0\u9642 
\f0 	}BaggingRegressor \
{\listtext	
\f1 \uc0\u9642 
\f0 	}RandomForestRegressor \
{\listtext	
\f1 \uc0\u9642 
\f0 	}ExtraTreesRegressor\
{\listtext	
\f1 \uc0\u9642 
\f0 	}AdaBoostRegressor\
{\listtext	
\f1 \uc0\u9642 
\f0 	}GradientBoostingRegressor\
{\listtext	
\f1 \uc0\u9642 
\f0 	}XGBRegressor\
{\listtext	
\f1 \uc0\u9642 
\f0 	}KerasRegressor\
{\listtext	
\f1 \uc0\u9642 
\f0 	}How to determine how many n_estimators we need for most of the regressors\
{\listtext	
\f1 \uc0\u9642 
\f0 	}Memory Usage Feature in Jupyter Notebook\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls13\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Survival Analysis\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls13\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://stats.idre.ucla.edu/stata/seminars/stata-survival/"}}{\fldrslt https://stats.idre.ucla.edu/stata/seminars/stata-survival/}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls14\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Data Engineering\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls14\ilvl1\cf0 {\listtext	\uc0\u8259 	}Goals of a Data Engineer\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls14\ilvl2\cf0 {\listtext	\uc0\u8259 	}Developing data pipelines (most undergrad courses don\'92t teach you how to do this)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls14\ilvl3\cf0 {\listtext	\uc0\u8259 	}Taking data from an operational system and moving it to something so it can be used by analysts and data scientists \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls14\ilvl2\cf0 {\listtext	\uc0\u8259 	}Manage tables and data sets for analysts and data scientists\
{\listtext	\uc0\u8259 	}Design with the product in mind\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls14\ilvl3\cf0 {\listtext	\uc0\u8259 	}What data is being tracked, what questions are the users going to be asking while they are using the product (for example, dashboards) \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls14\ilvl1\cf0 {\listtext	\uc0\u8259 	}Skills of a Data Engineer\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls14\ilvl2\cf0 {\listtext	\uc0\u8259 	}Data Modeling\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls14\ilvl3\cf0 {\listtext	\uc0\u8259 	}Relational Databases\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls14\ilvl4\cf0 {\listtext	\uc0\u8259 	}Note, databases prefer data in long format\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls14\ilvl3\cf0 {\listtext	\uc0\u8259 	}Data Warehouses\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls14\ilvl2\cf0 {\listtext	\uc0\u8259 	}Automation\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls14\ilvl3\cf0 {\listtext	\uc0\u8259 	}\uc0\u932 iming (isolated task/function at a specific time routinely)\
{\listtext	\uc0\u8259 	}Dependencies (for example, we want to make sure task/table A comes before task/table B)\
{\listtext	\uc0\u8259 	}Failures (you want to make sure you capture your failures like a notification system (email, conbon?)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls14\ilvl2\cf0 {\listtext	\uc0\u8259 	}ETL Development (there is another type called ELT, essentially Data Pipeline, means Extract Transformation Label) \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls14\ilvl3\cf0 {\listtext	\uc0\u8259 	}For Example, taking data from an operational system such as Workday (an HR system) or MySQL (if its a product), and moving this data into a data warehouse. The old school way is to use SQL Server, Oracle databases, etc as the data warehouse. The more modern way is Hadoop, Redshift, etc.\
{\listtext	\uc0\u8259 	}Also in ETL development, data engineers can also be in charged of implementing an analytical data layer on the data to aggregate it so it can be easily fed to dashboards.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls14\ilvl2\cf0 {\listtext	\uc0\u8259 	}Product Understanding (data is our product, we want to understand what the data scientists want)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls14\ilvl3\cf0 {\listtext	\uc0\u8259 	}Dashboards\
{\listtext	\uc0\u8259 	}Datasets\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls14\ilvl2\cf0 {\listtext	\uc0\u8259 	}Data engineers develop data pipelines to\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls14\ilvl3\cf0 {\listtext	\uc0\u8259 	}improve ease of data access\
{\listtext	\uc0\u8259 	}produce analytical layers for dashboards\
{\listtext	\uc0\u8259 	}clean up data (eliminate duplicate data, etc)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls14\ilvl4\cf0 {\listtext	\uc0\u8259 	}we want data scientists to know that every observation they get is that its accurate/valid)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls14\ilvl2\cf0 {\listtext	\uc0\u8259 	}Tools Used\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls14\ilvl3\cf0 {\listtext	\uc0\u8259 	}Pipeline Framework (SSIS (GUI, hard to customize with Python), Infromatica, Airflow (AirBnB\'92s version of a data pipeline management system, easy to customize)), Spark\
{\listtext	\uc0\u8259 	}SQL\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls14\ilvl4\cf0 {\listtext	\uc0\u8259 	}It is easier to hire someone who knows SQL instead of map/reduce in Hadoop, Hive, etc\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls14\ilvl3\cf0 {\listtext	\uc0\u8259 	}Python, Powershell, Linux and/or Java (Java is for map/reduce stuff)\
{\listtext	\uc0\u8259 	}Tableau (heavily used, easiest), D3.js (fun and customizable, but painful if you are not Java script fan), SSRS, etc.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls14\ilvl2\cf0 {\listtext	\uc0\u8259 	}How does a Data Engineer make an impact?\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls14\ilvl3\cf0 {\listtext	\uc0\u8259 	}Creating optimized pipelines\
{\listtext	\uc0\u8259 	}influencing\
{\listtext	\uc0\u8259 	}designing\
{\listtext	\uc0\u8259 	}maintaining\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls14\ilvl2\cf0 {\listtext	\uc0\u8259 	}Note, Business Intelligence and Data Engineering are now more of the same. \
{\listtext	\uc0\u8259 	}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls15\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Undersampling vs Oversampling\
{\listtext	
\f1 \uc0\u9642 
\f0 	}Neural Network Structures\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls15\ilvl1\cf0 {\listtext	\uc0\u8259 	}Feed Forward\
{\listtext	\uc0\u8259 	}Recurrent\
{\listtext	\uc0\u8259 	}LSTM\
{\listtext	\uc0\u8259 	}Neat\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls15\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Neural Network Training (Common ones)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls15\ilvl1\cf0 {\listtext	\uc0\u8259 	}Supervised\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls15\ilvl2\cf0 {\listtext	\uc0\u8259 	}Gradient Descent \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls15\ilvl3\cf0 {\listtext	\uc0\u8259 	}Back Propagation  \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls15\ilvl1\cf0 {\listtext	\uc0\u8259 	}Unpsupervised\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls15\ilvl2\cf0 {\listtext	\uc0\u8259 	}K-means\
{\listtext	\uc0\u8259 	}Generative Adversarial Nets\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls15\ilvl1\cf0 {\listtext	\uc0\u8259 	}Reinforcement\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls15\ilvl2\cf0 {\listtext	\uc0\u8259 	}Q-Learning\
{\listtext	\uc0\u8259 	}Brute Force\
{\listtext	\uc0\u8259 	}Genetic Algorithm\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\
\'97\'97\'97\'97\'97\
GENERAL\
\
\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls16\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Great Reads\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls16\ilvl1\cf0 {\listtext	\uc0\u8259 	}LinkedIn post by NABIH IBRAHIM BAWAZIR\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls16\ilvl2\cf0 {\listtext	\uc0\u8259 	}Here are some essential math/stats for #DataScience\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}Theory \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}1. Linear Algebra (Matrices, Vectors, Eigenvalues/Eigenvectors, Linear Transformations)  - {\field{\*\fldinst{HYPERLINK "https://lnkd.in/gMzkkup"}}{\fldrslt https://lnkd.in/gMzkkup}}\
{\listtext	\uc0\u8259 	}2. Basic Calculus (Derivatives & Integrals) - {\field{\*\fldinst{HYPERLINK "https://lnkd.in/gDg4Nsz"}}{\fldrslt https://lnkd.in/gDg4Nsz}}\
{\listtext	\uc0\u8259 	}3.  Optimization (Gradient Algorithms & Objective Functions) - {\field{\*\fldinst{HYPERLINK "https://lnkd.in/g_e9sJu"}}{\fldrslt https://lnkd.in/g_e9sJu}}\
{\listtext	\uc0\u8259 	}4. Inferential Statistics (Distributions, CLT, Hypothesis Testing, Erors, ANOVA, Chi-Square, T-Test)- {\field{\*\fldinst{HYPERLINK "https://lnkd.in/gbh3aRj"}}{\fldrslt https://lnkd.in/gbh3aRj}}\
{\listtext	\uc0\u8259 	}5.  Probability Theory (Random Variables, Types of Distributions, Sampling, CI)  - {\field{\*\fldinst{HYPERLINK "https://lnkd.in/gf6q8FN"}}{\fldrslt https://lnkd.in/gf6q8FN}}\
{\listtext	\uc0\u8259 	}6. Graph Theory (Trees, Nodes, Edges) - {\field{\*\fldinst{HYPERLINK "https://lnkd.in/gYUgBhA"}}{\fldrslt https://lnkd.in/gYUgBhA}}\
{\listtext	\uc0\u8259 	}7. Data Structures (Algorithms, Big-O, Sorting, Time Complexity) - {\field{\*\fldinst{HYPERLINK "https://lnkd.in/gHZEw3d"}}{\fldrslt https://lnkd.in/gHZEw3d}}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}If you want to apply Machine Learning on Business\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}1. Data Science Process {\field{\*\fldinst{HYPERLINK "https://lnkd.in/fMHtxYP"}}{\fldrslt https://lnkd.in/fMHtxYP}}\
{\listtext	\uc0\u8259 	}2. Data Visualization in Business {\field{\*\fldinst{HYPERLINK "https://lnkd.in/fYUCzgC"}}{\fldrslt https://lnkd.in/fYUCzgC}}\
{\listtext	\uc0\u8259 	}3. Understand How to answer Why {\field{\*\fldinst{HYPERLINK "https://lnkd.in/f396Dqg"}}{\fldrslt https://lnkd.in/f396Dqg}}\
{\listtext	\uc0\u8259 	}4. Know ML Key Terminology {\field{\*\fldinst{HYPERLINK "https://lnkd.in/fCihY9W"}}{\fldrslt https://lnkd.in/fCihY9W}}\
{\listtext	\uc0\u8259 	}5. Understand ML Implementation {\field{\*\fldinst{HYPERLINK "https://lnkd.in/f5aUbBM"}}{\fldrslt https://lnkd.in/f5aUbBM}}\
{\listtext	\uc0\u8259 	}6. ML Applications on Marketing {\field{\*\fldinst{HYPERLINK "https://lnkd.in/fUDGAQW"}}{\fldrslt https://lnkd.in/fUDGAQW}}\
{\listtext	\uc0\u8259 	}and Retail {\field{\*\fldinst{HYPERLINK "https://lnkd.in/fihPTJf"}}{\fldrslt https://lnkd.in/fihPTJf}}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}Definitely brush up on these concepts and you'll be great.\
{\listtext	\uc0\u8259 	}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls17\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Data Mining\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls17\ilvl1\cf0 {\listtext	\uc0\u8259 	}Overview\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls17\ilvl2\cf0 {\listtext	\uc0\u8259 	}Introduction to Data Mining in SQL Server Analysis Services by PASStv (fantastic lecture, {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=S_j3tDFnwO8"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=S_j3tDFnwO8}})\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls18\ilvl2\cf0 \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls19\ilvl1\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls20\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Overfitting occurs when there is greater accuracy seen with the training set than the test set\
{\listtext	
\f1 \uc0\u9642 
\f0 	}Anything model that is built with X_train and y_train is considered supervised learning. Any model but with just X_train is considered unsupervised learning.\
{\listtext	
\f1 \uc0\u9642 
\f0 	}Features are column-wise data \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls21\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Supervised Learning vs Unsupervised Learning vs Reinforcement Learning (note, I need to un indent this over)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls21\ilvl1\cf0 {\listtext	\uc0\u8259 	}The main difference between the two types is that supervised learning is done using a ground truth, or in other words, we have prior knowledge of what the output values for our samples should be.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls21\ilvl2\cf0 {\listtext	\uc0\u8259 	}Supervised Learning\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls21\ilvl3\cf0 {\listtext	\uc0\u8259 	}the goal of supervised learning is to learn a function that, given a sample of data and desired outputs, best approximates the relationship between input and output observable in the data.\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls21\ilvl4\cf0 {\listtext	\uc0\u8259 	}Classification or Regression\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls21\ilvl5\cf0 {\listtext	\uc0\u8259 	}Classification:\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls22\ilvl5\cf0 {\listtext	\uc0\u8259 	}Regression:\
{\listtext	\uc0\u8259 	}Common Algorithms:\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls22\ilvl6\cf0 {\listtext	\uc0\u8259 	}logistic regression\
{\listtext	\uc0\u8259 	}naive bayes\
{\listtext	\uc0\u8259 	}support vector machines\
{\listtext	\uc0\u8259 	}artificial neural networks\
{\listtext	\uc0\u8259 	}random forests\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls22\ilvl4\cf0 {\listtext	\uc0\u8259 	}In both regression and classification, the goal is to find specific relationships or structure in the input data that allow us to effectively produce correct output data. Note that \'93correct\'94 output is determined entirely from the training data, so while we do have a ground truth that our model will assume is true, it is not to say that data labels are always correct in real-world situations. Noisy, or incorrect, data labels will clearly reduce the effectiveness of your model. When conducting supervised learning, the main considerations are model complexity, and the bias-variance tradeoff. Note that both of these are interrelated.\
{\listtext	\uc0\u8259 	}When conducting supervised learning, the main considerations are model complexity, and the bias-variance tradeoff. Note that both of these are interrelated.\
{\listtext	\uc0\u8259 	}The bias-variance tradeoff also relates to model generalization. In any model, there is a balance between bias, which is the constant error term, and variance, which is the amount by which the error may vary between different training sets. So, high bias and low variance would be a model that is consistently wrong 20% of the time, whereas a low bias and high variance model would be a model that can be wrong anywhere from 5%-50% of the time, depending on the data used to train it. Note that bias and variance typically move in opposite directions of each other; increasing bias will usually lead to lower variance, and vice versa. When making your model, your specific problem and the nature of your data should allow you to make an informed decision on where to fall on the bias-variance spectrum. Generally, increasing bias (and decreasing variance) results in models with relatively guaranteed baseline levels of performance, which may be critical in certain tasks. Additionally, in order to produce models that generalize well, the variance of your model should scale with the size and complexity of your training data \'97 small, simple data-sets should usually be learned with low-variance models, and large, complex data-sets will often require higher-variance models to fully learn the structure of the data.\
{\listtext	\uc0\u8259 	}To train a neural network using supervised learning, \cf6 \cb10 gradient descent/back propagation\cf0 \cb1  is often used.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls22\ilvl4\cf0 {\listtext	\uc0\u8259 	}Unsupervised Learning \
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls22\ilvl4\cf0 {\listtext	\uc0\u8259 	}Does not have labeled outputs, so its goal is to infer the natural structure present within a set of data points\
{\listtext	\uc0\u8259 	}The most common tasks within unsupervised learning are clustering, representation learning, and density estimation. In all of these cases, we wish to learn the inherent structure of our data without using explicitly-provided labels. \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls22\ilvl5\cf0 {\listtext	\uc0\u8259 	}Common Algorithms:\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls22\ilvl6\cf0 {\listtext	\uc0\u8259 	}K-Means Clustering\
{\listtext	\uc0\u8259 	}Principal Component Analysis (PCA)\
{\listtext	\uc0\u8259 	}Autoencoders\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls22\ilvl5\cf0 {\listtext	\uc0\u8259 	}Some common use-cases for unsupervised learning are exploratory analysis (I thinkk, this is apriori) and dimensionality reduction (PCA, LDA)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls22\ilvl4\cf0 {\listtext	\uc0\u8259 	}To train a neural network using unsupervised learning, \cf6 \cb11 K-means\cf0 \cb1  and \cf6 \cb10 generative adversarial nets\cf0 \cb1  are often used\
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\cf0 	\uc0\u8259 	Reinforcement Learning\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls23\ilvl4\cf0 {\listtext	\uc0\u8259 	}To train a neural network using reinforcement learning, \cf6 \cb11 Q-Learning\cf0 \cb1  and \cf6 \cb10 brute-force\cf0 \cb1  and \cf6 \cb10 genetic algorithm\cf0 \cb1  are often used\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls24\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Decision Tree Terminology\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls24\ilvl1\cf0 {\listtext	\uc0\u8259 	}The very top of the tree is called the \'93Root Node\'94 or just \'93The Root\'94. It represents the entire population or sample, and this further gets divided into two or more homogeneous sets.  \
{\listtext	\uc0\u8259 	}\'93Internal/Child Nodes\'94 have arrows pointing to them and away from them\
{\listtext	\uc0\u8259 	}\'93Leaf Nodes\'94 only have arrows pointing to them \
{\listtext	\uc0\u8259 	}\'93Splitting\'94 is dividing the root node/sub node into different parts on the basis of some condition\
{\listtext	\uc0\u8259 	}\'93Pruning\'94 is the opposite of splitting, basically removing unwanted branches from the trees. In other words, pruning is a method of limiting tree depth to reduce overfitting in decision trees. There are two types of pruning:\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls24\ilvl2\cf0 {\listtext	\uc0\u8259 	}Pre-pruning: A decision tree involves setting the parameters of a decision tree before building  it. For example, setting the maximum tree depth, maximum number of terminal nodes, minimum samples for a node split (controls the size of the resultant terminal nodes), maximum number of features \
{\listtext	\uc0\u8259 	}Post-pruning: To post-prune, validate the performance of the model on a test set. Afterwards, cut back splits that seem to result from overfitting noise in the training set. Pruning these splits dampens the noise in the data set.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls24\ilvl3\cf0 {\listtext	\uc0\u8259 	}Post-pruning may result in overfitting the model and is currently not available in Pythons sklearn, but its available in R.\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls24\ilvl1\cf0 {\listtext	\uc0\u8259 	}\'93Branch/SubTree\'94 is formed by splitting the tree/node\
{\listtext	\uc0\u8259 	}\'93Entropy\'94 & \'93Information Gain\'94\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls24\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://homes.cs.washington.edu/~shapiro/EE596/notes/InfoGain.pdf"}}{\fldrslt https://homes.cs.washington.edu/~shapiro/EE596/notes/InfoGain.pdf}}\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=qDcl-FRnwSU&t=235s"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=qDcl-FRnwSU&t=235s}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul  
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone (great example, 27:00, 34:00)\
{\listtext	\uc0\u8259 	}Entropy\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls24\ilvl3\cf0 {\listtext	\uc0\u8259 	}Common way to measure impurity. This value is used in the information gain formula.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls24\ilvl2\cf0 {\listtext	\uc0\u8259 	}Information Gain \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls24\ilvl3\cf0 {\listtext	\uc0\u8259 	}Measures the reduction in entropy. Decides which attribute should be selected as the decision node. We select the attribute with the maximum gain to be the root node.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls24\ilvl2\cf0 {\listtext	\uc0\u8259 	}Also, in other words, which ever feature has the lowest Gini Impurity will be the one that provides the greatest information gain. Therefore, this feature will be selected as the node and so forth for the remaining nodes.\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls24\ilvl1\cf0 {\listtext	\uc0\u8259 	}Important decision trees can be constructed using binary data, continuous data, and categorical/ranked data:\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls24\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example with binary data (3 independent vars, 1 dependent var, all binary data (0,1))\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls24\ilvl3\cf0 {\listtext	\uc0\u8259 	}See @03:25  {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=7VeUPuFGJHk"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=7VeUPuFGJHk}}\
{\listtext	\uc0\u8259 	}To begin a decision tree, you calculate the Gini impurity using each independent var with the dependent var one at a time and keep track of all the true pos, false pos, false neg, true neg. Which ever feature shows the lowest 
\f2\i total
\f0\i0  Gini impurity is the one selected to start the root node. Next,  the 
\f2\i subsection 
\f0\i0 Gini impurity\'92s that were created for the \'93true pos, false pos\'94 block and \'93false neg, true neg\'94 block are the splits for that node. Now let\'92s start with going down the left side of the root node (the true pos, false pos) side.  We must compare the Gini impurity found in this section to the Gini impurities of the other features (note, these features have to fall under the left side of the of the root node too). If a smaller Gini impurity is found in one of the other features, then it becomes the node for this level of the tree. Therefore, in a sense, it overrides the \'93true pos, false pos\'94 section of the root node. \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls25\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example with continuous data (1 independent var (continuous data), 1 dependent var (binary data))\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls25\ilvl3\cf0 {\listtext	\uc0\u8259 	}See @13:57  {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=7VeUPuFGJHk"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=7VeUPuFGJHk}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\ls25\ilvl3
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}To begin a decision tree with the independent var as continuous, you first have to sort the column data (eg, 125, 135, 155, and so forth (make sure the dependent vars stay with the appropriate ones)). Then you take an average between each observation (130, 145). From there, you then start calculating the 
\f2\i subsection
\f0\i0  Gini Impurities, and then get the 
\f2\i total
\f0\i0  Gini impurity. See the link directly above.
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls26\ilvl2
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}Example with ranked/categorical data (1 independent var (categorical data), 1 dependent var (binary data))\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls26\ilvl3\cf0 {\listtext	\uc0\u8259 	}See @15:25  {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=7VeUPuFGJHk"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=7VeUPuFGJHk}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\ls26\ilvl3
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}Need to revisit\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls27\ilvl1\cf0 {\listtext	\uc0\u8259 	}Advantages: Decision trees are easy to interpret. To build a decision tree requires little data preparation from the user - there is no need to normalize the data\
{\listtext	\uc0\u8259 	}Disadvantages: Decision trees are likely to overfit noisy data. The probability of overfitting on noise increases as a tree gets deeper.\
{\listtext	\uc0\u8259 	}Ensembles\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls27\ilvl2\cf0 {\listtext	\uc0\u8259 	}Creating ensembles involves aggregating the results of different models. Ensemble decision trees are used in bagging and random forests while ensemble regression trees are used in boosting\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls27\ilvl1\cf0 {\listtext	\uc0\u8259 	}Bagging/Bootstrap aggregating\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls27\ilvl2\cf0 {\listtext	\uc0\u8259 	}Bagging involves creating multiple decision trees each trained on a different bootstrap sample of the data. Because bootstrapping involves samples with replacement, some of the data in the sample is left out of each tree.\
{\listtext	\uc0\u8259 	}Consequently, the decision trees created are made using different samples which solves the problem of overfitting to the training sample. Ensembling decision trees in this way helps reduce the total error because variance of the model continues to decrease with each new tree added without an increase in the bias of the ensemble.\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls27\ilvl1\cf0 {\listtext	\uc0\u8259 	}Random Forest\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls27\ilvl2\cf0 {\listtext	\uc0\u8259 	}A bag of decision trees that uses subspace sampling is referred to as a random forest. Only a selection of the features is considered at each node split which decor relates the trees in the forest.\
{\listtext	\uc0\u8259 	}Another advantage of random forests is that they have an in-built validation mechanism. Because only a percentage of the data is used for each model, an out-of-bag error of the moles performance can be calculated using the 37% of the sample left out of each model\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls27\ilvl1\cf0 {\listtext	\uc0\u8259 	}Boosting	\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls27\ilvl2\cf0 {\listtext	\uc0\u8259 	}Boosting involves aggregating a collection of weak learners(regression trees) to form a strong predictor. Ad boosted model is built over time adding a new tree into the model that minimizes the error by previous learners. This is done by fitting the end tree on the residuals of the previous trees.\
{\listtext	\uc0\u8259 	}If it isn\'92t clear this far, for many real-world applications a single decision tree is not a preferable classification as it is likely to overfit and generalize very poorly to new examples. However, an ensemble of decision or regression trees minimizes the overfitting disadvantage and these models become stellar, state of the art classification and regression algorithms.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls28\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Model Selection\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls28\ilvl1\cf0 {\listtext	\uc0\u8259 	}Regression\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls28\ilvl2\cf0 {\listtext	\uc0\u8259 	}Cross-Validation\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls28\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d}}\
{\listtext	\uc0\u8259 	}MAE\
{\listtext	\uc0\u8259 	}RSME\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls28\ilvl1\cf0 {\listtext	\uc0\u8259 	}Classification\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls28\ilvl2\cf0 {\listtext	\uc0\u8259 	}Confusion Matrix\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls28\ilvl1\cf0 {\listtext	\uc0\u8259 	}Selecting the best model in scikit-learn using cross-validation\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls28\ilvl2\cf0 {\listtext	\uc0\u8259 	}What is the drawback of using the train/test split procedure for model evaluation?\
{\listtext	\uc0\u8259 	}How does K-fold cross-validation overcome this limitation?\
{\listtext	\uc0\u8259 	}How can cross validation be used for selecting tuning parameters, choosing between models, and selecting features?\
{\listtext	\uc0\u8259 	}What are some possible improvements to cross-validation?\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls28\ilvl1\cf0 {\listtext	\uc0\u8259 	}K Fold Validation\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls28\ilvl2\cf0 {\listtext	\uc0\u8259 	}This is used to over come importing just one random state and testing it. For example, if cv=10 is set in the cross_val_score model, 10 increments will be tested as the test set with the remaining data being used as the training set each time.\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls28\ilvl1\cf0 {\listtext	\uc0\u8259 	}Cross Validation\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls28\ilvl2\cf0 {\listtext	\uc0\u8259 	}Machine Learning Fundamentals: Cross Validation by StatQuest with Josh Starmer ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=fSytzGwwBVw"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=fSytzGwwBVw}}, great video)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\'97\'97\'97\'97\'97  \
DATA PREPROCESSING\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls29\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Terminology\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls29\ilvl1\cf0 {\listtext	\uc0\u8259 	}feature scaling/normalization/\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls29\ilvl2\cf0 {\listtext	\uc0\u8259 	}Checking for NaN values in Pandas\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls29\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe}}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls29\ilvl1\cf0 {\listtext	\uc0\u8259 	}Data Transformation\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls29\ilvl2\cf0 {\listtext	\uc0\u8259 	}Why should we transform data when it is already clean?\
{\listtext	\uc0\u8259 	}Different features in the data set may have values in different ranges. For example, in an employee data set, the range of salary feature may lie from thousands to lakhs but the range of values of age feature will be in 20- 60. That means a column is more weighted compared to other.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls29\ilvl3\cf0 {\listtext	\uc0\u8259 	}Two most common methods for normalization:\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls29\ilvl4\cf0 {\listtext	\uc0\u8259 	}Min-Max\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls29\ilvl5\cf0 {\listtext	\uc0\u8259 	}Min- Max tries to get the values closer to mean. But when there are outliers in the data which are important and we don\'92t want to loose their impact ,we go with Z score normalization.\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls29\ilvl4\cf0 {\listtext	\uc0\u8259 	}Z score\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls29\ilvl3\cf0 {\listtext	\uc0\u8259 	}Skewness of data:\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls29\ilvl4\cf0 {\listtext	\uc0\u8259 	}According to Wikipedia,\'94 In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean.\'94\
{\listtext	\uc0\u8259 	}Generally, if the skewness value lies above +1 or \cf12 BELOW\cf0  -1, data is highly skewed. If it lies between +0.5 to -0.5, it is moderately skewed. If the value is 0, then the data is symmetric\
{\listtext	\uc0\u8259 	}It is also important to make sure the absolute value of the skewness is greater than twice the std error value (04:45, {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=_c3dVTRIK9c"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=_c3dVTRIK9c}})\
{\listtext	\uc0\u8259 	}NOTE, we want to be very careful when sampling data sets if the results are highly skewed. For example, in the fraud analysis I am going through {\field{\*\fldinst{HYPERLINK "https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul  
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone there are 99.83% of cases that are not fraud and 0.17 % of cases that are. Therefore, if we did sampling with this dataset, we may not get any of the fraud cases in our training set.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls29\ilvl3\cf0 {\listtext	\uc0\u8259 	}Heat maps\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls29\ilvl4\cf0 {\listtext	\uc0\u8259 	}are great for seeing correlated variables (<myDataFrameName1>.corr() \'97> with the Pandas dataframe, this gives a print out of all the correlation coefficients).\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls29\ilvl3\cf0 {\listtext	\uc0\u8259 	}Copy a dataframe\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls29\ilvl4\cf0 {\listtext	\uc0\u8259 	}temp3 = <myDataFrameName1>.copy() # our dataframe\
{\listtext	\uc0\u8259 	}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls29\ilvl4\cf0 {\listtext	\uc0\u8259 	}Data Frame Types\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls29\ilvl1\cf0 {\listtext	\uc0\u8259 	}Calling <myDataFrameName1>.info() will show us information related to the Dataframe \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls29\ilvl2\cf0 {\listtext	\uc0\u8259 	}\'93Object\'94 are typically string values\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 	\'95	Imputer\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 	\uc0\u8259 	import Imputes module to handle missing data\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls30\ilvl0\cf0 {\listtext	\uc0\u8226 	}Encoding variables\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls30\ilvl1\cf0 {\listtext	\uc0\u8259 	}Terminology\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls30\ilvl2\cf0 {\listtext	\uc0\u8259 	}Same Thing\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls30\ilvl3\cf0 {\listtext	\uc0\u8259 	}Dummy Encoding (if you are coming from the statistics field) \
{\listtext	\uc0\u8259 	}One Hot Encoding (if you are coming from the computer science or electrical engineering field) \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 	\uc0\u8259 	eg, male and female into 1 and 0\
	\uc0\u8259 	eg, France, Spain, and Germany into 0, 2,and 1 (note, we have to be careful of the dummy variable trap here. We must use OneHotEncoder to create ONLY TWO new columns that show only 1 and 0\'92s). We do not need three columns for each country. We need one minus for some reason. I think this has something to do with the degrees of freedom. We are avoiding what is called the Dummy Variable Trap.\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls31\ilvl0\cf0 {\listtext	\uc0\u8226 	}Feature scaling\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls31\ilvl1\cf0 {\listtext	\uc0\u8259 	}5.3.1.3. Scaling data with outliers\
{\listtext	\uc0\u8259 	}If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use robust_scale and RobustScaler as drop-in replacements instead. They use more robust estimates for the center and range of your data. (see {\field{\*\fldinst{HYPERLINK "https://scikit-learn.org/stable/modules/preprocessing.html#scaling-data-with-outliers"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://scikit-learn.org/stable/modules/preprocessing.html#scaling-data-with-outliers}})\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\tx10256\li1440\fi-1440\pardirnatural\partightenfactor0
\ls32\ilvl1\cf0 {\listtext	\uc0\u8259 	}Feature extraction is very different from Feature selection: the former (feature extraction) consists in transforming arbitrary data, such as text or images, into numerical features usable for machine learning. The latter is a machine learning technique applied on these features. \
{\listtext	\uc0\u8259 	}Examples of Algorithms where Feature Scaling matters \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\tx10256\li2160\fi-2160\pardirnatural\partightenfactor0
\ls32\ilvl2\cf0 {\listtext	\uc0\u8259 	}K-Means uses the Euclidean distance measure here feature scaling matters.\
{\listtext	\uc0\u8259 	}K-Nearest-Neighbours also require feature scaling.\
{\listtext	\uc0\u8259 	}Principal Component Analysis (PCA): Tries to get the feature with maximum variance, here too feature scaling is required.\
{\listtext	\uc0\u8259 	}Gradient Descent: Calculation speed increase as Theta calculation becomes faster after feature scaling.\
{\listtext	\uc0\u8259 	}Note: Naive Bayes, Linear Discriminant Analysis, and Tree-Based models are not affected by feature scaling. In Short, any Algorithm which is Not Distance based is Not affected by Feature Scaling.\
\pard\tx720\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\tx10256\pardirnatural\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls33\ilvl0\cf0 {\listtext	\uc0\u8226 	}Good Reads\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls33\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://stats.stackexchange.com/questions/189652/is-it-a-good-practice-to-always-scale-normalize-data-for-machine-learning"}}{\fldrslt https://stats.stackexchange.com/questions/189652/is-it-a-good-practice-to-always-scale-normalize-data-for-machine-learning}}\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://stats.stackexchange.com/questions/342140/standardization-of-continuous-variables-in-binary-logistic-regression"}}{\fldrslt https://stats.stackexchange.com/questions/342140/standardization-of-continuous-variables-in-binary-logistic-regression}} (eg, 2 regressors are continuous and 2 are binary)\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://datascience.stackexchange.com/questions/20237/why-do-we-convert-skewed-data-into-a-normal-distribution"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://datascience.stackexchange.com/questions/20237/why-do-we-convert-skewed-data-into-a-normal-distribution}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls33\ilvl0\cf0 {\listtext	\uc0\u8226 	}Parametric and Nonparametric: Demystifying the Terms\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls33\ilvl1\cf0 {\listtext	\uc0\u8259 	}Parametric\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls33\ilvl2\cf0 {\listtext	\uc0\u8259 	}We have to normally distributed datasets (not positively or negatively skewed)\
{\listtext	\uc0\u8259 	}Common transformations of this data include square root, cube root, and log10.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls33\ilvl3\cf0 {\listtext	\uc0\u8259 	}The cube root transformation involves converting x to x^(1/3). This is a fairly strong transformation with a substantial effect on distribution shape: but is weaker than the logarithm. It can be applied to negative and zero values too. Negatively skewed data.\
{\listtext	\uc0\u8259 	}The square root transformation, x^2 can only be applied to positive values only. Hence, observe the values of column before applying.\
{\listtext	\uc0\u8259 	}The logarithm transformation , x to log base 10 of x, or x to log base e of x (ln x), or x to log base 2 of x, is a strong transformation and can be used to reduce right skewness (positively skewed).\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls33\ilvl2\cf0 {\listtext	\uc0\u8259 	}If tail is on the right as that of the second image in the figure, it is right skewed data. It is also called positive skewed data.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls33\ilvl3\cf0 {\listtext	\uc0\u8259 	}In order to fix a positively skewed distribution using a log10 distribution, the following assumptions have to be met:\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls33\ilvl4\cf0 {\listtext	\uc0\u8259 	}no negative values, no zeroes, and the positively skewed (if you have negative values or zeros, see around ~08:00 mins, {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=_c3dVTRIK9c"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=_c3dVTRIK9c}})\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls33\ilvl2\cf0 {\listtext	\uc0\u8259 	} If the tail is to the left of data, then it is called left skewed data. It is also called negatively skewed data.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls33\ilvl3\cf0 {\listtext	\uc0\u8259 	}Similarly, to fix a negatively skewed distribution using a log10 distribution, the same conditions have to be met:\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls33\ilvl4\cf0 {\listtext	\uc0\u8259 	}but this time we call the log10 function as \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls33\ilvl5\cf0 {\listtext	\uc0\u8259 	}log10(max(x) - 1 + x)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls33\ilvl2\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls33\ilvl0\cf0 {\listtext	\uc0\u8226 	}Resolving outliers\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls33\ilvl1\cf0 {\listtext	\uc0\u8259 	}Anomaly Detection (best seen from boxplot)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls33\ilvl2\cf0 {\listtext	\uc0\u8259 	}Interquatile Range Method\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls33\ilvl0\cf0 {\listtext	\uc0\u8226 	}Aggregation of data\
{\listtext	\uc0\u8226 	}Sequence Data\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls33\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9"}}{\fldrslt https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9}}\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls33\ilvl2\cf0 {\listtext	\uc0\u8259 	}For example, if we want to get information about which way a ball is moving, we will need to take a few snapshots over time. These snapshots are a form of sequence data.\
{\listtext	\uc0\u8259 	}Text is another form of sequence data\
{\listtext	\uc0\u8259 	}Audio files are another form of sequence data. We can chop a file into 5 \
\pard\tx720\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\tx10256\pardirnatural\partightenfactor0
\cf0 \
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\
CLASSIFICATION\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls34\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Decision Tree Classification\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls34\ilvl1\cf0 {\listtext	\uc0\u8259 	}graphical representation of all the possible solutions to a decision\
{\listtext	\uc0\u8259 	}decisions are based on some conditions \
{\listtext	\uc0\u8259 	}decisions made can be easily explained\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls34\ilvl2\cf0 {\listtext	\uc0\u8259 	}\uc0\u917 xample\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls34\ilvl3\cf0 {\listtext	\uc0\u8259 	}Independent variables: \'93Am I hungry?\'94     \'93Do I have $25?\'94\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls34\ilvl4\cf0 {\listtext	\uc0\u8259 	}Data is in binary form (yes or no)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls34\ilvl3\cf0 {\listtext	\uc0\u8259 	}Dependent variable: \'93Outcome\'94\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls34\ilvl4\cf0 {\listtext	\uc0\u8259 	}Data is \'93Go to restaurant\'94 \'93Buy a hamburger\'94 \'93Go to sleep\'94 (this is a multiple classification problem)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls34\ilvl3\cf0 {\listtext	\uc0\u8259 	}NOTE: When building a decision tree you first have to individually compare each independent variable to the dependent variable in order to determine which independent variable will begin the decision tree (classification problem, yes or no for all results for the independent and dependent variable, you keep track of which independent variables that has the lowest \'93Gini Impurity\'94 (aka the one that is most pure) and you begin with that one at the top of the tree.. See {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=7VeUPuFGJHk&list=PLsCzljdLz2iR66dIIpz9E9veNj7wijPS7"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=7VeUPuFGJHk&list=PLsCzljdLz2iR66dIIpz9E9veNj7wijPS7 }}. I thinkkk you can also look at the true pos, false pos, true neg, false neg. \
{\listtext	\uc0\u8259 	}Important: Decision trees for classification use Gini Impurity and/or Information Gain. Decision trees for regression use Standard Deviation Reduction (see {\field{\*\fldinst{HYPERLINK "http://saedsayad.com/decision_tree_reg.htm"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul http://saedsayad.com/decision_tree_reg.htm}}).\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls35\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Random Forest Classification\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls35\ilvl1\cf0 {\listtext	\uc0\u8259 	}Builds multiple decision trees and merges them together\
{\listtext	\uc0\u8259 	}more accurate and stable prediction\
{\listtext	\uc0\u8259 	}random decision forests correct for decision trees\'92 habit of overfitting to their training set\
{\listtext	\uc0\u8259 	}trained with the \'93bagging\'94 method\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 	
\f1 \uc0\u9642 
\f0 	Na\'efve Bayes Classification\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls36\ilvl1\cf0 {\listtext	\uc0\u8259 	}Classification technique based on Bayes\'92 Theorem\
{\listtext	\uc0\u8259 	}Assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0  \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 	
\f1 \uc0\u9642 
\f0 	K Nearest Neighbors Classifier\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls37\ilvl1\cf0 {\listtext	\uc0\u8259 	}Stores all the available cases and classifies new cases based on a similarity measure\
{\listtext	\uc0\u8259 	}The \'93K\'94 is the KNN algorithm is the nearest neighbors we wish to take a vote from\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls37\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example:\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls37\ilvl3\cf0 {\listtext	\uc0\u8259 	}Imagine a scatter plot with real training data on it representing 5 classes. Each dot on the scatter plot is a color. Generally, the color dots in this training set data are grouped by similar color dots. So visually, you can see five classes. Now, I think the KNN algorithm builds the model by looking at super small segments of the plot and starts a computational radius going outward from each segment. So if K is specified to be \'933.\'94 The computational radius will stop expanding for each segment when it hits three dots of the same color and will give that segment of the plot the appropriate color.\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls37\ilvl0\cf0 {\listtext	\uc0\u8259 	}from lightgbm import LightGBMClassifier\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\'97\
REGRESSION\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls38\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Decision Tree Regression\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls38\ilvl1\cf0 {\listtext	\uc0\u8259 	}For example, when visualizing two independent variables on a scatter plot, a decision tree algorithm \'93splits\'94 your dataset based off of information entropy (need to look up more) and based off of these splits is how the tree is made.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls38\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.saedsayad.com/decision_tree_reg.htm"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.saedsayad.com/decision_tree_reg.htm }}(example)
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul  \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls38\ilvl1
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}Important: Decision trees for classification use Gini Impurity and/or Information Gain. Decision trees for regression use Standard Deviation Reduction (see {\field{\*\fldinst{HYPERLINK "http://saedsayad.com/decision_tree_reg.htm"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul http://saedsayad.com/decision_tree_reg.htm}}).\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls39\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Random Forest Regression\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls39\ilvl1\cf0 {\listtext	\uc0\u8259 	}a form of ensemble learning, ensemble learning is when you take the same algorithm multiple times to make your model much more powerful than the original. Gradient boosting is a form of ensemble learning.\
{\listtext	\uc0\u8259 	}Not very sensitive to hyperparameters (need to verify still)\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls39\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Support Vector Regression\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls39\ilvl1\cf0 {\listtext	\uc0\u8259 	}Support Vector Machines by StatQuest {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=efR1C6CvhmE"}}{\fldrslt https://www.youtube.com/watch?v=efR1C6CvhmE}} (fantastic video)\
{\listtext	\uc0\u8259 	}Support Vector Machine (SVM) by Augmented Startups {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=Y6RRHw9uN9o&t=366s"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=Y6RRHw9uN9o&t=366s}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul  
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone (great video)
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls39\ilvl2
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}Dog and cat example. X axis there is snout length and on the Y axis there is ear length. SVR algorithm looks at the outliers within the data set and applies a linear linear / nonlinear line according to them. These outliers are none as support vectors. Note, in the example in the video the data is separable between the cat and dog classes.\
{\listtext	\uc0\u8259 	}When using Support Vector algorithms it is often helpful to map your vectors to a higher dimension in order to fully (better?) separate the data. However, this is often computationally expensive.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls39\ilvl3\cf0 {\listtext	\uc0\u8259 	}See Kernel function or Kernel Trick. The dot product can be computed to project the vectors into a higher dimension.\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls40\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Logistic Regression\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls40\ilvl1\cf0 {\listtext	\uc0\u8259 	}Note, logistic regression is probably better placed under CLASSIFICATION.\
{\listtext	\uc0\u8259 	}Great explanation of how to interpret the coefficients in a logistic or linear regression model (see ~02:48:00, {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=5rNu16O3YNE&list=PLsCzljdLz2iR9pnDlZkTqSvbUZqZenhcK&index=5"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=5rNu16O3YNE&list=PLsCzljdLz2iR9pnDlZkTqSvbUZqZenhcK&index=5}})\
{\listtext	\uc0\u8259 	}Has to do only with probability. Think about example of where a bank decides they should allow you to get a loan or not depending on your credit score, income, age, etc..\
{\listtext	\uc0\u8259 	}Or think about an image data set of digits where the target variable is 0-9. You can do classification binary or multi class problems with LogisticRegression. \
{\listtext	\uc0\u8259 	}Not very sensitive to hyperparameters (need to verify still)\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls40\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Linear Regression\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls40\ilvl1\cf0 {\listtext	\uc0\u8259 	}Great explanation of how to interpret the coefficients in a logistic or linear regression model (see ~02:48:00, {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=5rNu16O3YNE&list=PLsCzljdLz2iR9pnDlZkTqSvbUZqZenhcK&index=5"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=5rNu16O3YNE&list=PLsCzljdLz2iR9pnDlZkTqSvbUZqZenhcK&index=5}})\
{\listtext	\uc0\u8259 	}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\'97\'97\'97\'97\'97\
CLUSTERING:\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls41\ilvl0\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls42\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Hierarchial Clustering\
{\listtext	
\f1 \uc0\u9642 
\f0 	}Clustering\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls42\ilvl1\cf0 {\listtext	\uc0\u8259 	}Example Applications\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls42\ilvl2\cf0 {\listtext	\uc0\u8259 	}Fraud detection, SkyNet (the more it learns, the smarter it becomes) \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls42\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls42\ilvl1\cf0 {\listtext	\uc0\u8259 	}K-Means\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls42\ilvl2\cf0 {\listtext	\uc0\u8259 	}Very good for discovering categories or groups in your data that you may of not been able to recognize yourself.\
{\listtext	\uc0\u8259 	}Can work for multiple dimension problems\
{\listtext	\uc0\u8259 	}How it works (imagine a 2D scatter plot with a bunch of gray markers and we want to separate our data into 3 clusters (red, green, and blue)):\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls42\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.udemy.com/course/machinelearning/learn/lecture/5714416#overview"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.udemy.com/course/machinelearning/learn/lecture/5714416#overview}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul   
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone (great example)\
{\listtext	\uc0\u8259 	}Step 1. Choose the number K of clusters\
{\listtext	\uc0\u8259 	}Step 2. Select 
\f4\b at random
\f0\b0  K points.  The centroids (they don\'92t not necessarily have to be from or around your dataset).\
{\listtext	\uc0\u8259 	}Step 3. Assign each data point on the plot to the closest centroid that will in result form K clusters (generally in the form of Euclidean distance (there are other types, need to look up))\
{\listtext	\uc0\u8259 	}Step 4. Compute and place the new centroid of each cluster.\
{\listtext	\uc0\u8259 	}Step 5. Reassign each data point to the new closest centroid. If any reassignment took place, go back to Step 4, otherwise go to finish.\
{\listtext	\uc0\u8259 	}This is an iterative process.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls42\ilvl2\cf0 {\listtext	\uc0\u8259 	}The algorithm runs quickly by drawing a straight line connecting to each centroid and then drawing a perpendicular line from that line easily separate the data set and see which data points are closest (well, Im not sure if that\'92s how the algorithm actually runs, but that is a great visual way to think about it, this would be really good to show on a technical presentation).\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 	
\f1 \uc0\u9642 
\f0 	Sequence Clustering (all about ORDER)\
		\uc0\u8259 	Example Applications\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls43\ilvl2\cf0 {\listtext	\uc0\u8259 	}If you are manufacturing something and a product is going down the line and the fault is happening here you can predict it with sequence clustering\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\
MACHINE LEARNING ALGORITHMS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls44\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}A/B Testing\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls44\ilvl1\cf0 {\listtext	\uc0\u8259 	}Example, comparing which landing page of a website performs best.\
{\listtext	\uc0\u8259 	}A/A tests should be conducted first to make sure there\'92s nothing wrong on the backend such as if the data is messy, sampling problem because not randomizing properly, or too much noise.\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls44\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}KNeighborsRegressor (aka KNN)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls44\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html#sphx-glr-auto-examples-neighbors-plot-regression-py"}}{\fldrslt https://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html#sphx-glr-auto-examples-neighbors-plot-regression-py}}\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"}}{\fldrslt https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm}} (great explanation of the weights parameter)\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls44\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}DecisionTreeRegressor (aka CART)\
{\listtext	
\f1 \uc0\u9642 
\f0 	}SVM\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls44\ilvl1\cf0 {\listtext	\uc0\u8259 	}C-Parameter:\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls44\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel"}}{\fldrslt https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel}}     see Kent Munthe Caspersen answer\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls44\ilvl3\cf0 {\listtext	\uc0\u8259 	}\'93In general, having few training instances and many attributes make it easier to make a linear separation of the data.\'94\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls44\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}xgboost\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls44\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://xgboost.readthedocs.io/en/latest/tutorials/index.html"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://xgboost.readthedocs.io/en/latest/tutorials/index.html}}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\'97\
TIME SERIES\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls45\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Example Applications\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls45\ilvl1\cf0 {\listtext	\uc0\u8259 	}Predicting your home value years from now, stock ticket price, budget, manufacturing (eg, comparing your projected inventory levels and sale levels), calls coming into a call center and how many workers you need on the floor)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\'97\'97\'97\'97\'97\
ASSOCIATED RULE LEARNING\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls46\ilvl0\cf0 {\listtext	\uc0\u8226 	}Apriori Algorithm (think of grocery cart example \'97> diapers and beer relationship)\
{\listtext	\uc0\u8226 	}FP Growth Model\
{\listtext	\uc0\u8226 	}Business Examples\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls46\ilvl1\cf0 {\listtext	\uc0\u8259 	}Netflix, Amazon, If you look at this article you might also like this article, If you like this video you might also like this video, Grocery shopping (Beer and Diaper example), and et al.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\
REINFORCEMENT LEARNING\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls47\ilvl0\cf0 {\listtext	\uc0\u8226 	}Upper Confidence Bound Algorithm (\cf9 REVISIT 2019-08-10 21:33:37, DONT UNDERSTAND FULLY\cf0 )\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls47\ilvl1\cf0 {\listtext	\uc0\u8259 	}Multi Armed Bandit Problem\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls48\ilvl0\cf0 {\listtext	\uc0\u8226 	}Thompson Sampling\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls48\ilvl1\cf0 {\listtext	\uc0\u8259 	}Multi Armed Bandit Problem\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 	\'95	Important Terminology\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 	\uc0\u8259 	Explore vs. Exploitation\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\
METRICS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls49\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Use the sklearn.metrics to create a confusion matrix to analyze true negatives, false negatives, true positives, and false positives\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls50\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}See the section Model Selection & Boosting below for a better way to look at the performance of your models (GridSearchCV)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls50\ilvl1\cf0 {\listtext	\uc0\u8259 	}Note, You have to fit your data before you can get the best parameter combination using GridSearchCV. \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls51\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}{\field{\*\fldinst{HYPERLINK "https://www.theanalysisfactor.com/assessing-the-fit-of-regression-models/"}}{\fldrslt https://www.theanalysisfactor.com/assessing-the-fit-of-regression-models/}}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls51\ilvl1\cf0 {\listtext	\uc0\u8259 	}Lower values of RMSE indicate better fit.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\
NATURAL LANGUAGE PROCESSING\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls52\ilvl0\cf7 \
{\listtext	\uc0\u8226 	}rule learning\
{\listtext	\uc0\u8226 	}sparsity\
{\listtext	\uc0\u8226 	}sparse matrices\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls52\ilvl0\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\'97\
DEEP LEARNING\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 	\'95	Important Terminology\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 	\uc0\u8259 	Output values of the neural network can either be continuous (eg, price), binary (eg, yes/no), or categorical\
	\uc0\u8259 	weighted sum, activation function (sigmoid, threshold, rectifier, hyperbolic tangent)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls53\ilvl1\cf0 {\listtext	\uc0\u8259 	}Think about churn modeling problem (trying to predict which customers will leave the bank or not)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 	\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\
ARTIFICIAL NEURAL NETWORKS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls54\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}NEURAL NETWORKS 	\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls54\ilvl1\cf0 {\listtext	\uc0\u8259 	}Additional Reading\
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\sl280\partightenfactor0
\ls55\ilvl2
\f3 \cf3 {\listtext	
\f1 \uc0\u8259 
\f3 	}{\field{\*\fldinst{HYPERLINK "https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications"}}{\fldrslt \expnd0\expndtw0\kerning0
\ul https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications}}
\f0 \cf0 \
\pard\tx720\tx1440\tx2160\pardeftab720\sl280\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 		\'95	How do neural networks learn?\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 		\uc0\u8259 	You can either hard code them or let them learn on their own. We are always 	trying to let the models learn on their own.\
		\uc0\u8259 	Basically the only thing we have control over in the programming are the weights\
		\uc0\u8259 	Batch gradient descent (think of the smooth parabolic cost function curve with the 	ball bouncing back and forth trying to find the minimum)\
		\uc0\u8259 	Stochastic gradient descent (this method is used to find the global minimum of a 	rough shaped parabolic cost function curve) - this method is faster than batch 	(there is also a mini-batch method)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls56\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://iamtrask.github.io/2015/07/27/python-network-part2/"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://iamtrask.github.io/2015/07/27/python-network-part2/}}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 		\'95	Important Terminology\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 		\uc0\u8259 	Back propagation (the process of optimizing the cost function (aka adjusting the 	weights to find optimal value to agree with the known value))\
		\uc0\u8259 	Normally the rectifier function is used for the development of the hidden layers 	and for the output layer the sigmoid function is used.\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls57\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}CONVOLUTIONAL NEURAL NETWORKS \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls58\ilvl1\cf0 {\listtext	\uc0\u8259 	}Think about the following images: duck or rabbit,  man looking at you or away from you, and image of the girl with duplicate facial features (eyes, nose, mouth, eyebrows)\
{\listtext	\uc0\u8259 	}Steps \'97> Convolution, Max Pooling, Flattening, and Full Connection\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls58\ilvl2\cf0 {\listtext	\uc0\u8259 	}\ul Convolution\ulnone : The key take away of what convolution is, is to find features in your image by using a feature detector (think of a feature detector as a specific feature that distinguishes an image from the other images) and putting them into a feature map. Then from the feature map, it preserves the spatial relationships between pixels\
{\listtext	\uc0\u8259 	}Convolution (Cont.): ReLU Layer \'97> we introduce this Rectifier Linear Units algorithm to break up linearity.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls58\ilvl3\cf0 {\listtext	\uc0\u8259 	}Good read on convolution filters:  {\field{\*\fldinst{HYPERLINK "http://www.roborealm.com/help/Convolution.php"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul http://www.roborealm.com/help/Convolution.php}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul   
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls58\ilvl2\cf0 {\listtext	\uc0\u8259 	}\ul Max Pooling\ulnone : Looking for a specific feature of an image. For example, the tears/marks on a cheetah. It is one feature that makes the cheetah very distinct. Note, there are other forms of pooling (mean pooling, sum pooling, etc). In the example I was following on Udemy, the instructor mentioned that max pooling eliminates the need of unnecessary features. In the cheetah example, we were able to get rid of 75% of the information. Max pooling allows us to instantly see specific distinct regions. In summary, we are able to preserve distinct features, inducing spatial variance, and reducing the size of information and parameters (this helps us prevent over fitting)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls58\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "http://scs.ryerson.ca/~aharley/vis/conv/flat.html"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul http://scs.ryerson.ca/~aharley/vis/conv/flat.html}}\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\ls58\ilvl3
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "http://scs.ryerson.ca/~aharley/vis/conv/"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul http://scs.ryerson.ca/~aharley/vis/conv/}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\ls58\ilvl3
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "http://www.cs.cmu.edu/~aharley/"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul http://www.cs.cmu.edu/~aharley/}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone 	\uc0\u8259 	\ul Flattening\ulnone : For example, making a 3x3 feature map display vertically 1-9\
	\uc0\u8259 	\ul Full Connection\ulnone : The middle/inner hidden layers (these develop attributes that describe each output). The final inner layer nodes each pass on a vote/probability from 0.0 to 1.0 on whether the image presented is a cat or dog (for example)\
	\uc0\u8259 	\ul SUMMARY\ulnone : {\field{\*\fldinst{HYPERLINK "https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html"}}{\fldrslt https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html\
}}\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls59\ilvl1\cf0 {\listtext	\uc0\u8259 	}Softmax & Cross-Entropy (Loss Function)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls59\ilvl2\cf0 {\listtext	\uc0\u8259 	}These functions behave similar to how the the mean square error (MSE) / cost function works when linear fitting\
{\listtext	\uc0\u8259 	}Classification Error, Mean Squared Error, Cross-Entropy\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls59\ilvl3\cf0 {\listtext	\uc0\u8259 	}Cross-Entropy is typically the best value to look at to evaluate your neural network (note, cross-entropy involves a logarithmic function. Also, cross-entropy is only the best for classification problems)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 	\uc0\u8259 	Image Preprocessing\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\cf0 	\uc0\u8259 	{\field{\*\fldinst{HYPERLINK "https://keras.io/preprocessing/image/"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://keras.io/preprocessing/image/}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \

\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone 	\uc0\u8259 	To avoid overfitting, data augmentation is used.\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls60\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Graph Neural Networks\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls60\ilvl1\cf0 {\listtext	\uc0\u8259 	}Deep Learning on Graphs For Computer Vision \'97 CNN, RNN, and GNN\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls60\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://medium.com/@utorontomist/deep-learning-on-graphs-for-computer-vision-cnn-rnn-and-gnn-c114d6004678"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://medium.com/@utorontomist/deep-learning-on-graphs-for-computer-vision-cnn-rnn-and-gnn-c114d6004678}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls60\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Recurrent Neural Networks\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls60\ilvl1\cf0 {\listtext	\uc0\u8259 	}Deep Learning on Graphs For Computer Vision \'97 CNN, RNN, and GNN\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls60\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://medium.com/@utorontomist/deep-learning-on-graphs-for-computer-vision-cnn-rnn-and-gnn-c114d6004678"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://medium.com/@utorontomist/deep-learning-on-graphs-for-computer-vision-cnn-rnn-and-gnn-c114d6004678}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls60\ilvl1
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}Illustrated Guide to Recurrent Neural Networks (watch video, great)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls60\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9}}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\'97\
DIMENSIONALITY REDUCTION\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls61\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Two Types\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls61\ilvl1\cf0 {\listtext	\uc0\u8259 	}Feature Selection\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls61\ilvl2\cf0 {\listtext	\uc0\u8259 	}Backward elimination, forward selection, bidirectional elimination, score comparison, and more (these topics were covered in the Regression lecture)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls61\ilvl1\cf0 {\listtext	\uc0\u8259 	}Feature Extraction\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls61\ilvl2\cf0 {\listtext	\uc0\u8259 	}Principal Component Analysis (PCA)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls61\ilvl3\cf0 {\listtext	\uc0\u8259 	}see \'97> {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=_UVHneBUBW0"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=_UVHneBUBW0}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul  
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone and {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=FgakZw6K1QQ"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=FgakZw6K1QQ}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul  
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone (great) and {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=HMOI_lkzW08"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=HMOI_lkzW08}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul   
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone (NOTE: I think the cells are considered the classes/dependent variables and the genes are considered as the independent variables)\
{\listtext	\uc0\u8259 	}PCA is a form of data preprocessing I thinkk. It has nothing to do with the model/classifier. PCA is a method of compressing a lot of data into something that captures the essence of the original data. PCA reduces dimensions by focusing on the genes with the most variation.The business example I followed in the Udemy course involved analyzing 178 data observations (12 independent variables, 1 dependent variable (3 classes, each class represents a different wine)). The business was trying to predict based off the ingredients what type of wine the drink should be classified as. PCA was applied to reduce the independent variables to the ones that show the most variance. This also allows us to visualize the data in 2D or 3D better.  \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls61\ilvl2\cf0 {\listtext	\uc0\u8259 	}Linear Discriminant Analysis (LDA)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls61\ilvl3\cf0 {\listtext	\uc0\u8259 	}see \'97> {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=azXCzI57Yfc"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=azXCzI57Yfc}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul  
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone (great)\
{\listtext	\uc0\u8259 	}LDA is very similar to PCA. It is also used in the preprocessing step for pattern classification. Its goal is to project a dataset onto a lower-dimensional space. However, LDA differs because in addition to finding the component axises with LDA. In other words, we are interested in maximizing the separability between the two (or more) groups/categories so we can make the best decisions. We are interested in the axes that maximize the separation between classes (think of the diagram that shows gaussian distributions on both the x-axis and y-axis). The goal of LDA is to project a feature space (a dataset n-dimensional samples) onto a small subspace k(where k is less than or equal to n-1) while maintaining the class-discriminatory information.\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls61\ilvl4\cf0 {\listtext	\uc0\u8259 	}Both PCA and LDA are linear transformation techniques used for dimensional reduction. PCA is described as unsupervised but LDA is supervised because of the relation to the dependent variable. \
{\listtext	\uc0\u8259 	}PCA: Component axes that maximize the variance\
{\listtext	\uc0\u8259 	}LDA: Maximizing the component axes for class-separation.\
\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls61\ilvl2\cf0 {\listtext	\uc0\u8259 	}Kernel PCA (this is a nonlinear reduction strategy)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls61\ilvl3\cf0 {\listtext	\uc0\u8259 	}This is for nonlinear problems. Note, the same linear model can be used with the normal PCA object but this time we must use an additional argument "rbf.\'92 This accounts for the nonlinearity.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\'97\
MODEL SELECTION & BOOSTING\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls62\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Two Types \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls62\ilvl1\cf0 {\listtext	\uc0\u8259 	}Cross Vaidation (multiple methods)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls62\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=L_dQrZZjGDg"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=L_dQrZZjGDg}}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls62\ilvl1\cf0 {\listtext	\uc0\u8259 	}k-Fold Cross Validation\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls62\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=gJo0uNL-5Qw"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=gJo0uNL-5Qw}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul  
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls62\ilvl1\cf0 {\listtext	\uc0\u8259 	}Grid Search (GridSearchCV)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls62\ilvl2\cf0 {\listtext	\uc0\u8259 	}This technique involves grid search to find optimal hyperparameters. Note, 
\fs22 hyperparameters
\fs24  are the parameters you specify when you build your model.\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls63\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Boosting\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls63\ilvl1\cf0 {\listtext	\uc0\u8259 	}XGBoost\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls63\ilvl2\cf0 {\listtext	\uc0\u8259 	}It is the most powerful implementation of gradient boosting\
{\listtext	\uc0\u8259 	}When using XGBoost, you don\'92t have to use feature scaling,\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls64\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Mean Absolute Error (MAE) vs Root mean squared error (RMSE)        \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls64\ilvl1\cf0 {\listtext	\uc0\u8259 	}For continuous dependent variables only\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls64\ilvl2\cf0 {\listtext	\uc0\u8259 	}great read\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d}}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\'97\'97\'97\'97\'97\'97\'97\
PROS AND CONS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls65\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}CART vs Random Forests\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls65\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://dzone.com/articles/cart-and-random-forests"}}{\fldrslt https://dzone.com/articles/cart-and-random-forests}}\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls65\ilvl2\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\
\
\
\'97\'97\'97\'97\'97\
PLOTS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls66\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Violin plots\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls66\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=M6Nu59Fsyyw"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=M6Nu59Fsyyw}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls67\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Line plot\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls67\ilvl1\cf0 {\listtext	\uc0\u8259 	}ax = sns.lineplot(x="pickup_hour", y="Trip_distance", legend="full" , data=summary_wdays_avg_duration, estimator=None, hue="day_of_week")\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\'97\'97\'97\'97\'97\
VISUALIZATION\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls68\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}The Power of Visualizations\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Virtualitics Webinar - Introduction to our platform and our API by VirtualiticsAR (fantastic video, see around 9:00, {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=p58Lysfv4FY"}}{\fldrslt https://www.youtube.com/watch?v=p58Lysfv4FY}})\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\'97\
STATISTICS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls69\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Pearson Coefficient\
{\listtext	
\f1 \uc0\u9642 
\f0 	}Skewness\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls69\ilvl1\cf0 {\listtext	\uc0\u8259 	}measures the symmetry of a distribution\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls69\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Kurtosis \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls69\ilvl1\cf0 {\listtext	\uc0\u8259 	}measures how peaked or flat a distribution is\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls69\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Standard Error\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls69\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.investopedia.com/terms/s/standard-error.asp"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.investopedia.com/terms/s/standard-error.asp}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls70\ilvl0
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	
\f1 \uc0\u9642 
\f0 	}Standard Deviation\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls70\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.investopedia.com/terms/s/standarddeviation.asp"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.investopedia.com/terms/s/standarddeviation.asp}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls70\ilvl0
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	
\f1 \uc0\u9642 
\f0 	}Homoscedasticity\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls70\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://datascience.stackexchange.com/questions/20237/why-do-we-convert-skewed-data-into-a-normal-distribution"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://datascience.stackexchange.com/questions/20237/why-do-we-convert-skewed-data-into-a-normal-distribution}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls70\ilvl2
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}The errors your model commits should have the same variance, i.e. you should ensure the linear regression does not make small errors for low values of 
\f5 \uc0\u55349 \u56395 
\f0  and big errors for higher values of 
\f5 \uc0\u55349 \u56395 
\f0 . In other words, the difference between what you predict 
\f5 \uc0\u55349 \u56396 \u770 
\f0   and the true values 
\f5 \uc0\u55349 \u56396 
\f0  should be constant. You can ensure that by making sure that 
\f5 \uc0\u55349 \u56396 
\f0  follows a Gaussian distribution. (The proof is highly mathematical.)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\'97\'97\'97\'97\'97\
PYTHON\
\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls71\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Commonly used functions\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls71\ilvl1\cf0 {\listtext	\uc0\u8259 	}list()\
{\listtext	\uc0\u8259 	}set()\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls71\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://realpython.com/python-sets/"}}{\fldrslt https://realpython.com/python-sets/}}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls71\ilvl3\cf0 {\listtext	\uc0\u8259 	}see the \'93Operators vs Methods\'94 section (the or operator)\
{\listtext	\uc0\u8259 	}that structure comes up a lot while getting unique elements for encoding with both a training and test set\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls71\ilvl1\cf0 {\listtext	\uc0\u8259 	}zip()\
{\listtext	\uc0\u8259 	}sys.getsizeof()     # get byte size of object\
{\listtext	\uc0\u8259 	}dict()\
{\listtext	\uc0\u8259 	}sort()\
{\listtext	\uc0\u8259 	}sorted()\
{\listtext	\uc0\u8259 	}sum()\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls72\ilvl1\cf0 {\listtext	\uc0\u8259 	}len()\
{\listtext	\uc0\u8259 	}split()\
{\listtext	\uc0\u8259 	}range()\
{\listtext	\uc0\u8259 	}append()\
{\listtext	\uc0\u8259 	}apply()\
{\listtext	\uc0\u8259 	}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls72\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}General\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls72\ilvl1\cf0 {\listtext	\uc0\u8259 	}Data Structures\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls72\ilvl2\cf0 {\listtext	\uc0\u8259 	}Primitive\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls72\ilvl3\cf0 {\listtext	\uc0\u8259 	}Integer\
{\listtext	\uc0\u8259 	}Float\
{\listtext	\uc0\u8259 	}String\
{\listtext	\uc0\u8259 	}Boolean\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls72\ilvl2\cf0 {\listtext	\uc0\u8259 	}Non-Primitive\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls72\ilvl3\cf0 {\listtext	\uc0\u8259 	}Array\
{\listtext	\uc0\u8259 	}List\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls72\ilvl4\cf0 {\listtext	\uc0\u8259 	}Linear\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls72\ilvl5\cf0 {\listtext	\uc0\u8259 	}Stacks\
{\listtext	\uc0\u8259 	}Queues\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls72\ilvl4\cf0 {\listtext	\uc0\u8259 	}Non-Linear\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls72\ilvl5\cf0 {\listtext	\uc0\u8259 	}Graphs\
{\listtext	\uc0\u8259 	}Trees\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls72\ilvl3\cf0 {\listtext	\uc0\u8259 	}Tuple\
{\listtext	\uc0\u8259 	}Dictionary\
{\listtext	\uc0\u8259 	}Set\
{\listtext	\uc0\u8259 	}File \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls72\ilvl1\cf0 {\listtext	\uc0\u8259 	}BitWise Operations (great for memory management)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls72\ilvl2\cf0 {\listtext	\uc0\u8259 	}Complement (~)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls72\ilvl3\cf0 {\listtext	\uc0\u8259 	}Used to store negative numbers. Negative numbers aren\'92t actually stored on a computer\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls72\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1 (Python Tutorial for Beginners | Python BitWise Operators by Telusko, ~30 sec , {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=PyfKCvHALj8"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=PyfKCvHALj8}})\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls72\ilvl5\cf0 {\listtext	\uc0\u8259 	}~12 # outputs -13 \
{\listtext	\uc0\u8259 	}\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls72\ilvl2\cf0 {\listtext	\uc0\u8259 	}And (&)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls72\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1 (Python Tutorial for Beginners | Python BitWise Operators by Telusko, ~4:30, {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=PyfKCvHALj8"}}{\fldrslt https://www.youtube.com/watch?v=PyfKCvHALj8}}) \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls72\ilvl5\cf0 {\listtext	\uc0\u8259 	}12 & 13 # outputs 12\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls72\ilvl6\cf0 {\listtext	\uc0\u8259 	}12 Bit -----\'97> 00001100\
{\listtext	\uc0\u8259 	}13 Bit ------\'97> 00001101\
{\listtext	\uc0\u8259 	}12 & 13 Bit\'97> 00001100   # aka, 12\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls72\ilvl2\cf0 {\listtext	\uc0\u8259 	}Or (|)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls72\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1 (Python Tutorial for Beginners | Python BitWise Operators by Telusko, ~5:45, {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=PyfKCvHALj8"}}{\fldrslt https://www.youtube.com/watch?v=PyfKCvHALj8}}) \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls72\ilvl5\cf0 {\listtext	\uc0\u8259 	}12 | 13 # outputs 13\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls72\ilvl6\cf0 {\listtext	\uc0\u8259 	}12 Bit -----\'97> 00001100\
{\listtext	\uc0\u8259 	}13 Bit ------\'97> 00001101\
{\listtext	\uc0\u8259 	}12 | 13 Bit\'97> 00001101 # aka, 13\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls72\ilvl2\cf0 {\listtext	\uc0\u8259 	}XOR (^)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls72\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1 (Python Tutorial for Beginners | Python BitWise Operators by Telusko, ~7:20, {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=PyfKCvHALj8"}}{\fldrslt https://www.youtube.com/watch?v=PyfKCvHALj8}})\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls72\ilvl5\cf0 {\listtext	\uc0\u8259 	} 12^13 #outputs 1\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls72\ilvl6\cf0 {\listtext	\uc0\u8259 	}12 Bit -----\'97> 00001100\
{\listtext	\uc0\u8259 	}13 Bit ------\'97> 00001101\
{\listtext	\uc0\u8259 	}12 ^ 13 Bit\'97> 00000001 # aka, 1\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls72\ilvl7\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls72\ilvl6\cf0 {\listtext	\uc0\u8259 	}Left Shift (<<) \
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls72\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1 (Python Tutorial for Beginners | Python BitWise Operators by Telusko, ~8:40, {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=PyfKCvHALj8"}}{\fldrslt https://www.youtube.com/watch?v=PyfKCvHALj8}})\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls72\ilvl5\cf0 {\listtext	\uc0\u8259 	} 10<<2 # outputs 40\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls72\ilvl6\cf0 {\listtext	\uc0\u8259 	}10 Bit \'97\'97\'97\'97\'97> 00001010\
{\listtext	\uc0\u8259 	}10<<2 Bit ------\'97>  00101000 #aka 40\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls72\ilvl2\cf0 {\listtext	\uc0\u8259 	}Right Shift (>>) \
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls72\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1 (Python Tutorial for Beginners | Python BitWise Operators by Telusko, ~11:10, {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=PyfKCvHALj8"}}{\fldrslt https://www.youtube.com/watch?v=PyfKCvHALj8}})\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls72\ilvl5\cf0 {\listtext	\uc0\u8259 	} 10>>2 # outputs 2\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls72\ilvl6\cf0 {\listtext	\uc0\u8259 	}10 Bit \'97\'97\'97\'97\'97> 00001010\
{\listtext	\uc0\u8259 	}10>>2 Bit ------\'97>  00000010 #aka 1\
{\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls72\ilvl1\cf0 {\listtext	\uc0\u8259 	}Bytes and encodings in Python by Reuven Lerner ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=ls-177DIGao"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=ls-177DIGao}})\
{\listtext	\uc0\u8259 	}\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls72\ilvl2\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls72\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}CSV file size\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls72\ilvl1\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls72\ilvl2\cf0 {\listtext	\uc0\u8259 	}os.path.getsize(\'93<myCSVFileName>.csv") # get file size in bytes\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls72\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Syntax\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls72\ilvl1\cf0 {\listtext	\uc0\u8259 	}ebola_long["variable"].str.split("_", expand=True)         \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls72\ilvl2\cf0 {\listtext	\uc0\u8259 	}to view the syntax better, you can rewrite as \
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls72\ilvl4\cf0 {\listtext	\uc0\u8259 	}(ebola_long["variable\'92]\
{\listtext	\uc0\u8259 	}.str\
{\listtext	\uc0\u8259 	}.split("_", expand=True)\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls72\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Functions\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls73\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Libraries\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls73\ilvl1\cf0 {\listtext	\uc0\u8259 	}import os\
{\listtext	\uc0\u8259 	}import sys \
{\listtext	\uc0\u8259 	}tflearn\
{\listtext	\uc0\u8259 	}keras \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls73\ilvl2\cf0 {\listtext	\uc0\u8259 	}High-level API that sits on top of TensorFlow\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls73\ilvl1\cf0 {\listtext	\uc0\u8259 	}theano\
{\listtext	\uc0\u8259 	}numba \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls73\ilvl2\cf0 {\listtext	\uc0\u8259 	}Tries to make all the computations numpy does really fast\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls73\ilvl1\cf0 {\listtext	\uc0\u8259 	}seaborn\
{\listtext	\uc0\u8259 	}matplotlib.pyplot\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls73\ilvl2\cf0 {\listtext	\uc0\u8259 	}Anatomy of a figure\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls73\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://matplotlib.org/3.1.1/gallery/showcase/anatomy.html"}}{\fldrslt https://matplotlib.org/3.1.1/gallery/showcase/anatomy.html}}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\
NUMPY\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls74\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Common functions\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 	\uc0\u8259 	np.log1p()    # fixing skewed data\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls75\ilvl1\cf0 {\listtext	\uc0\u8259 	}np.expm1()  # done after np.log1p\
{\listtext	\uc0\u8259 	}np.linspace\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\'97\'97\'97\'97\'97\
PANDAS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 	
\f1 \uc0\u9642 
\f0 	Dataframe\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls76\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Dataframe\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls76\ilvl1\cf0 {\listtext	\uc0\u8259 	}Terminology\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls76\ilvl2\cf0 {\listtext	\uc0\u8259 	}Mean the same thing\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls76\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls76\ilvl4\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>[\'93<myFeatureName1>\'94] and <myDataFrameName1>.<myFeatureName1>\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls76\ilvl1\cf0 {\listtext	\uc0\u8259 	}There are three parts to a dataframe\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls76\ilvl2\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.columns       (feature names)\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.index           (row names)\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.values          (body values, this is good for extracting data from the Dataframe to use the data in another library that may just use numpy arrays)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls76\ilvl1\cf0 {\listtext	\uc0\u8259 	}Dataframe information\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls76\ilvl2\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.info()\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls76\ilvl3\cf0 {\listtext	\uc0\u8259 	}\'93object\'94 are typically string values\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls76\ilvl2\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.types\
{\listtext	\uc0\u8259 	}If you extract a single column of data from a DataFrame set that new data  then becomes a Series object\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls76\ilvl3\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls76\ilvl1\cf0 {\listtext	\uc0\u8259 	}aggregate()\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\'97\'97\'97\'97\'97\
Q&A STACKEXCHANGE\
\
\pard\pardeftab720\sl280\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://datascience.stackexchange.com/questions/26776/how-to-calculate-ideal-decision-tree-depth-without-overfitting"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://datascience.stackexchange.com/questions/26776/how-to-calculate-ideal-decision-tree-depth-without-overfitting}}\
\
\pard\pardeftab720\sl280\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://stackoverflow.com/questions/34162443/why-do-many-examples-use-fig-ax-plt-subplots-in-matplotlib-pyplot-python"}}{\fldrslt \cf0 https://stackoverflow.com/questions/34162443/why-do-many-examples-use-fig-ax-plt-subplots-in-matplotlib-pyplot-python}}\
\
\pard\pardeftab720\sl280\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://stackoverflow.com/questions/3584805/in-matplotlib-what-does-the-argument-mean-in-fig-add-subplot111"}}{\fldrslt \cf0 https://stackoverflow.com/questions/3584805/in-matplotlib-what-does-the-argument-mean-in-fig-add-subplot111}}\
\
\pard\pardeftab720\sl280\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://www.geeksforgeeks.org/python-pandas-dataframe-ix/"}}{\fldrslt \cf0 https://www.geeksforgeeks.org/python-pandas-dataframe-ix/}}\
\
\pard\pardeftab720\sl280\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://stackoverflow.com/questions/31593201/how-are-iloc-ix-and-loc-different"}}{\fldrslt \cf0 https://stackoverflow.com/questions/31593201/how-are-iloc-ix-and-loc-different}}\
\
\pard\pardeftab720\sl280\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://stats.stackexchange.com/questions/56302/what-are-good-rmse-values"}}{\fldrslt \cf0 https://stats.stackexchange.com/questions/56302/what-are-good-rmse-values}}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://stackoverflow.com/questions/22409855/randomforestclassifier-vs-extratreesclassifier-in-scikit-learn"}}{\fldrslt \cf0 https://stackoverflow.com/questions/22409855/randomforestclassifier-vs-extratreesclassifier-in-scikit-learn}}\
\
\'97\'97\'97\'97\'97\
RANDOM SOURCES\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://www.kdnuggets.com/2017/10/edge-analytics.html"}}{\fldrslt \cf0 https://www.kdnuggets.com/2017/10/edge-analytics.html}}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://medium.com/@livewithai/the-significance-of-edge-cases-and-the-cost-of-imperfection-as-it-pertains-to-ai-adoption-dc1cebeef72c"}}{\fldrslt \cf0 https://medium.com/@livewithai/the-significance-of-edge-cases-and-the-cost-of-imperfection-as-it-pertains-to-ai-adoption-dc1cebeef72c}}\
\
\
\'97\'97\'97\'97\'97\
JUPYTER\
\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls77\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Shortcuts/Commands\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls77\ilvl1\cf0 {\listtext	\uc0\u8259 	}shift + enter\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls77\ilvl2\cf0 {\listtext	\uc0\u8259 	}run current cell\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls77\ilvl1\cf0 {\listtext	\uc0\u8259 	}shift + tab \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls77\ilvl2\cf0 {\listtext	\uc0\u8259 	}will show you the Docstring (documentation) for the the object you have just typed in a code cell \'96 you can keep pressing this short cut to cycle through a few modes of documentation.\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls77\ilvl1\cf0 {\listtext	\uc0\u8259 	}fn + a\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls77\ilvl2\cf0 {\listtext	\uc0\u8259 	}insert cell above\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls77\ilvl1\cf0 {\listtext	\uc0\u8259 	}fn + b\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls77\ilvl2\cf0 {\listtext	\uc0\u8259 	}insert cell below\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls77\ilvl1\cf0 {\listtext	\uc0\u8259 	}shift + m \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls77\ilvl2\cf0 {\listtext	\uc0\u8259 	}merge multiple cells\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls78\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Tutorials\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls78\ilvl1\cf0 {\listtext	\uc0\u8259 	}YouTube\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls78\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=HW29067qVWk"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=HW29067qVWk}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul  
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone great overview\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls78\ilvl1\cf0 {\listtext	\uc0\u8259 	}Reading\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls78\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/#:~:targetText=Shift%20%2B%20Tab%20will%20show%20you,from%20where%20your%20cursor%20is."}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/#:~:targetText=Shift%20%2B%20Tab%20will%20show%20you,from%20where%20your%20cursor%20is.}}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls79\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}To display HTML content in Jupyter notebook (for example, embed YouTube)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls79\ilvl1\cf0 {\listtext	\uc0\u8259 	}%%HTML          (line one within cell)\
{\listtext	\uc0\u8259 	}<iframe width="560" height="315" src="//www.youtube.com/embed/HW29067qVWk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>    (line two within cell, note got to \'93Share\'94 underneath a YouTube video to get this syntax)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls80\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}To display an image from local machine within the same folder (example)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls80\ilvl1\cf0 {\listtext	\uc0\u8259 	}<img src="d11281b44c2e440b36aaf29156b5032105d2d06b.png" style="with:200px;400px"/>\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls80\ilvl2\cf0 {\listtext	\uc0\u8259 	}or\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls80\ilvl1\cf0 {\listtext	\uc0\u8259 	}![](d11281b44c2e440b36aaf29156b5032105d2d06b.png)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls81\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Table of Contents / Bookmarking (Example)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls81\ilvl1\cf0 {\listtext	\uc0\u8259 	}[Gather Sense of our data](#gatherSenseOfOurData)      (place this at the start, in ToC)\
{\listtext	\uc0\u8259 	}<a name="gatherSenseOfOurData"></a>        (place this at the destination)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls82\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Debugging\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls82\ilvl1\cf0 {\listtext	\uc0\u8259 	}you can type out the name of any object within a cell and press enter and it will output what type of object it is\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls82\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Shortcuts\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls82\ilvl1\cf0 {\listtext	\uc0\u8259 	}fn + a = insert cell above\
{\listtext	\uc0\u8259 	}fn + b = insert cell below\
{\listtext	\uc0\u8259 	}Option + enter = run current cell and insert below\
{\listtext	\uc0\u8259 	}Shift + enter = run current cell\
{\listtext	\uc0\u8259 	}shift+tab = see arguments for a function\
{\listtext	\uc0\u8259 	}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls82\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}General\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls82\ilvl1\cf0 {\listtext	\uc0\u8259 	}To start a notebook, cd to working directory in terminal then run \'93jupyter notebook\'94\
{\listtext	\uc0\u8259 	}You can run bash commands by using an \'93!\'94 in front of the commands (eg, !pip list)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}To see line and cell magics (note you can run bash commands to with the \'93%\'94)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls82\ilvl2\cf0 {\listtext	\uc0\u8259 	}%lsmagic\
{\listtext	\uc0\u8259 	}%ls \
{\listtext	\uc0\u8259 	}%ls -la\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls82\ilvl1\cf0 {\listtext	\uc0\u8259 	}To show all output with Pandas (Python)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls82\ilvl2\cf0 {\listtext	\uc0\u8259 	}pd.set_option('display.max_columns', None)\
{\listtext	\uc0\u8259 	}pd.set_option('display.max_rows', None)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
COMMANDS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls83\ilvl0\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\
\'97\'97\'97\'97\'97 \
GREAT YOUTUBE CHANNELS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls84\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Data School ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/channel/UCnVzApLJE2ljPZSeQylSEyg"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/channel/UCnVzApLJE2ljPZSeQylSEyg}})\
{\listtext	
\f1 \uc0\u9642 
\f0 	}codebasics\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls84\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=gJo0uNL-5Qw"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=gJo0uNL-5Qw}}          (Machine Learning Tutorial Python 12 - K Fold Cross Validation)\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://github.com/codebasics/py"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://github.com/codebasics/py}}            (see all repositories)\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls84\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Sentdex \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls84\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ"}}{\fldrslt https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ}}\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls84\ilvl2\cf0 {\listtext	\uc0\u8259 	}Playlists\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls84\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=FLZvOKSCkxY&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL"}}{\fldrslt https://www.youtube.com/watch?v=FLZvOKSCkxY&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL}}  NLTK Natural Language Processing with Python\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/playlist?list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/playlist?list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul  
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone Deep Learning basics with Python, TensorFlow and Keras\
{\listtext	\uc0\u8259 	}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
---\'97\'97\'97\'97\
MEDIUM \
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls85\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}{\field{\*\fldinst{HYPERLINK "https://towardsdatascience.com/understanding-input-and-output-shapes-in-convolution-network-keras-f143923d56ca"}}{\fldrslt https://towardsdatascience.com/understanding-input-and-output-shapes-in-convolution-network-keras-f143923d56ca}}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\'97\'97\'97\'97\'97\
\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\
\
\'97\'97\'97\'97\
\
\
\
\
\'97\'97\'97\
PROBLEM SOLVING TIPS\
\
\
\pard\tx220\tx720\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls86\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Classification Problem\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls86\ilvl1\cf0 {\listtext	\uc0\u8259 	}Typically its good to test logistic regression vs random forest classifier to see which one performs best. If the random forest classifier performs best, then we should look into boosting with either xgboost, lightgbm, or catboost\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\
\
Overall process (from Allstate Severity Example):\
\
\pard\tx220\tx720\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls87\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Data statistics\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls87\ilvl1\cf0 {\listtext	\uc0\u8259 	}Shape\
{\listtext	\uc0\u8259 	}Peek\
{\listtext	\uc0\u8259 	}Description\
{\listtext	\uc0\u8259 	}Skew\
\pard\tx220\tx720\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls87\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Transformation\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls87\ilvl1\cf0 {\listtext	\uc0\u8259 	}Correction of skew\
\pard\tx220\tx720\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls87\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Data Interaction\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls87\ilvl1\cf0 {\listtext	\uc0\u8259 	}Correlation\
{\listtext	\uc0\u8259 	}Scatter plot\
\pard\tx220\tx720\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls87\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Data Visualization\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls87\ilvl1\cf0 {\listtext	\uc0\u8259 	}Box and density plots\
{\listtext	\uc0\u8259 	}Grouping of one hot encoded attributes\
\pard\tx220\tx720\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls87\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Data Preparation\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls87\ilvl1\cf0 {\listtext	\uc0\u8259 	}One hot encoding of categorical data\
{\listtext	\uc0\u8259 	}Train-test split\
\pard\tx220\tx720\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls87\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Evaluation, Prediction, and Analysis\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls87\ilvl1\cf0 {\listtext	\uc0\u8259 	}Linear Regression (Linear algo)\
{\listtext	\uc0\u8259 	}Ridge Regression (Linear algo)\
{\listtext	\uc0\u8259 	}LASSO Linear Regression (Linear algo)\
{\listtext	\uc0\u8259 	}Elastic Net Regression (Linear algo)\
{\listtext	\uc0\u8259 	}KNN (non-linear algo)\
{\listtext	\uc0\u8259 	}CART (non-linear algo)\
{\listtext	\uc0\u8259 	}SVM (Non-linear algo)\
{\listtext	\uc0\u8259 	}Bagged Decision Trees (Bagging)\
{\listtext	\uc0\u8259 	}Random Forest (Bagging)\
{\listtext	\uc0\u8259 	}Extra Trees (Bagging)\
{\listtext	\uc0\u8259 	}AdaBoost (Boosting)\
{\listtext	\uc0\u8259 	}Stochastic Gradient Boosting (Boosting)\
{\listtext	\uc0\u8259 	}MLP (Deep Learning)\
{\listtext	\uc0\u8259 	}XGBoost\
{\listtext	\uc0\u8259 	}Make Predictions\
\pard\tx720\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\
######################\
GENERAL SNIPPETS\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls88\ilvl2\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx220\tx720\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls88\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Jupyter Notebook / Markdown\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls88\ilvl1\cf0 {\listtext	\uc0\u8259 	}Github\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls88\ilvl2\cf0 {\listtext	\uc0\u8259 	}This is a self-exercise notebook that was created while following along the Natural Language Processing section in the following Udemy course by SuperDataScience:\
{\listtext	\uc0\u8259 	}This is a self-exercise notebook that was created while following along in the following YouTube video by codebasics:\
\pard\tx220\tx720\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls88\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Python\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls88\ilvl1\cf0 {\listtext	\uc0\u8259 	}Important\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls88\ilvl2\cf0 {\listtext	\uc0\u8259 	}There seems to be a lot of issues with copying snippets over from TextEdit to Jupyter. It\'92s something to do with single quotes. Change them to double quotes.\
{\listtext	\uc0\u8259 	}Snippet Template Names\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls88\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>\
{\listtext	\uc0\u8259 	}<>\
{\listtext	\uc0\u8259 	}<myFeatureName1>\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls88\ilvl4\cf0 {\listtext	\uc0\u8259 	}<myFeatureName1_categorical>\
{\listtext	\uc0\u8259 	}<myFeatureName1_continuous>\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls88\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myVarName1> \
{\listtext	\uc0\u8259 	}<myModelName1> \
{\listtext	\uc0\u8259 	}<myXTestName1>\
{\listtext	\uc0\u8259 	}<myFunctionName1> \
{\listtext	\uc0\u8259 	}<myInt1>\
{\listtext	\uc0\u8259 	}<myIndexName1>\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls88\ilvl1\cf0 {\listtext	\uc0\u8259 	}Pandas\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls88\ilvl2\cf0 {\listtext	\uc0\u8259 	}Info \
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls88\ilvl3\cf0 {\listtext	\uc0\u8259 	}pandas.__version__       # this is really useful when trying to get help from Google or StackOverflow\
{\listtext	\uc0\u8259 	}[]   # single brackets are for specifying if you are in rows or columns\
{\listtext	\uc0\u8259 	}[[]]  # inner brackets are typically used when you want to implement a list/multiple things\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls88\ilvl4\cf0 {\listtext	\uc0\u8259 	}[["year\'92, "date\'92]]        # example\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls88\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>[\'93<myFeatureName1>\'94] is the same thing as <myDataFrameName1>.<myFeatureName1>\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls88\ilvl2\cf0 {\listtext	\uc0\u8259 	}Set Options\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls88\ilvl3\cf0 {\listtext	\uc0\u8259 	}pd.set_option('display.max_columns', 999) # allows us to see all columns in Jupyter notebook\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls88\ilvl2\cf0 {\listtext	\uc0\u8259 	}Converting Data Types\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls88\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1 (great)\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls88\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}convert_dict = \{"<myFeatureNameList1>": int,\
{\listtext	\uc0\u8259 	}                "<myFeatureNameList2>": float\
{\listtext	\uc0\u8259 	}               \}\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.astype(convert_dict) \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
\pard\tx2380\tx2880\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls88\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls88\ilvl4\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>[<myFeatureNameList1>] = <myDataFrameName1>[<myFeatureNameList1>].apply(pd.to_numeric, errors='coerce') \
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls88\ilvl3\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls88\ilvl2\cf0 {\listtext	\uc0\u8259 	}\cf13 \cb10 Note, the inplace argument saves that change to the current instance of the dataframe object!\cf0 \cb1 \
{\listtext	\uc0\u8259 	}Column headers/names to a list\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls88\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.to_list()\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls88\ilvl2\cf0 {\listtext	\uc0\u8259 	}Replace\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls88\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls88\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# replacing different types of zero (string/object, int, float)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}cols = <myDataFrameName1>.columns.to_list()\
{\listtext	\uc0\u8259 	}<myDataFrameName1>[cols] = <myDataFrameName1>[cols].replace(\{\'930\'94:np.nan, 0:np.nan, 0.0:np.nan\}) \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls88\ilvl1\cf0 {\listtext	\uc0\u8259 	}Commonly used (\cf6 \cb10 things to get really good at\cf0 \cb1 )\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls88\ilvl2\cf0 {\listtext	\uc0\u8259 	}timeit example\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls88\ilvl3\cf0 {\listtext	\uc0\u8259 	}%%timeit                                                                   # line 1 in Jupyter cell\
{\listtext	\uc0\u8259 	}<myFunctionName1>(<myDataFrameName1>[\'93<myFeatureName1>\'94].values, <myDataFrameName1>[\'93<myFeatureName2>\'94].values)    # line 2 in Jupyter cell\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls88\ilvl2\cf0 {\listtext	\uc0\u8259 	}print(train_X.shape)\
{\listtext	\uc0\u8259 	}print(type(train_X.shape))\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}.size() # {\field{\*\fldinst{HYPERLINK "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.size.html"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.size.html}}\
{\listtext	\uc0\u8259 	}.value_counts() # {\field{\*\fldinst{HYPERLINK "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html}}\
{\listtext	\uc0\u8259 	}.groupby() # {\field{\*\fldinst{HYPERLINK "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html}}\
{\listtext	\uc0\u8259 	}.groupby(level=[0,1]).size()   \
{\listtext	\uc0\u8259 	}.get_group()\
{\listtext	\uc0\u8259 	}.melt() \
{\listtext	\uc0\u8259 	}.unique()\
{\listtext	\uc0\u8259 	}.pivot_table()\
{\listtext	\uc0\u8259 	}.reset_index() # {\field{\*\fldinst{HYPERLINK "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html}} \
{\listtext	\uc0\u8259 	}.sort_index(<myIndexName1>, ascending=True)\
{\listtext	\uc0\u8259 	}.sort_values(<myFeatureName1>, ascending=True) \
{\listtext	\uc0\u8259 	}.tolist()\
{\listtext	\uc0\u8259 	} Panda Types\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls88\ilvl3\cf0 {\listtext	\uc0\u8259 	}object\
{\listtext	\uc0\u8259 	}datetime64[ns]\
{\listtext	\uc0\u8259 	}timedelta64[ns]\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls88\ilvl2\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls88\ilvl1\cf0 {\listtext	\uc0\u8259 	}Commonly used libraries\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls88\ilvl2\cf0 {\listtext	\uc0\u8259 	}import pandas as pd\
{\listtext	\uc0\u8259 	}import numpy as np\
{\listtext	\uc0\u8259 	}import matplotlib.pyplot as plt\
{\listtext	\uc0\u8259 	}import seaborn as sns\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}import warnings\
{\listtext	\uc0\u8259 	}warnings.filterwarnings('ignore')\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}import tensorflow as tf \
{\listtext	\uc0\u8259 	}tf.logging.set_verbosity(tf.logging.ERROR)     # To surpress any TensorFlow warnings.  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}from sklearn.preprocessing import LabelEncoder, OneHotEncoder\
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.linear_model import LinearRegression \
{\listtext	\uc0\u8259 	}from sklearn.neighbors import KNeighborsRegressor      # KNN (Non-linear Algo) \
{\listtext	\uc0\u8259 	}from sklearn.tree import DecisionTreeRegressor  # CART\
{\listtext	\uc0\u8259 	}from sklearn.svm import SVR\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import BaggingRegressor\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import RandomForestRegressor\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import ExtraTreesRegressor\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import AdaBoostRegressor\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import GradientBoostingRegressor\
{\listtext	\uc0\u8259 	}from xgboost import XGBRegressor\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}import re # used often for NLP, removes and replaces characters \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}  \
{\listtext	\uc0\u8259 	}from keras.models import Sequential \
{\listtext	\uc0\u8259 	}from keras.layers import Convolution2D\
{\listtext	\uc0\u8259 	}from keras.layers import MaxPooling2D\
{\listtext	\uc0\u8259 	}from keras.layers import Flatten\
{\listtext	\uc0\u8259 	}from keras.layers import Dense\
{\listtext	\uc0\u8259 	}from keras.preprocessing.image import ImageDataGenerator\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls88\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://keras.io/preprocessing/image/"}}{\fldrslt https://keras.io/preprocessing/image/}}\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls88\ilvl2\cf0 {\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import libraries for deep learning\
{\listtext	\uc0\u8259 	}from keras.wrappers.scikit_learn import KerasRegressor\
{\listtext	\uc0\u8259 	}from keras.models import Sequential\
{\listtext	\uc0\u8259 	}from keras.layers import Dense\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Optimize using dropout and decay\
{\listtext	\uc0\u8259 	}from keras.optimizers import SGD \
{\listtext	\uc0\u8259 	}from keras.layers import Dropout \
{\listtext	\uc0\u8259 	}from keras.constraints import maxnorm \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls88\ilvl1\cf0 {\listtext	\uc0\u8259 	}Encoding\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls88\ilvl2\cf0 {\listtext	\uc0\u8259 	}pandas\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls88\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls88\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}<myDataFrameName1> = sns.load_dataset("tips")\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.head()\
{\listtext	\uc0\u8259 	}<myDataFrameName2> = pd.get_dummies(<myDataFrameName1>, drop_first=True) \
{\listtext	\uc0\u8259 	}<myDataFrameName2>.head() \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls88\ilvl2\cf0 {\listtext	\uc0\u8259 	}sklearn\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls88\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls89\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.preprocessing import LabelEncoder, OneHotEncoder\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}<myDataFrameName1> = pd.read_csv("train.csv") \
{\listtext	\uc0\u8259 	}<myDataFrameName2> = pd.read_csv("test.csv") \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}cols = <myDataFrameName1>.columns\
{\listtext	\uc0\u8259 	}split = <myIntOfCategoricalColumnsToEncode> \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}labels = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Making sure we account for all of the unique variables that show up in both the training and test set provided. \
{\listtext	\uc0\u8259 	}# For instance, this ensures we dont run into any unforeseen variables when going from the training set to test set.\
{\listtext	\uc0\u8259 	} for i in range(0, split):\
{\listtext	\uc0\u8259 	}     train = <myDataFrameName1>[cols[i]].unique()\
{\listtext	\uc0\u8259 	}     test = <myDataFrameName2>[cols[i]].unique()\
{\listtext	\uc0\u8259 	}     labels.append(list(set(train) | set(test))) #note the OR operator!  (See how sets work {\field{\*\fldinst{HYPERLINK "https://realpython.com/python-sets/"}}{\fldrslt https://realpython.com/python-sets/}})\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} print("labels %s" % labels)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}cats = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for i in range(0, split):\
{\listtext	\uc0\u8259 	}    label_encoder = LabelEncoder()  # Label Encode\
{\listtext	\uc0\u8259 	}    label_encoder.fit(labels[i])\
{\listtext	\uc0\u8259 	}    feature = label_encoder.transform(dataset_train.iloc[:,i])\
{\listtext	\uc0\u8259 	}    feature = feature.reshape(dataset_train.shape[0], 1)\
{\listtext	\uc0\u8259 	}    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))         # One Hot Encode\
{\listtext	\uc0\u8259 	}    feature = onehot_encoder.fit_transform(feature)\
{\listtext	\uc0\u8259 	}    cats.append(feature)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}print("################")\
{\listtext	\uc0\u8259 	}print("List of 1D array of cats--> %s" % cats)\
{\listtext	\uc0\u8259 	}print("################")\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Make a 2D array from a list of 1D arrays\
{\listtext	\uc0\u8259 	}encoded_cats = np.column_stack(cats) \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}print("################")\
{\listtext	\uc0\u8259 	}print("List of 2D array of cats--> %s" % encoded_cats)\
{\listtext	\uc0\u8259 	}print("################")\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}print("################")  \
{\listtext	\uc0\u8259 	}print(encoded_cats.shape)  # These are the categorical variables we just encoded. Now, we just need to concatenate it with the continuous vars.\
{\listtext	\uc0\u8259 	}print("################")\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Concatenate encoded attributes with continuous attributes\
{\listtext	\uc0\u8259 	}<myDataFrameName3> = np.concatenate((encoded_cats,dataset_train.iloc[:,split:].values),axis=1) \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}del cats\
{\listtext	\uc0\u8259 	}del dataset_train\
{\listtext	\uc0\u8259 	}del encoded_cats\
{\listtext	\uc0\u8259 	} \
\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Data Set Details\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.info() # data info. ex: datatypes, size etc.\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.dtypes\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.describe()\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.skew()      # Values close to 0 show less skew.\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.shape\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}Column Details\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}<myVarName1> = <myDataFrameName1>["<myFeatureName1>\'92].min() # minimum trip distance, same for max()\
{\listtext	\uc0\u8259 	}<myVarName2> = <myDataFrameName1>["<myFeatureName1>\'92].value_counts()      # counts unique occurrences , returns a series! See .size() to return a dataframe\
 	\uc0\u8259 	<myVarName2>.head(10) \
\pard\tx1660\tx2160\tx2880\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}Searching for NaN Values\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe}}\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.isnull().sum()                (searches for which features contain NaN values)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}Renaming columns\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1> = <myDataFrameName1>.rename(columns=\{"<myOldName1>\'92: "<myNewName1>\'92, "<myOldName2>\'92: "<myNewName2>\'92\})\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}Renaming multiindex dataframes\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://stackoverflow.com/questions/19851005/rename-pandas-dataframe-index"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://stackoverflow.com/questions/19851005/rename-pandas-dataframe-index}}\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}To see a random sample of data\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.sample(30) \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}Return names of columns where a certain character shows up\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.columns[<myDataFrameName1>.isin(['?']).any()] \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}Return of unique items \
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>[\'93<myFeatureName1>\'94].nunique()        # nunique() function return Series with number of distinct observations over requested axis. If we set the value of axis to be 0, then it finds the total number of unique observations over the index axis.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}View unique items\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.<myFeatureName1>.unique()      \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Alternative method of setting index of DataFrame (set_index)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.set_index.html"}}{\fldrslt https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.set_index.html}}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Shuffling the DataFrame\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1> = <myDataFrameName1>.sample(frac=1).reset_index(drop=True)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Reading In Data\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}CSV\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}df = pd.read_csv("green_tripdata_2015-09.csv", low_memory=False, parse_dates=["lpep_pickup_datetime"])                 # the parse_dates argument is turning "lpep_pickup_datetime" from an object type into a datetime64 type \
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}df  = pd.read_csv(\'93myFolderName/mySubFolderName/myCSVFileName.csv\'94)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}TSV\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}dataset = pd.read_csv("Restaurant_Reviews.tsv", delimiter="\\t")\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Saving DataFrame\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}CSV\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>("myFolderName/mySubFolderName/myCSVFileName.csv", index=False, header=True)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Converting data types\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}series and numpy array\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>[\'93<myFeatureName1>\'94]                 # this gives a series\
{\listtext	\uc0\u8259 	}mySeriesObject = data.groupby(["Item", "System", "failureObservation"])["repairTimeWholeHour"].value_counts()     # another example of getting a series from a dataframe\
{\listtext	\uc0\u8259 	}<myDataFrameName1>[\'93<myFeatureName1>\'94].values      # this gives a numpy array\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}Convert Series Object to a Data Frame\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}myDataFrame2 = mySeriesObject.to_frame()\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}Dataframe/Series to List\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>["<myFeatureName1>"].tolist()\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Creating a DataFrame\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}Manually\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1> = pd.DataFrame(\{\
{\listtext	\uc0\u8259 	}  "<myFeatureName1>": [10, 20, 30],\
{\listtext	\uc0\u8259 	}  "<myFeatureName2>": [20, 30, 40]\
{\listtext	\uc0\u8259 	}\})\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Operations on a DataFrame\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}By Feature (this is called Broadcasting)\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>[\'93<myFeatureName1>\'94] ** 2     # this squares every row\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}Adding rows\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>["<myFeatureName1>"] + <myDataFrameName1>["<myFeatureName2>"]\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Copy a data frame\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName2> = <myDataFrameName1>.copy()                                  # our dataframe\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Subsetting a data frame\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}By columns\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}Into a series\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName2> = <myDataFrameName1>["<myFeatureName1>\'92]\
{\listtext	\uc0\u8259 	}<myDataFrameName2>.head()\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}Into a dataframe\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1 \
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName2> = <myDataFrameName1>[["<myFeatureName1>", "<myFeatureName2>", "<myFeatureName3>"]]\
{\listtext	\uc0\u8259 	}<myDataFrameName2>.head()     # note if we did a single column it would be a series \
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	} \
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName2> = <myDataFrameName1>.loc[:,[\'93<myFeatureName1>\'94]]\
{\listtext	\uc0\u8259 	}<myDataFrameName2>.head()\
{\listtext	\uc0\u8259 	}\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}By rows\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.loc[[2,0]]           # label based indexing, note the algorithm isn\'92t actually looking for the index. It is doing string matching instead.\
{\listtext	\uc0\u8259 	}loc is typically used \
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.iloc[[0, 1, -1]]    # another example of positional indexing, note this prints the first two rows and the last row of the data frame\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}Range Selection\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}By Columns\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>[[\'93<myFeatureName1>\'94,\'94<myFeatureName2>\'94]] \
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}# Get the column names\
{\listtext	\uc0\u8259 	}<myColumnNames1>  = <myDataFrameName1>.columns[<myIndexInt1>:<myIndexInt2>]\
{\listtext	\uc0\u8259 	}# Then get the data columns \
{\listtext	\uc0\u8259 	} <myDataFrameName1>[<myColumnNames1>]\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}By Rows\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.iloc[<myIndexInt1>:<myIndexInt2>]\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}By Columns and Rows\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}<myIndexName1>  = <myDataFrameName1>.columns[<myInt1>:<myInt2>]  # returns an Index of column names\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}<myDataFrameSeriesName1>  = <myDataFrameName1>[<myIndexName1>] # returns a data frame series\
{\listtext	\uc0\u8259 	}<myDataFrameName2> =  <myDataFrameSeriesName1>.iloc[<myInt3>:<myInt4>] # returns data frame subsetted by rows and columns\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName2> = <myDataFrameName1>[<myDataFrameName1>.columns[<myInt1>:<myInt2>]].iloc[<myInt3>:<myInt4>]\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}Aggregating multiple metrics to a dataframe\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Filtering dataframe\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}df.loc[(df[\'93smoker"]=="No\'94)&(df["total_bill"] >= 10)]                                  # Filter rows by smoker == "No" and total_bill >= 10\
{\listtext	\uc0\u8259 	}df (["smoker","day","time"])["total_bill"].mean()                                            # What is the average total_bill for each value of smoker, day, and time?\
{\listtext	\uc0\u8259 	}df.groupby(["smoker","day","time"])["total_bill"].mean().reset_index()          # To get back to a Dataframe \
{\listtext	\uc0\u8259 	}df.loc[df["year"]==1967,["year", "pop"]]\
{\listtext	\uc0\u8259 	}df.loc[(df["year"]==1967) & (df["pop"] > 1_000_000),["year", "pop"]]         # Note: Python has a neat feature where it ignores underscores in integers to help visualize.\
{\listtext	\uc0\u8259 	}df.loc[(df["year"]==1967) | (df["pop"] > 1_000_000),["year", "pop"]]\
{\listtext	\uc0\u8259 	}df[billboard_melt["track"] == "Loser\'94]\
{\listtext	\uc0\u8259 	}df.groupby(["year"])["lifeExp"].mean() \
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}The aggregate function in the numpy library can do the same thing\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}df.groupby(["year"])["lifeExp"].agg(np.mean)\
{\listtext	\uc0\u8259 	}df.groupby(["year"])["lifeExp"].agg(np.std)\
{\listtext	\uc0\u8259 	}df.groupby(["year", "continent"])[["lifeExp", "gdpPercap"]].agg(np.mean)      # note, the created Dataframe becomes wonky when using a list\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Sorting a Multi-Index DataFrame (Important Concept)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrame1>.sort_values(\'93<myFeatureName1>\'94, ascending=True).sort_index(level=0)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrame1>.sort_values(["<myFeatureName1>", "<myFeatureName2>"], ascending=[1,0]).sort_index(level=1)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}MultiIndexing\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}How do I use the MultiIndex in pandas? by Data School ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=tcRGa2soc-c"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=tcRGa2soc-c}})\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrame1>.index \
{\listtext	\uc0\u8259 	} \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Time-Saving Tricks\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}4 new time-saving tricks in pandas by Data School ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=-NbY7E9hKxk"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=-NbY7E9hKxk}})\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Inplace\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}\cf13 \cb14 Note, \'93inplace=True\'94 saves the change to the data frame! ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=jvhD4B5zw-8"}}{\fldrslt 
\f3 \expnd0\expndtw0\kerning0
\ul \ulc15 https://www.youtube.com/watch?v=jvhD4B5zw-8}}
\f3 \expnd0\expndtw0\kerning0
\ul \ulc15  
\f0 \kerning1\expnd0\expndtw0 \ulnone ~02:00 min)\cf0 \cb1 \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Getting Average Values of Unique Categories within a Feature\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}<myDataFrame1>.groupby(["<myFeatureName1>"])["<myFeatureName2>"].mean()  # note, myFeatureName1 is categorical, myFeatureName2 is continuous\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Fixing Skewed Data\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}#log1p function applies log(1+x) to all elements of the column\
{\listtext	\uc0\u8259 	}<myDataFrame1>[\'93<myFeatureName1>\'94] = np.log1p(<myDataFrame1>[\'93<myFeatureName1>\'94])\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Unskewing Skewed Data\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}<myVarName1> = np.expm1(<myModelName1>.predict(<myXTest1>)) \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Tidy data (cleaning data)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "http://vita.had.co.nz/papers/tidy-data.pdf"}}{\fldrslt http://vita.had.co.nz/papers/tidy-data.pdf}} (first three sections are a good read)\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}Criteria 1 (what we don"t want): Column headers are values, not variable names\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}aka going from wide data to long data\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}pew_long = pd.melt(pew, id_vars="religion")       # doesn\'92t touch the religion column, and condenses the rest of the data frame into a \'93variable\'94 and \'93value\'94 column\
{\listtext	\uc0\u8259 	}pew_long = pd.melt(pew, id_vars="religion", var_name="income", value_name="count")   # renames the variable and value heading. \
{\listtext	\uc0\u8259 	}billboard_melt = pd.melt(billboard, id_vars=["year","artist","track","time","date.entered"], var_name=\'93week\'94, value_name=\'93rating\'94)        # many column example\
{\listtext	\uc0\u8259 	}billboard_melt = pd.melt(billboard, id_vars=["year","artist","track","time","date.entered"], var_name=\'93week\'94, value_name=\'93rating\'94).groupby("artist")["rating\'92].mean() \
{\listtext	\uc0\u8259 	}billboard_melt = pd.melt(billboard, id_vars=["year","artist","track","time","date.entered"], var_name=\'93week\'94, value_name=\'93rating\'94).groupby("artist")["rating\'92].mean().reset_index() \
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}Criteria 2 (what we don"t want): Multiple variables are stored in one column. #random keywords: parse\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}variable_split = ebola_long["variable"].str.split("_")        # Note, the \'93.str.\'94 is called an accessor. There is a commonly used \'93.dt.\'94 accessor too.\
{\listtext	\uc0\u8259 	}ebola_long["status"] = variable_split.str.get(0)\
{\listtext	\uc0\u8259 	}ebola_long["country"] = variable_split.str.get(1) \
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}or, you can do the above code all in one line with\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls90\ilvl6\cf0 {\listtext	\uc0\u8259 	}ebola_long[["status", "country"]] = (ebola_long["cd_country"].str.split("_", expand=True))             #this is also an example of concatenating\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}Criteria 3 (what we don"t want): Variables are stored in both rows and columns\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}Note, # if we see a lot of rows with repeated values, this is a symptom of this type of problem. \
{\listtext	\uc0\u8259 	}weather_melt = pd.melt(weather, id_vars=["id","year", "month", "element"], var_name="day", value_name="temp") \
{\listtext	\uc0\u8259 	}weather_tidy = weather_melt.pivot_table(index=["id","year", "month","day"], columns="element",values="temp")        # note, pivot_table is the opposite of melt. Also, by default, this drops NaN values.\
{\listtext	\uc0\u8259 	}weather_tidy\
{\listtext	\uc0\u8259 	}weather_tidy.reset_index()\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}Criteria 4 (what we don"t want): Multiple types of observational units are stored in the same table  / This is also called "normalization"\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}billboard_melt.loc[billboard_melt["track"] == "Loser"]\
{\listtext	\uc0\u8259 	}billboard_songs = billboard_melt[["year", "artist", "track", "time"]]\
{\listtext	\uc0\u8259 	}billboard_songs = billboard_songs.drop_duplicates()\
{\listtext	\uc0\u8259 	}billboard_songs["id"] = range(len(billboard_songs))\
{\listtext	\uc0\u8259 	}billboard_ratings = billboard_melt.merge(billboard_songs, on = ["year", "artist", "track", "time"]) # merges billboard_melt (left) and billboard_songs (right)\
{\listtext	\uc0\u8259 	}billboard_ratings = billboard_ratings[["id", "date.entered","week","rating"]]\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}Criteria 5 (what we don\'92t want): A single observational unit is stored in multiple tables\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Joining Tables/Datasets\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}billboard_songs["id"] = range(len(billboard_songs)) \
{\listtext	\uc0\u8259 	}billboard_ratings = billboard_melt.merge(billboard_songs, on = ["year", "artist", "track", "time\'92])    # merges billboard_melt (left) and billboard_songs (right)\
{\listtext	\uc0\u8259 	}billboard_ratings = billboard_ratings[["id", "date.entered","week","rating"]]\
{\listtext	\uc0\u8259 	}billboard_ratings.head()\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Saving to csv\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}billboard_songs.to_csv("billboard_songs.csv", index=False) \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Concatenating (similar to joining)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}X = np.concatenate((X_train,X_test), axis=0)   \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Functions\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}We use what is called an assert statement to test a function. This is the basis of unit testing.\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}assert my_sq(4) == 16              # if you run this code nothing happens, but if you change \'9316\'94 to \'9315\'94 the code will crash\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}Broadcasting\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}df[\'93a\'94] ** 2     # this squares every row in the column labeled a\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}With a data series (specific to one feature/column)\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}def my_sq(x):\
{\listtext	\uc0\u8259 	}    return x ** 2\
{\listtext	\uc0\u8259 	}df[\'93a\'94].apply(my_sq)\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}def my_sq(x, e):\
{\listtext	\uc0\u8259 	}    return x ** e\
{\listtext	\uc0\u8259 	}df[\'93a\'94].apply(my_sq, e)\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}With a data frame (applies a function to each entire column of a dataframe)\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1   # the apply statement here will print all data out in each column of the dataframe\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}def print_me(x):\
{\listtext	\uc0\u8259 	}    return x\
{\listtext	\uc0\u8259 	}df.apply(print_me)  \
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 2  #  Get the mean of every column\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}def avg_apply(col):\
{\listtext	\uc0\u8259 	}    return np.mean(col)\
{\listtext	\uc0\u8259 	}df.apply(avg_apply)\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 3 # Get the mean of every column \
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}def avg_apply(col):\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}    x = col[0]\
{\listtext	\uc0\u8259 	}    y = col[1]\
{\listtext	\uc0\u8259 	}    z = col[2]\
{\listtext	\uc0\u8259 	}    return (x+y+z)/3\
{\listtext	\uc0\u8259 	}df.apply(avg_apply)       # or we could do df.apply(avg_apply, axis=\'93columns\'94) to get the average of each row\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 4 # A more realistic example\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}@np.vectorize              # Important step. We have to vectorize our functions so it can pass rows one at a time. We write this at the top right before the function.\
{\listtext	\uc0\u8259 	}def avg_2_mod(x, y):\
{\listtext	\uc0\u8259 	}    if (x==20):\
{\listtext	\uc0\u8259 	}        return np.NaN\
{\listtext	\uc0\u8259 	}    else:\
{\listtext	\uc0\u8259 	}        return (x + y) / 2\
{\listtext	\uc0\u8259 	}avg_2_mod(df["a"], df["b"])        #if we use number we have to do avg_2_mod(df[\'93a\'94].values, df[\'93b\'94].values)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls90\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 5\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls90\ilvl5\cf0 {\listtext	\uc0\u8259 	}def extract_population(rate):\
{\listtext	\uc0\u8259 	}    population = rate.split("/")[1]\
{\listtext	\uc0\u8259 	}    return population                                            # if we want to return population as an integer we can just return it as int(population)\
{\listtext	\uc0\u8259 	}assert extract_population("123/456") == "456\'94   # quick way to check to make sure the function won\'92t fail       # random keywords: unit testing\
{\listtext	\uc0\u8259 	}tbl3["population"] = tbl3["rate"].apply(extract_population)                  # to apply the function to the column and create a new column called \'93population\'94\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls90\ilvl1\cf0 {\listtext	\uc0\u8259 	}Splitting into train and test set\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}IMPORTANT: Split into X_train, X_test, y_train, y_test like this (MAKE sure X and y are data frames! Not np.ndarray\'92s! It keeps X_train, etc. as data frames so you can still run X_train.head() after splitting ) \
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}X = df_model                             # entire data frame\
{\listtext	\uc0\u8259 	}y = df_model.tip_percent           # tip_percent (dependent / target var)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(\
{\listtext	\uc0\u8259 	}    X, y, test_size=0.2, random_state=seed)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls90\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls90\ilvl3\cf0 {\listtext	\uc0\u8259 	}  \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	} r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)]\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del <myDataFrameName1>\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0 \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Splitting the data\
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split \
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split( \
{\listtext	\uc0\u8259 	}    X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls91\ilvl3\cf0 {\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls92\ilvl3\cf0 {\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls93\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls94\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls95\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls96\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls97\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls98\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls99\ilvl1\cf0 {\listtext	\uc0\u8259 	}Grabbing specific feature values\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls99\ilvl2\cf0 {\listtext	\uc0\u8259 	}df["Trip_distance"][df["Trip_distance"]<50]\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls99\ilvl1\cf0 {\listtext	\uc0\u8259 	}Duplicating time column, then striping hour field of it, and putting the hour value into a new column\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls99\ilvl2\cf0 {\listtext	\uc0\u8259 	}df["pickup"] = df["lpep_pickup_datetime"].apply(lambda x: datetime.strptime(x, "%Y-%m-%d %H:%M:%S"))\
{\listtext	\uc0\u8259 	}df["pickup_hour"] = df["pickup"].apply(lambda x: x.hour) \
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls99\ilvl1\cf0 {\listtext	\uc0\u8259 	}Creating a new feature from an existing date time column (datetime64 type):\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls99\ilvl2\cf0 {\listtext	\uc0\u8259 	}X_train["pickup_day"] = X_train["pickup_datetime"].dt.day\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls99\ilvl1\cf0 {\listtext	\uc0\u8259 	}This puts, the average trip distance of every pickup hour into a new column\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls99\ilvl2\cf0 {\listtext	\uc0\u8259 	}df["avg_trip_dist_per_pickup_hour"] = df[["Trip_distance","pickup_hour"]].groupby("pickup_hour").mean()\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls99\ilvl1\cf0 {\listtext	\uc0\u8259 	}List Unique Values In A pandas Column\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls99\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://chrisalbon.com/python/data_wrangling/pandas_list_unique_values_in_column/"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://chrisalbon.com/python/data_wrangling/pandas_list_unique_values_in_column/}}\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls100\ilvl1\cf0 {\listtext	\uc0\u8259 	}select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls100\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls101\ilvl1
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}Groups two features, and provides the average of a third feature, and stores it into a dataframe\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls101\ilvl2\cf0 {\listtext	\uc0\u8259 	}summary_wdays_avg_duration = pd.DataFrame(df.groupby(["day_of_week","pickup_hour"])["Trip_distance"].mean())     (line 1)\
{\listtext	\uc0\u8259 	}summary_wdays_avg_duration              (line 2)\
{\listtext	\uc0\u8259 	}summary_wdays_avg_duration.reset_index(inplace = True)                 (resets the index for the data frame / makes one?)\
{\listtext	\uc0\u8259 	}summary_wdays_avg_duration["unit"]=1                (makes a new column with all one\'92s)\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls101\ilvl1\cf0 {\listtext	\uc0\u8259 	}Drop row or drop column\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls101\ilvl2\cf0 {\listtext	\uc0\u8259 	}train_X = X_train.drop(["Tip_amount", "tip_percent", "log_tip_percent"], axis=1)               # how to drop multiple columns\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/"}}{\fldrslt https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/}}\
{\listtext	\uc0\u8259 	}df = df[df["Tip_amount"] > 0]                                                     # removes all instances in dataframe where Tip_amount <= 0\
{\listtext	\uc0\u8259 	}Calculations with two columns \
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls101\ilvl3\cf0 {\listtext	\uc0\u8259 	}df["tip_percent"] = df["Tip_amount"]/df["Total_amount"] # calculate tip percentage \
{\listtext	\uc0\u8259 	}df["tip_percent"] = df["tip_percent"].apply(lambda x: x * 100) # multiply by 100 to get the %\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls101\ilvl2\cf0 {\listtext	\uc0\u8259 	}Python Pandas : Drop columns in DataFrame by label Names or by Index Positions\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls101\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://thispointer.com/python-pandas-drop-columns-in-dataframe-by-label-names-or-by-index-positions/"}}{\fldrslt https://thispointer.com/python-pandas-drop-columns-in-dataframe-by-label-names-or-by-index-positions/}}\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls101\ilvl2\cf0 {\listtext	\uc0\u8259 	}Drop Column\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls101\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls101\ilvl4\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.drop(\'91<myFeatureName1>', axis=1, inplace=True)\
\pard\tx1660\tx2160\tx2880\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls101\ilvl2\cf0 {\listtext	\uc0\u8259 	}Drop Index\
\pard\tx2380\tx2880\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls101\ilvl3\cf0 {\listtext	\uc0\u8259 	}See \'93Drop Column\'94\
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls101\ilvl1\cf0 {\listtext	\uc0\u8259 	}Good To Know\'92s When Plotting\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls101\ilvl2\cf0 {\listtext	\uc0\u8259 	}Always pay attention to what your objects are returning while looking at the documentation for matplotlib, seaborn, etc.\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls101\ilvl1\cf0 {\listtext	\uc0\u8259 	}Preparing data to plot with matplotlib, etc (note all features that go into the plot arguments have the size of \'93(some#, )\'94    )\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls101\ilvl2\cf0 {\listtext	\uc0\u8259 	}grouped_df = pd.DataFrame(df.groupby(["pickup_hour", "Airport"])["tip_percent"].aggregate(np.mean).reset_index())\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls101\ilvl1\cf0 {\listtext	\uc0\u8259 	}Quick plotting with Pandas\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls101\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls101\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.<myFeatureName1_categorical>.plot(kind=\'93bar\'94)\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls101\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls101\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.<myFeatureName1_continuous>.plot(kind=\'93hist\'94)\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls101\ilvl1\cf0 {\listtext	\uc0\u8259 	}Heatmaps\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls101\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html"}}{\fldrslt https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html}}\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://docs.scipy.org/doc/numpy/reference/generated/numpy.triu_indices.html#numpy.triu_indices"}}{\fldrslt https://docs.scipy.org/doc/numpy/reference/generated/numpy.triu_indices.html#numpy.triu_indices}}\
{\listtext	\uc0\u8259 	}Example 1 (shows one heat map)\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls101\ilvl3\cf0 {\listtext	\uc0\u8259 	}sns.set(style="white")\
{\listtext	\uc0\u8259 	}# Generate a large random dataset\
{\listtext	\uc0\u8259 	}df_model_copy = <myDataFrameName1>.copy() # our dataframe\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Compute the correlation matrix\
{\listtext	\uc0\u8259 	}corr = df_model_copy.corr() # corr calculation\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Generate a mask for the upper triangle. Note, we only want to show the relevant part\
{\listtext	\uc0\u8259 	}# of the heat map\
{\listtext	\uc0\u8259 	}mask = np.zeros_like(corr, dtype=np.bool)\
{\listtext	\uc0\u8259 	}mask[np.triu_indices_from(mask)] = True\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Generate a custom diverging colormap\
{\listtext	\uc0\u8259 	}cmap = sns.diverging_palette(220, 10, as_cmap=True)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Draw the heatmap with the mask and correct aspect ratio\
{\listtext	\uc0\u8259 	}sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\
{\listtext	\uc0\u8259 	}            square=True, linewidths=.5, cbar_kws=\{"shrink": .5\})\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}plt.show()\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls101\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2 (shows two different heat maps, one before data skewing, and one after)\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls101\ilvl3\cf0 {\listtext	\uc0\u8259 	}# Make sure we use the subsample in our correlation\
{\listtext	\uc0\u8259 	}f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Entire DataFrame\
{\listtext	\uc0\u8259 	}corr = <myDataFrameName1>.corr()\
{\listtext	\uc0\u8259 	}sns.heatmap(corr, cmap="coolwarm_r", annot_kws=\{"size":20\}, ax=ax1)\
{\listtext	\uc0\u8259 	}ax1.set_title("Imbalanced Correlation Matrix \\n (don"t use for reference)", fontsize=14)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}sub_sample_corr = <myDataFrameName2_withSkewCorrection>.corr()\
{\listtext	\uc0\u8259 	}sns.heatmap(sub_sample_corr, cmap="coolwarm_r", annot_kws=\{"size":20\}, ax=ax2)\
{\listtext	\uc0\u8259 	}ax2.set_title("SubSample Correlation Matrix \\n (use for reference)", fontsize=14)\
{\listtext	\uc0\u8259 	}plt.show()\
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls101\ilvl1\cf0 {\listtext	\uc0\u8259 	}Violin Plots\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls101\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls101\ilvl3\cf0 {\listtext	\uc0\u8259 	}# We will visualize all the continuous attributes using Violin Plot - a combination of box and density plots\
{\listtext	\uc0\u8259 	}# Creating a dataframe with only continuous features\
{\listtext	\uc0\u8259 	}data = <myDataFrameSetForContinuousVarOnly>.iloc[:, 116:] # Creating a dataframe with only continuous features\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}cols=data.columns \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Plot violin for all attributes in a 7x2 facetgrid \
{\listtext	\uc0\u8259 	}n_cols = 2\
{\listtext	\uc0\u8259 	}n_rows = 7\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(n_rows):\
{\listtext	\uc0\u8259 	}    fg, ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(12,8))\
{\listtext	\uc0\u8259 	}    for j in range(n_cols):\
{\listtext	\uc0\u8259 	}        sns.violinplot(y=cols[i*n_cols+j], data=dataset_train, ax=ax[j])\
{\listtext	\uc0\u8259 	} \
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls101\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls101\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Single Violin Plot\
{\listtext	\uc0\u8259 	}sns.violinplot(data=<myDataFrameName1>, y=\'93<myFeatureName1_continuous>\'93) \
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls101\ilvl1\cf0 {\listtext	\uc0\u8259 	}Subplotting (subplots)\
\pard\tx720\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\
\
\
\
\
\
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls102\ilvl1\cf0 {\listtext	\uc0\u8259 	}Pair Plots \
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls102\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1  (sns.pairplot)\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls102\ilvl3\cf0 {\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}data = <myDataFrameSetForContinuousVarOnly>.iloc[:, 116:] # Creating a dataframe with only continuous features\
{\listtext	\uc0\u8259 	}cols=data.columns                                                            # Getting the names of all the columns\
{\listtext	\uc0\u8259 	}data_corr = data.corr()                                                      # Calculating Pearson coefficient for all combinations\
{\listtext	\uc0\u8259 	}threshold = <myThresholdValue>                                     # Setting the threshold to select only highly correlated attributes. Eg, 0.5. \
{\listtext	\uc0\u8259 	}corr_list = []                                                                       # List of pairs along with correlation above threshold\
{\listtext	\uc0\u8259 	}size = <myIntegerOfFeaturesToBeConsidered>                          # Eg, 15. \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Searching for the highly correlated pairs\
{\listtext	\uc0\u8259 	}for i in range(0, size):                                                       #for continuous features\
{\listtext	\uc0\u8259 	}    for j in range(i+1, size):\
{\listtext	\uc0\u8259 	}        if (data_corr.iloc[i,j] >= threshold and data_corr.iloc[i,j] < 1) or (data_corr.iloc[i,j] < 0 and data_corr.iloc[i,j] <= -threshold):\
{\listtext	\uc0\u8259 	}            corr_list.append([data_corr.iloc[i,j],i,j])                   # stores coefficient and appropriate column indexes\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Sorting to show higher ones first            \
{\listtext	\uc0\u8259 	}s_corr_list = sorted(corr_list,key=lambda x: -abs(x[0])).    # See key function, https://docs.python.org/3/howto/sorting.html\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for v, i, j in s_corr_list: \
{\listtext	\uc0\u8259 	}    print("%s and %s = %.2f" % (cols[i],cols[j],v))\
{\listtext	\uc0\u8259 	}                    \
{\listtext	\uc0\u8259 	}# Scatter plot of all the highly correlated pairs\
{\listtext	\uc0\u8259 	}for v, i, j in s_corr_list:\
{\listtext	\uc0\u8259 	}    sns.pairplot(dataset_train, size=6, x_vars=cols[i], y_vars=cols[j])\
{\listtext	\uc0\u8259 	}    plt.show\
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls102\ilvl1\cf0 {\listtext	\uc0\u8259 	}Boxplots\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls102\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1 (box plots with hours)\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls102\ilvl3\cf0 {\listtext	\uc0\u8259 	}f, axes = plt.subplots(ncols=4, figsize=(20,4))\
{\listtext	\uc0\u8259 	}# Negative Correlations with our <myFeatureName1_categorical> (The lower our feature value the more likely it will be a fraud transaction)\
{\listtext	\uc0\u8259 	}sns.boxplot(x=\'93<myFeatureName1_categorical>\'94, y="<myFeatureName2_continuous>", data=<myDataFrameName1>, palette=colors, ax=axes[0])\
{\listtext	\uc0\u8259 	}axes[0].set_title("<myFeatureName2_continuous> vs <myFeatureName1_categorical> Negative Correlation")\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}sns.boxplot(x="<myFeatureName1_categorical>", y="<myFeatureName3_continuous>", data=<myDataFrameName1>, palette=colors, ax=axes[1])\
{\listtext	\uc0\u8259 	}axes[1].set_title("<myFeatureName3_continuous> vs <myFeatureName1_categorical> Negative Correlation")\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}sns.boxplot(x="<myFeatureName1_categorical>", y="<myFeatureName4_continuous>", data=<myDataFrameName1>, palette=colors, ax=axes[2])\
{\listtext	\uc0\u8259 	}axes[2].set_title("<myFeatureName4_continuous> vs <myFeatureName1_categorical> Negative Correlation")\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}sns.boxplot(x="<myFeatureName1_categorical>", y="<myFeatureName5_continuous>", data=<myDataFrameName1>, palette=colors, ax=axes[3])\
{\listtext	\uc0\u8259 	}axes[3].set_title("<myFeatureName5_continuous> vs <myFeatureName1_categorical> Negative Correlation")\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}plt.show()\
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls102\ilvl1\cf0 {\listtext	\uc0\u8259 	}Histograms \
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls102\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0"}}{\fldrslt https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0}}\
{\listtext	\uc0\u8259 	}Example 1  # a quick way to generate a histogram within Pandas    (plot)\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls102\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.<myFeatureName1_continuous>.plot(kind="hist")            \
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls102\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2   # with seaborn, good for continuous variables (distplot)\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls102\ilvl3\cf0 {\listtext	\uc0\u8259 	}sns.distplot(<myDataFrameName1>.<myFeatureName1_continuous>)       # this plots a histogram plot along with the kernel density shape\
{\listtext	\uc0\u8259 	}sns.distplot(<myDataFrameName1>.<myFeatureName1_continuous>, kde=False)       # plots only a histogram\
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls102\ilvl1\cf0 {\listtext	\uc0\u8259 	}Scatter Plot (with Linear fits)\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls102\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1 # using seaborn, this shows a scatter plot and applies a linear fit\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls102\ilvl3\cf0 {\listtext	\uc0\u8259 	}sns.lmplot(x=\'93<myFeatureName1_continuous>\'94, y=\'93<myFeatureName2_continuous>\'94, data=<myDataFrameName1>)      # note, sns.lmplot returns back an entire figure whereas sns.regplot can be used for subplotting/in axes form (still a little confused, but overall I think sns.regplot is typically used when subplotting)\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls102\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2 # using seaborn, this shows the data as a scatter plot and colors the scattered data based on categories in myFeatureName3 (note, there are two linear fits applied to this plot if <myFeatureName3> is binary). \
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls102\ilvl3\cf0 {\listtext	\uc0\u8259 	}sns.lmplot(x=\'93<myFeatureName1_continuous>\'94, y=\'93<myFeatureName2_continuous>\'94, data=<myDataFrameName1>, hue=\'93<myFeatureName3_categorical>\'94)\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls102\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 3 # using seaborn, this shows the data as a scatter plot and colors the scattered data based on categories in myFeatureName3 (note, there are two linear fits applied to this plot if <myFeatureName3> is binary).  The addition of the col argument <myFeatureName4> separates the data into two plots. This categorical var must match the categorical var in <myFeatureName3>\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls102\ilvl3\cf0 {\listtext	\uc0\u8259 	}sns.lmplot(x=\'93<myFeatureName1_continuous>\'94, y=\'93<myFeatureName2_continuous>\'94, data=<myDataFrameName1>, hue=\'93<myFeatureName3_categorical>\'94, col=\'93<myFeatureName4_categorical>\'94)\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls102\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 4 # similar to above, but including the row argument turns this into a FacetGrid type plot (think of an 8x2 grid, etc). This is a great way to plot using multiple vars. Note <myFeatureName3>, <myFeatureName4>, <myFeatureName5> can be binary, binary, multi categorical, respectively. \
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls102\ilvl3\cf0 {\listtext	\uc0\u8259 	}sns.lmplot(x=\'93<myFeatureName1_continuous>\'94, y=\'93<myFeatureName2_continuous>\'94, data=<myDataFrameName1>, hue=\'93<myFeatureName3_categorical>\'94, col=\'93<myFeatureName4_categorical>\'94, row=\'93<myFeatureName5_categorical>\'94)\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls102\ilvl2\cf0 {\listtext	\uc0\u8259 	}Exampe 5 \
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls102\ilvl3\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls102\ilvl1\cf0 {\listtext	\uc0\u8259 	}FacetGrid / Subplotting\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls102\ilvl2\cf0 {\listtext	\uc0\u8259 	}Scatter Plot\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls102\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls102\ilvl4\cf0 {\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}<someVar1> = sns.FacetGrid(<myDataFrameName1>, col="<myFeatureName1_categorical>", row=\'93<myFeatureName2_categorical>\'94, hue="<myFeatureName3_categorical>\'94)\
{\listtext	\uc0\u8259 	}<someVar1>.map(plt.scatter, \'93<myFeatureName4_continuous>\'94, \'93<myFeatureName5_continuous>\'94)       #think of "map()" similar to how "apply()" works. Here, map() provides/applies the data to each subplot in the facet grid\
\pard\tx1660\tx2160\tx2880\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls102\ilvl2\cf0 {\listtext	\uc0\u8259 	}Histogram and Scatter Plot (with linear fit)\
\pard\tx2380\tx2880\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls102\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls102\ilvl4\cf0 {\listtext	\uc0\u8259 	}fig, (ax1, ax2) = plt.subplots(1,2)\
{\listtext	\uc0\u8259 	}sns.distplot(<myDataFrameName1>.<myFeatureName1_continuous>, ax=ax1) \
{\listtext	\uc0\u8259 	}sns.regplot(x="<myFeatureName2_continuous>",y="<myFeatureName1_continuous>", data=<myDataFrameName1>, ax=ax2)    # note, this seems very similar to sns.lmplot\
{\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls102\ilvl1\cf0 {\listtext	\uc0\u8259 	}Bar Graph / Count Plot\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls102\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1 # a quick way to generate a bar plot within Pandas\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls102\ilvl3\cf0 {\listtext	\uc0\u8259 	}cts = <myDataFrameName1>.<myFeatureName1_categorical>.value_counts\
{\listtext	\uc0\u8259 	}cts.plot(kind=\'93bar\'94)\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls102\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2 # with seaborn\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls102\ilvl3\cf0 {\listtext	\uc0\u8259 	}sns.countplot(x=\'93<myFeatureName1_categorical>\'94, data = <myDataFrameName1>)\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls102\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 3 # FacetGrid type with seaborn\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls102\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}n_rows = <myIntOfRows>\
{\listtext	\uc0\u8259 	}n_cols = <myIntOfColumns>\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(n_rows):\
{\listtext	\uc0\u8259 	}    fg, ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(16,8))\
{\listtext	\uc0\u8259 	}    for j in range(n_cols):\
{\listtext	\uc0\u8259 	}        sns.countplot(x=cols[i*n_cols+j], data=<myDataFrameName1_categorical>, ax=ax[j])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls102\ilvl1\cf0 {\listtext	\uc0\u8259 	}Dendogram\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls102\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1 \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls102\ilvl1\cf0 {\listtext	\uc0\u8259 	}Modeling\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls102\ilvl2\cf0 {\listtext	\uc0\u8259 	}Regression\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls102\ilvl3\cf0 {\listtext	\uc0\u8259 	}Linear \
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls102\ilvl4\cf0 {\listtext	\uc0\u8259 	}Linear Regression\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls102\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls102\ilvl6\cf0 {\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}from sklearn.linear_model import LinearRegression\
{\listtext	\uc0\u8259 	}<myDataFrameName1> = sns.load_dataset("tips") \
{\listtext	\uc0\u8259 	}<myDataFrameName1>.head() \
{\listtext	\uc0\u8259 	}<myModel1> = LinearRegression() \
{\listtext	\uc0\u8259 	}<myModel1>.fit(X=<myDataFrameName1>[["<myFeatureName1>","<myFeatureName2>"]], y=<myDataFrameName1>["<myFeatureName3>"]) \
{\listtext	\uc0\u8259 	}<myModel1>.coef_  \
{\listtext	\uc0\u8259 	}<myModel1>.intercept_ \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# LEARNING:\
{\listtext	\uc0\u8259 	}# For every one dollar increase in total_bill, given that\
{\listtext	\uc0\u8259 	}# the size remains the same, the tip increases by 9 cents.\
{\listtext	\uc0\u8259 	}\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls102\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls102\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}	\
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	} r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# LINEAR REGRESSION (Linear Algo) \
{\listtext	\uc0\u8259 	}from sklearn.linear_model import LinearRegression\
{\listtext	\uc0\u8259 	}lin_reg = LinearRegression(n_jobs=-1) # using all processors \
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls103\ilvl6\cf0 {\listtext	\uc0\u8259 	}algo = "LR" \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}for name, i_cols_list in X_all: \
{\listtext	\uc0\u8259 	}    print(name)\
{\listtext	\uc0\u8259 	}    lin_reg.fit(X_train[:, i_cols_list], y_train) #fitting all features to the target column\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls104\ilvl6\cf0 {\listtext	\uc0\u8259 	}    result = mean_absolute_error(np.expm1(y_test), np.expm1(lin_reg.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}    mae.append(result)\
{\listtext	\uc0\u8259 	}    print(name + " %s" % result) \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}comb.append(algo) \
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls105\ilvl6\cf0 {\listtext	\uc0\u8259 	}print(comb) \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# #Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}# fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}# plt.plot(mae)\
{\listtext	\uc0\u8259 	}# #Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}# ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}# ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}# #Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}# plt.show() \
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls106\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls107\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls108\ilvl4\cf0 {\listtext	\uc0\u8259 	}Logistic Regression\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls108\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls108\ilvl6\cf0 {\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}from sklearn.linear_model import LogisticRegression\
{\listtext	\uc0\u8259 	}<myDataFrameName1> = sns.load_dataset("titanic")\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.head()\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.info()\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.drop([\'93<myFeatureName1>\'94], axis=1).head()\
{\listtext	\uc0\u8259 	}<myDataFrameName2> = <myDataFrameName1>[["<myFeatureName2>", "<myFeatureName3>", "<myFeatureName4>"]] \
{\listtext	\uc0\u8259 	}<myDataFrameName2>.head()\
{\listtext	\uc0\u8259 	}<myDataFrameName3> = pd.get_dummies(<myDataFrameName2>, drop_first=True)\
{\listtext	\uc0\u8259 	}<myDataFrameName3>.head()\
{\listtext	\uc0\u8259 	}<myXTrain1> = <myDataFrameName3>.iloc[:,1:]\
{\listtext	\uc0\u8259 	}<myXTrain1>.head()\
{\listtext	\uc0\u8259 	}<myYTrain1> = <myDataFrameName3>.iloc[:,0]\
{\listtext	\uc0\u8259 	}<myYTrain1>.head()\
{\listtext	\uc0\u8259 	}<myModel1> = LogisticRegression(random_state=0, solver="lbfgs", multi_class="multinomial") \
{\listtext	\uc0\u8259 	}<myModel1>.fit(<myXTrain1>, <myYTrain1>)\
{\listtext	\uc0\u8259 	}<myModel1>.coef_\
{\listtext	\uc0\u8259 	} \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls108\ilvl3\cf0 {\listtext	\uc0\u8259 	}Non-Linear\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls108\ilvl4\cf0 {\listtext	\uc0\u8259 	}KNN\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls108\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls108\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	} r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# KNN (Non-linear Algo)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Evaluation of various combinations of KNN\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Fitting Classifier to the Training set\
{\listtext	\uc0\u8259 	}from sklearn.neighbors import KNeighborsRegressor     \
{\listtext	\uc0\u8259 	}# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Add the N value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}n_list = np.array([]) #note, when the list is empty, the algo doesnt run\
{\listtext	\uc0\u8259 	}'''\
{\listtext	\uc0\u8259 	}With n_list = np.array([5])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}All 1434.788795356784\
{\listtext	\uc0\u8259 	}['LR', 'KNN 5']\
{\listtext	\uc0\u8259 	}'''\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}'''\
{\listtext	\uc0\u8259 	}With n_list = np.array([2])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}All 1526.7442802258656\
{\listtext	\uc0\u8259 	}['LR', 'KNN 2']\
{\listtext	\uc0\u8259 	}'''\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# we can use multiple values into n_list if we want to search for the optimal n_neighbors. However, xgboost is usally the best for parameter tuning.\
{\listtext	\uc0\u8259 	}for n_neighbors in n_list:\
{\listtext	\uc0\u8259 	}    # Setting the base model\
{\listtext	\uc0\u8259 	}    regressor = KNeighborsRegressor(n_neighbors=n_neighbors,n_jobs=-1)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "KNN"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name, i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        regressor.fit(X_train[:, i_cols_list], y_train) #fitting all features to the target column\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(regressor.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % n_neighbors)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}print(comb)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# since we know the outcome, we can skip the algorithm and append the result\
{\listtext	\uc0\u8259 	}if (len(n_list)==0):\
{\listtext	\uc0\u8259 	}    mae.append(1527)\
{\listtext	\uc0\u8259 	}    comb.append("KNN" + " %s" % 2 )\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Set figure size, this figure compares mae for all of the algorithms ran\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Very high computation time\
{\listtext	\uc0\u8259 	}#Best estimated performance is 1745 for n=1\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# LEARNING:\
{\listtext	\uc0\u8259 	}# KNN 5 performed the best. Lowest MAE. \
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls108\ilvl4\cf0 {\listtext	\uc0\u8259 	}Decision Tree (CART)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls108\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls108\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	} r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# CART (Non-linear Algo)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	} #Evaluation of various combinations of CART\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import the library\
{\listtext	\uc0\u8259 	}from sklearn.tree import DecisionTreeRegressor\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Add the max_depth value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}d_list = np.array([])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for max_depth in d_list:\
{\listtext	\uc0\u8259 	}    #Set the base model\
{\listtext	\uc0\u8259 	}    model = DecisionTreeRegressor(max_depth=max_depth,random_state=seed)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "CART"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % max_depth )\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}# since we know the outcome, we can skip the algorithm and append the result\
{\listtext	\uc0\u8259 	}if (len(d_list)==0):\
{\listtext	\uc0\u8259 	}    mae.append(1741)\
{\listtext	\uc0\u8259 	}    comb.append("CART" + " %s" % 5 )    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#High computation time\
{\listtext	\uc0\u8259 	}#Best estimated performance is 1741 for depth=5 \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls108\ilvl4\cf0 {\listtext	\uc0\u8259 	}Support Vector Machine (SVM, SVR)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls108\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1  \
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls108\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} # https://medium.com/@pushkarmandot/what-is-the-significance-of-c-value-in-support-vector-machine-28224e852c5a\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	} r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	} # SVM (Non-linear Algo)\
{\listtext	\uc0\u8259 	}# https://medium.com/@pushkarmandot/what-is-the-significance-of-c-value-in-support-vector-machine-28224e852c5a\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import the library\
{\listtext	\uc0\u8259 	}from sklearn.svm import SVR\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Add the C value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}c_list = np.array([])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for C in c_list:\
{\listtext	\uc0\u8259 	}    #Set the base model\
{\listtext	\uc0\u8259 	}    model = SVR(C=C)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "SVM"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % C )\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#very very high computation time, not running\
{\listtext	\uc0\u8259 	} \
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls108\ilvl4\cf0 {\listtext	\uc0\u8259 	}Bagged Decision Trees (Bagging)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls108\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls108\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	}r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} # Bagged Decision Trees (Bagging)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Evaluation of various combinations of Bagged Decision Trees\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import the library\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import BaggingRegressor\
{\listtext	\uc0\u8259 	}#from sklearn.tree import DecisionTreeRegressor\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Add the n_estimators value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}n_list = np.array([])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for n_estimators in n_list:\
{\listtext	\uc0\u8259 	}    #Setting the base model\
{\listtext	\uc0\u8259 	}    model = BaggingRegressor(n_jobs=-1,n_estimators=n_estimators)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "Bag"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % n_estimators )\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#very high computation time, not running\
{\listtext	\uc0\u8259 	} \
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls108\ilvl4\cf0 {\listtext	\uc0\u8259 	}Random Forest (Bagging) # note, we use bagging to find the sweet spot between a simple and  complex model (see bias vs variance tradeoff)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls108\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls108\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	} r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Random Forest (Bagging)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Evaluation of various combinations of RandomForest\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import the library\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import RandomForestRegressor\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Add the n_estimators value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}n_list = np.array([])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for n_estimators in n_list:\
{\listtext	\uc0\u8259 	}    #Set the base model\
{\listtext	\uc0\u8259 	}    model = RandomForestRegressor(n_jobs=-1,n_estimators=n_estimators,random_state=seed)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "RF"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_testl[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % n_estimators )\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}# since we know the outcome, we can skip the algorithm and append the result\
{\listtext	\uc0\u8259 	}if (len(n_list)==0):\
{\listtext	\uc0\u8259 	}    mae.append(1213)\
{\listtext	\uc0\u8259 	}    comb.append("RF" + " %s" % 50 )    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Best estimated performance is 1213 when the number of estimators is 50\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls108\ilvl4\cf0 {\listtext	\uc0\u8259 	}Extra Trees (Bagging) # note, we use bagging to find the sweet spot between a simple and  complex model (see bias vs variance tradeoff)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls108\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls108\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}  \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	}r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	}  \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Extra Trees (Bagging)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Evaluation of various combinations of ExtraTrees\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import the library\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import ExtraTreesRegressor\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Add the n_estimators value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}n_list = np.array([])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for n_estimators in n_list:\
{\listtext	\uc0\u8259 	}    #Set the base model\
{\listtext	\uc0\u8259 	}    model = ExtraTreesRegressor(n_jobs=-1,n_estimators=n_estimators,random_state=seed)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "ET"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % n_estimators )\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}# since we know the outcome, we can skip the algorithm and append the result\
{\listtext	\uc0\u8259 	}if (len(n_list)==0):\
{\listtext	\uc0\u8259 	}    mae.append(1254)\
{\listtext	\uc0\u8259 	}    comb.append("ET" + " %s" % 100 )    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Best estimated performance is 1254 for 100 estimators \
{\listtext	\uc0\u8259 	}\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls108\ilvl4\cf0 {\listtext	\uc0\u8259 	}Adaboost # note, we use bagging to find the sweet spot between a simple and complex model (see bias vs variance tradeoff)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls108\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls108\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	}r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Evaluation of various combinations of AdaBoost\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import the library\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import AdaBoostRegressor\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Add the n_estimators value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}n_list = np.array([])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for n_estimators in n_list:\
{\listtext	\uc0\u8259 	}    #Set the base model\
{\listtext	\uc0\u8259 	}    model = AdaBoostRegressor(n_estimators=n_estimators,random_state=seed)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "Ada"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % n_estimators )\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}# since we know the outcome, we can skip the algorithm and append the result\
{\listtext	\uc0\u8259 	}if (len(n_list)==0):\
{\listtext	\uc0\u8259 	}    mae.append(1678)\
{\listtext	\uc0\u8259 	}    comb.append("Ada" + " %s" % 100 )    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Best estimated performance is 1678 with n=100 \
{\listtext	\uc0\u8259 	} \
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls108\ilvl4\cf0 {\listtext	\uc0\u8259 	}Stochastic Gradient Boosting (Boosting) # note, we use bagging to find the sweet spot between a simple and  complex model (see bias vs variance tradeoff)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls108\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls108\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	}r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Evaluation of various combinations of SGB\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import the library\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import GradientBoostingRegressor\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Add the n_estimators value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}n_list = np.array([])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for n_estimators in n_list:\
{\listtext	\uc0\u8259 	}    #Set the base model\
{\listtext	\uc0\u8259 	}    model = GradientBoostingRegressor(n_estimators=n_estimators,random_state=seed)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "SGB"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_testl[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % n_estimators )\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# since we know the outcome, we can skip the algorithm and append the result\
{\listtext	\uc0\u8259 	}if (len(n_list)==0):\
{\listtext	\uc0\u8259 	}    mae.append(1278)\
{\listtext	\uc0\u8259 	}    comb.append("SGB" + " %s" % 50 )    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Best estimated performance is ?\
{\listtext	\uc0\u8259 	} \
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls108\ilvl4\cf0 {\listtext	\uc0\u8259 	}XGBoost # note, we use bagging to find the sweet spot between a simple and  complex model (see bias vs variance tradeoff)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls108\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls108\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} # Getting the number of rows and columns\
{\listtext	\uc0\u8259 	}r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} #XGBoost\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Evaluation of various combinations of XGB\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import the library\
{\listtext	\uc0\u8259 	}from xgboost import XGBRegressor\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Add the n_estimators value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}n_list = np.array([])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for n_estimators in n_list:\
{\listtext	\uc0\u8259 	}    #Set the base model\
{\listtext	\uc0\u8259 	}    model = XGBRegressor(n_estimators=n_estimators,seed=seed)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "XGB"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % n_estimators )\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# since we know the outcome, we can skip the algorithm and append the result\
{\listtext	\uc0\u8259 	}if (len(n_list)==0):\
{\listtext	\uc0\u8259 	}    mae.append(1169)\
{\listtext	\uc0\u8259 	}    comb.append("XGB" + " %s" % 1000 )    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Best estimated performance is 1169 with n=1000\
{\listtext	\uc0\u8259 	} \
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls108\ilvl4\cf0 {\listtext	\uc0\u8259 	}Multi-layer Perceptrons (Deep Learning)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls108\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls108\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	}r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#MLP (Deep Learning)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Evaluation of various combinations of multi-layer perceptrons\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import libraries for deep learning\
{\listtext	\uc0\u8259 	}from keras.wrappers.scikit_learn import KerasRegressor\
{\listtext	\uc0\u8259 	}from keras.models import Sequential\
{\listtext	\uc0\u8259 	}from keras.layers import Dense\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# define baseline model\
{\listtext	\uc0\u8259 	}def baseline(v):\
{\listtext	\uc0\u8259 	}     # create model\
{\listtext	\uc0\u8259 	}     model = Sequential()\
{\listtext	\uc0\u8259 	}     model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	}     model.add(Dense(1, init='normal'))\
{\listtext	\uc0\u8259 	}     # Compile model\
{\listtext	\uc0\u8259 	}     model.compile(loss='mean_absolute_error', optimizer='adam')\
{\listtext	\uc0\u8259 	}     return model\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# define smaller model\
{\listtext	\uc0\u8259 	}def smaller(v):\
{\listtext	\uc0\u8259 	}     # create model\
{\listtext	\uc0\u8259 	}     model = Sequential()\
{\listtext	\uc0\u8259 	}     model.add(Dense(v*(c-1)/2, input_dim=v*(c-1), init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	}     model.add(Dense(1, init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	}     # Compile model\
{\listtext	\uc0\u8259 	}     model.compile(loss='mean_absolute_error', optimizer='adam')\
{\listtext	\uc0\u8259 	}     return model\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# define deeper model\
{\listtext	\uc0\u8259 	}def deeper(v):\
{\listtext	\uc0\u8259 	} # create model\
{\listtext	\uc0\u8259 	} model = Sequential()\
{\listtext	\uc0\u8259 	} model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	} model.add(Dense(v*(c-1)/2, init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	} model.add(Dense(1, init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	} # Compile model\
{\listtext	\uc0\u8259 	} model.compile(loss='mean_absolute_error', optimizer='adam')\
{\listtext	\uc0\u8259 	} return model\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Optimize using dropout and decay\
{\listtext	\uc0\u8259 	}from keras.optimizers import SGD\
{\listtext	\uc0\u8259 	}from keras.layers import Dropout\
{\listtext	\uc0\u8259 	}from keras.constraints import maxnorm\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}def dropout(v):\
{\listtext	\uc0\u8259 	}    #create model\
{\listtext	\uc0\u8259 	}    model = Sequential()\
{\listtext	\uc0\u8259 	}    model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu',W_constraint=maxnorm(3)))\
{\listtext	\uc0\u8259 	}    model.add(Dropout(0.2))\
{\listtext	\uc0\u8259 	}    model.add(Dense(v*(c-1)/2, init='normal', activation='relu', W_constraint=maxnorm(3)))\
{\listtext	\uc0\u8259 	}    model.add(Dropout(0.2))\
{\listtext	\uc0\u8259 	}    model.add(Dense(1, init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	}    # Compile model\
{\listtext	\uc0\u8259 	}    sgd = SGD(lr=0.1,momentum=0.9,decay=0.0,nesterov=False)\
{\listtext	\uc0\u8259 	}    model.compile(loss='mean_absolute_error', optimizer=sgd)\
{\listtext	\uc0\u8259 	}    return model\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# define decay model\
{\listtext	\uc0\u8259 	}def decay(v):\
{\listtext	\uc0\u8259 	}    # create model\
{\listtext	\uc0\u8259 	}    model = Sequential()\
{\listtext	\uc0\u8259 	}    model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	}    model.add(Dense(1, init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	}    # Compile model\
{\listtext	\uc0\u8259 	}    sgd = SGD(lr=0.1,momentum=0.8,decay=0.01,nesterov=False)\
{\listtext	\uc0\u8259 	}    model.compile(loss='mean_absolute_error', optimizer=sgd)\
{\listtext	\uc0\u8259 	}    return model\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}est_list = []\
{\listtext	\uc0\u8259 	}#uncomment the below if you want to run the algo\
{\listtext	\uc0\u8259 	}#est_list = [('MLP',baseline),('smaller',smaller),('deeper',deeper),('dropout',dropout),('decay',decay)]\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for name, est in est_list:\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}    algo = name\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for m,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model = KerasRegressor(build_fn=est, v=1, nb_epoch=10, verbose=0)\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo )\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}# since we know the outcome, we can skip the algorithm and append the result\
{\listtext	\uc0\u8259 	}if (len(est_list)==0):\
{\listtext	\uc0\u8259 	}    mae.append(1168)\
{\listtext	\uc0\u8259 	}    comb.append("MLP" + " baseline" )    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}print("mae--> %s" % mae)\
{\listtext	\uc0\u8259 	}print("comb--> %s" % comb)\
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}plt.plot(mae)\
{\listtext	\uc0\u8259 	}#Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}#Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Best estimated performance is MLP=1168\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx1660\tx2160\tx2880\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls108\ilvl2\cf0 {\listtext	\uc0\u8259 	}Classification\
{\listtext	\uc0\u8259 	}Model Selection\
\pard\tx2380\tx2880\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls108\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls108\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\'93\'94\'94\
{\listtext	\uc0\u8259 	}Since XGBRegressor is showing the best performance, we can select it as our best model. Therefore, we now need to finalize the model with all of the available data.\
{\listtext	\uc0\u8259 	}\'93\'94\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# note, X_train and X_test are both coming from the training set CSV. axis=0 is stacking rows on top of one another.\
{\listtext	\uc0\u8259 	}X = np.concatenate((X_train,X_test), axis=0) \
{\listtext	\uc0\u8259 	}del X_train\
{\listtext	\uc0\u8259 	}del X_test\
{\listtext	\uc0\u8259 	}Y = np.concatenate((y_train,y_test),axis=0)\
{\listtext	\uc0\u8259 	}del y_train\
{\listtext	\uc0\u8259 	}del y_test\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print("I am here 0 - debug")\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}n_estimators = 1000\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Best model definition\
{\listtext	\uc0\u8259 	}best_model = XGBRegressor(n_estimators=n_estimators,seed=seed)\
{\listtext	\uc0\u8259 	}print("I am here 0.0 - debug")\
{\listtext	\uc0\u8259 	}best_model.fit(X,Y)\
{\listtext	\uc0\u8259 	}print("I am here 0.1 - debug")\
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del Y\
{\listtext	\uc0\u8259 	}#Read test dataset\
{\listtext	\uc0\u8259 	}dataset_test = pd.read_csv("test.csv")\
{\listtext	\uc0\u8259 	}print("I am here 0.2 - debug")\
{\listtext	\uc0\u8259 	}#Drop unnecessary columns\
{\listtext	\uc0\u8259 	}ID = dataset_test['id']\
{\listtext	\uc0\u8259 	}dataset_test.drop('id',axis=1,inplace=True)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#One hot encode all categorical attributes\
{\listtext	\uc0\u8259 	}cats = []\
{\listtext	\uc0\u8259 	}print("I am here 1 - debug")\
{\listtext	\uc0\u8259 	}for i in range(0, split):\
{\listtext	\uc0\u8259 	}    # label encoding\
{\listtext	\uc0\u8259 	}    label_encoder = LabelEncoder()\
{\listtext	\uc0\u8259 	}    label_encoder.fit(labels[i])\
{\listtext	\uc0\u8259 	}    feature = label_encoder.transform(dataset_test.iloc[:,i])\
{\listtext	\uc0\u8259 	}    feature = feature.reshape(dataset_test.shape[0], 1)\
{\listtext	\uc0\u8259 	}    #One hot encoding\
{\listtext	\uc0\u8259 	}    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))\
{\listtext	\uc0\u8259 	}    feature = onehot_encoder.fit_transform(feature)\
{\listtext	\uc0\u8259 	}    cats.append(feature)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print("I am here 2 - debug")\
{\listtext	\uc0\u8259 	}# Making a 2D array from a list of 1D arrays\
{\listtext	\uc0\u8259 	}encoded_cats = np.column_stack(cats)\
{\listtext	\uc0\u8259 	}del cats\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Concatenating encoded attributes with continous attributes\
{\listtext	\uc0\u8259 	}X_test = np.concatenate((encoded_cats, dataset_test.iloc[:,split:].values), axis=1)\
{\listtext	\uc0\u8259 	}print("I am here 3 - debug")\
{\listtext	\uc0\u8259 	}del encoded_cats\
{\listtext	\uc0\u8259 	}del dataset_test\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Making predictions using the best model now\
{\listtext	\uc0\u8259 	}predictions = np.expm1(best_model.predict(X_test))\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx720\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone \'97\'97\'97\'97\'97\
DATA WRANGLING SNIPPETS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls109\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Python\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls109\ilvl1\cf0 {\listtext	\uc0\u8259 	}Pandas\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls109\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls109\ilvl3\cf0 {\listtext	\uc0\u8259 	}Jupyter Notebook Cell 1\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls109\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}import pandas as pd\
{\listtext	\uc0\u8259 	}import numpy as np\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}import warnings\
{\listtext	\uc0\u8259 	}warnings.filterwarnings('ignore')\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}pd.set_option('display.max_columns', None)\
{\listtext	\uc0\u8259 	}pd.set_option('display.max_rows', None)\
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls109\ilvl3\cf0 {\listtext	\uc0\u8259 	}Jupyter Notebook Cell 2\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls109\ilvl4\cf0 {\listtext	\uc0\u8259 	}  \
{\listtext	\uc0\u8259 	}# copy, paste, and run in Jupyter cell to see dataframe better\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}d  = \{"Item":["Component 1","Component 1","Component 1","Component 1","Component 1","Component 1","Component 1","Component 1","Component 1","Component 2","Component 2","Component 2","Component 2","Component 2","Component 2"], "System":["System 01", "System 01", "System 01", "System 01","System 02","System 02","System 03","System 03","System 03","System 01","System 01","System 01", "System 02","System 02","System 02"],"failureObservation":["1990-03-19","1990-09-24","1996-02-19","1990-07-16","1995-07-31","2002-07-07","1998-03-11","2014-10-31","1990-01-30","1993-11-16","1995-01-30","1995-05-28","1996-09-03","1997-03-14","1999-01-10"]\}\
{\listtext	\uc0\u8259 	}df = pd.DataFrame(data=d)\
{\listtext	\uc0\u8259 	}df \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls110\ilvl3\cf0 {\listtext	\uc0\u8259 	}Jupyter Notebook Cell 3\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls110\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}df.info() \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls111\ilvl3\cf0 {\listtext	\uc0\u8259 	}Jupyter Notebook Cell 4\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls111\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# converting string object to datetime\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}df["failureObservation"] = pd.to_datetime(df.failureObservation) \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls112\ilvl3\cf0 {\listtext	\uc0\u8259 	}Jupyter Notebook Cell 5\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls112\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}df.info() \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls113\ilvl3\cf0 {\listtext	\uc0\u8259 	}Jupyter Notebook Cell 6\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls113\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}df.set_index(["Item","System"], inplace=True) \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls114\ilvl3\cf0 {\listtext	\uc0\u8259 	}Jupyter Notebook Cell 7\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls114\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}df \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls115\ilvl3\cf0 {\listtext	\uc0\u8259 	}Jupyter Notebook Cell 8\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls115\ilvl4\cf0 {\listtext	\uc0\u8259 	}  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# number of failures\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}df.groupby(level=[0,1]).size() \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls116\ilvl3\cf0 {\listtext	\uc0\u8259 	}Jupyter Notebook Cell 9\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls116\ilvl4\cf0 {\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# view max date\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}df.groupby(level=[0,1]).failureObservation.max() \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls117\ilvl3\cf0 {\listtext	\uc0\u8259 	}Jupyter Notebook Cell 10\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls117\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# view min date  \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	} df.groupby(level=[0,1]).failureObservation.min()\
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls118\ilvl3\cf0 {\listtext	\uc0\u8259 	}Jupyter Notebook Cell 11\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls118\ilvl4\cf0 {\listtext	\uc0\u8259 	}  \
{\listtext	\uc0\u8259 	}# day difference\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}df.groupby(level=[0,1]).failureObservation.max() - df.groupby(level=[0,1]).failureObservation.min()\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls119\ilvl3\cf0 {\listtext	\uc0\u8259 	}Jupyter Notebook Cell 9\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls119\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# day difference \'97> hour difference \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}(df.groupby(level=[0,1]).failureObservation.max() - df.groupby(level=[0,1]).failureObservation.min()).dt.total_seconds() / 3600 \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls120\ilvl3\cf0 {\listtext	\uc0\u8259 	}Jupyter Notebook Cell 13\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls120\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#  failures / hour\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}df.groupby(level=[0,1]).size() / ((df.groupby(level=[0,1]).failureObservation.max() - df.groupby(level=[0,1]).failureObservation.min()).dt.total_seconds() / 3600)\
{\listtext	\uc0\u8259 	}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\'97\'97\'97\'97\'97\
PostgreSQL (psql)\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls121\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Downloads\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls121\ilvl1\cf0 {\listtext	\uc0\u8259 	}PostgreSQL \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls121\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://postgresapp.com/downloads.html"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \ulc3 https://postgresapp.com/downloads.html}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \ulc3 \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls121\ilvl2
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}Good Reads\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls121\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.dataquest.io/blog/loading-data-into-postgres/"}}{\fldrslt https://www.dataquest.io/blog/loading-data-into-postgres/}}\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls121\ilvl2\cf0 {\listtext	\uc0\u8259 	}To Create a Database\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls121\ilvl3\cf0 {\listtext	\uc0\u8259 	}Start PostgreSQL > click on \'94anthony pendleton\'94 database > copy, paste, and run given command line > then execute the command \'93CREATE DATABASE <myNewDatabaseName>;\'93\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls121\ilvl4\cf0 {\listtext	\uc0\u8259 	}Note, this creates a database separate from the anthony pendleton database\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls121\ilvl4\cf0 {\listtext	\uc0\u8259 	}Command Line\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls121\ilvl3\cf0 {\listtext	\uc0\u8259 	}Create Database\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls121\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls121\ilvl5\cf0 {\listtext	\uc0\u8259 	}CREATE DATABASE <myNewDatabaseName>;\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls121\ilvl3\cf0 {\listtext	\uc0\u8259 	}List Databases\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls121\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls121\ilvl5\cf0 {\listtext	\uc0\u8259 	}\\l\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls121\ilvl3\cf0 {\listtext	\uc0\u8259 	}To exit psql in Terminal\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls121\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls121\ilvl5\cf0 {\listtext	\uc0\u8259 	}\\q\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls121\ilvl1\cf0 {\listtext	\uc0\u8259 	}pgAdmin 4 (GUI, Mac OS app (see small icon on the top menu bar))\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls121\ilvl2\cf0 {\listtext	\uc0\u8259 	}Tutorials\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls121\ilvl3\cf0 {\listtext	\uc0\u8259 	}Intro To PostgreSQL Databases With PgAdmin For Beginners - Full Course by Codemy.com ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=Dd2ej-QKrWY"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=Dd2ej-QKrWY}})\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls121\ilvl2\cf0 {\listtext	\uc0\u8259 	}Create Database\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls121\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1 (localhost on Mac, {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=lG2Nes-wi54"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=lG2Nes-wi54}})\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls121\ilvl4\cf0 {\listtext	\uc0\u8259 	}launch Postgres app > make sure server is running > launch pgAdmin 4 app > right click \'93servers\'94 > click \'93create\'94 > click \'93server\'94\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls121\ilvl5\cf0 {\listtext	\uc0\u8259 	}Tab Information\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls121\ilvl6\cf0 {\listtext	\uc0\u8259 	}General Tab\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls121\ilvl7\cf0 {\listtext	\uc0\u8259 	}Enter name of server \'97> <myServerName1>\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls121\ilvl6\cf0 {\listtext	\uc0\u8259 	}Connection\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls121\ilvl7\cf0 {\listtext	\uc0\u8259 	}Host name/address \'97> localhost OR 127.0.0.1 (same thing)\
{\listtext	\uc0\u8259 	}Password \'97> <myPassword>\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls121\ilvl4\cf0 {\listtext	\uc0\u8259 	}Now we can see the list of databases that are present in the Postgres app\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls122\ilvl1\cf0 {\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}Python\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls122\ilvl2\cf0 {\listtext	\uc0\u8259 	}Create Table and Import CSV Data\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls122\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}import psycopg2\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# connecting to the db\
{\listtext	\uc0\u8259 	}myDb = psycopg2.connect(\
{\listtext	\uc0\u8259 	}        host = "localhost",\
{\listtext	\uc0\u8259 	}        database = "sample_db",\
{\listtext	\uc0\u8259 	}        user = "postgres",\
{\listtext	\uc0\u8259 	}        password = "postgres"\
{\listtext	\uc0\u8259 	}        )\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}myCursor = myDb.cursor()\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# CREATE TABLE myCSV\
{\listtext	\uc0\u8259 	}myCursor.execute("""\
{\listtext	\uc0\u8259 	}                 CREATE TABLE myCSV(\
{\listtext	\uc0\u8259 	}                 pccn VARCHAR (255) NULL,\
{\listtext	\uc0\u8259 	}                 plisn VARCHAR (255) NULL,\
{\listtext	\uc0\u8259 	}                 item_name VARCHAR (255) NULL,\
{\listtext	\uc0\u8259 	}                 unit_price VARCHAR (255) NULL,\
{\listtext	\uc0\u8259 	}                 failure_rate VARCHAR (255) NULL,\
{\listtext	\uc0\u8259 	}                 next_higher_plisn VARCHAR (255) NULL,\
{\listtext	\uc0\u8259 	}                 qty_per_assembly VARCHAR (255) NULL)\
{\listtext	\uc0\u8259 	}                 """)\
{\listtext	\uc0\u8259 	}               \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# LOAD CSV FILE DATA TO myCSV TABLE\
{\listtext	\uc0\u8259 	}with open(\'93<myFolderName1>/<myFolderName2>/<myCSVFileName>.csv","r") as f:\
{\listtext	\uc0\u8259 	}    # Notice that we don't need the `csv` module.\
{\listtext	\uc0\u8259 	}    next(f) # Skip the header row.\
{\listtext	\uc0\u8259 	}    myCursor.copy_from(f, 'myCSV', sep=',')\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# COMMIT ALL\
{\listtext	\uc0\u8259 	}myDb.commit()\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# CLOSE OUR CONNECTION WITH THE DATABASE\
{\listtext	\uc0\u8259 	}myDb.close()\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls122\ilvl2\cf0 {\listtext	\uc0\u8259 	}Drop Table\
{\listtext	\uc0\u8259 	}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\'97\'97\'97\'97\'97\
SQL GENERAL\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls123\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Good Reads\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}Client-Server Model \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Client%E2%80%93server_model"}}{\fldrslt https://en.wikipedia.org/wiki/Client%E2%80%93server_model}}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}Data Redundancy and Data Inconsistency\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://pediaa.com/what-is-the-difference-between-data-redundancy-and-data-inconsistency/"}}{\fldrslt https://pediaa.com/what-is-the-difference-between-data-redundancy-and-data-inconsistency/}} \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}Data redundancy can lead to data inconsistency\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls123\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}General\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}Each \'93flavor\'94 of SQL (for example, MySQL and PostgreSQL) use different data types when creating table.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Therefore, see documentation for each whenever creating a table.\
{\listtext	\uc0\u8259 	}Note, most of their SQL commands conform to the SQL standard commands\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls123\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}PostreSQL\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}Create Table\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1 (note, if loading in a csv file, the csv file must contain 0,1,2,3 in first column if primary key is being used)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}CREATE TABLE <myTableName1>(\
 	\uc0\u8259 	                         id SERIAL PRIMARY KEY,\
 	\uc0\u8259 			   <myFeatureName1> VARCHAR (255) NULL,\
{\listtext	\uc0\u8259 	}		   <myFeatureName2> VARCHAR (255) NULL,\
{\listtext	\uc0\u8259 	}		   <myFeatureName3> VARCHAR (255) NULL,\
{\listtext	\uc0\u8259 	}		   <myFeatureName4> VARCHAR (255) NULL,\
{\listtext	\uc0\u8259 	}		   <myFeatureName5> INT (255) NULL\
{\listtext	\uc0\u8259 	}		  )\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}Drop Table\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}DROP TABLE <myTableName1>\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2 (If there are dependent objects, eg, a function)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}DROP TABLE IF EXISTS <myTableName1> CASCADE\
{\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}Highest Value in Feature\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}SELECT MAX(<myFeatureName1>) FROM <myTableName1>\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}How to Find the Nth Highest Salary\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}Common Table Expressions (CTE) (Transaction-SQL)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}CTE in sql server Part 49 by kudvenkat ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=ZXB5b-7HJHk"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=ZXB5b-7HJHk}})\
{\listtext	\uc0\u8259 	}Part 3 How does a recursive CTE work by kudvenkat ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=N3ChrpDRcXY&list=PL6n9fhu94yhXcztdLO7i6mdyaegC8CJwR&index=3"}}{\fldrslt https://www.youtube.com/watch?v=N3ChrpDRcXY&list=PL6n9fhu94yhXcztdLO7i6mdyaegC8CJwR&index=3}})\
{\listtext	\uc0\u8259 	}Derived tables and common table expressions in sql server Part 48 by kudvenkat ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=ZXB5b-7HJHk"}}{\fldrslt https://www.youtube.com/watch?v=ZXB5b-7HJHk}})\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}Insert Row to Existing Table\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}INSERT INTO <myTableName1>(\
{\listtext	\uc0\u8259 	}    <myFeatureName1>,\
{\listtext	\uc0\u8259 	}    <myFeatureName2>,\
{\listtext	\uc0\u8259 	}    <myFeatureName3>,\
{\listtext	\uc0\u8259 	}    <myFeatureName4>\
{\listtext	\uc0\u8259 	})\
{\listtext	\uc0\u8259 	}VALUES ('<myStringName1>', '<myStringName2>', '<myStringName1>',<myInt1>)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}ADD COLUMN / ALTER TABLE  (Add Column to Existing Table)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1	\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}ALTER TABLE <myTableName1>\
{\listtext	\uc0\u8259 	}ADD COLUMN <myNewFeatureName> VARCHAR; \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}UPDATE SET (eg, creating a new data column in an existing table)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}UPDATE <myTableName1>\
{\listtext	\uc0\u8259 	}SET <myExistingFeatureNameToUpdate> = concat_ws(', ', <myFeatureName1>, <myFeatureName2>)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}ORDER BY\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1 \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}SELECT <myFeatureName1>, <myFeatureName2> from <myTableName1>\
{\listtext	\uc0\u8259 	}ORDER BY <myFeatureName3> DESC   /* or ASC for ascending */\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}SELECT * from <myTableName1>\
{\listtext	\uc0\u8259 	}ORDER BY employee_name ASC\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}DISTINCT\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}SELECT DISTINCT <myFeatureName1> FROM <myTableName1>\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}SELECT DISTINCT manager FROM <myTableName1> ORDER BY DESC\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}WHERE\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}SELECT * FROM <myTableName1> WHERE <myFeatureName1> = '<myStringName1>' AND (<myFeatureName2> = \'91<myStringName2>\'92 OR <myFeatureName2> = \'91<myStringName3>\'92) AND <myFeatureName4> = \'91<myStringName4>\'92\
{\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}Comparison Operators (can be used with WHERE clause)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Arithmatic\
{\listtext	\uc0\u8259 	}Comparison\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}Not Equal Operator\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls123\ilvl4\cf0 {\listtext	\uc0\u8259 	}<>\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls123\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls123\ilvl6\cf0 {\listtext	\uc0\u8259 	}SELECT 1 <> 1 /* returns false */\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls123\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls123\ilvl6\cf0 {\listtext	\uc0\u8259 	}SELECT 'hello' <> 'hellO\'92  /* returns true */ \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}Equal \
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls123\ilvl4\cf0 {\listtext	\uc0\u8259 	}=\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}Less Than of Equal To \
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls123\ilvl4\cf0 {\listtext	\uc0\u8259 	}<=\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Logical\
{\listtext	\uc0\u8259 	}Bitwise\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}LIMIT (Note, the LIMIT keyword is not used in all SQL databases, see FETCH instead)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}SELECT * FROM <myTableName1> LIMIT 3  /* returns first three rows */ \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}OFFSET\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}SELECT * FROM <myTableName1> OFFSET 2 LIMIT 3  /* skips first two rows and returns the next three rows */ \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}SELECT * FROM <myTableName1> OFFSET 2 /* skips first two rows and returns all of the remaining rows */ \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}FETCH (SQL standard, in PostgreSQL you can use the LIMIT command as well)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}SELECT * FROM employee OFFSET 2 FETCH FIRST ROW ONLY \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}IN\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1a (what NOT to do)	\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}SELECT * FROM <myTableName1> \
{\listtext	\uc0\u8259 	}WHERE <myFeatureName1> = \'91<myStringName1>\'92 OR \
{\listtext	\uc0\u8259 	}<myFeatureName1> = '<myStringName2>' OR <myFeatureName1> = '<myStringName3>' ORDER BY <myFeatureName1> ASC\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1b (what TO do)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}SELECT * FROM <myTableName1>\
{\listtext	\uc0\u8259 	}WHERE <myFeatureName1> IN (\'91<myStringName1>\'92,\'92<myStringName2>\'92,\'92<myStringName3>\'92) ORDER BY manager ASC\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}\uc0\u914 \u917 \u932 WEEN\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}SELECT * FROM <myTableName1>\
{\listtext	\uc0\u8259 	}WHERE <myFeatureName1>\
{\listtext	\uc0\u8259 	}BETWEEN DATE '<myStringName1>' AND '<myStringName2>' \
{\listtext	\uc0\u8259 	}/*eg, BETWEEN DATE '2000-01-01' AND '2015-01-01' */\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}LIKE\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls123\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls123\ilvl3\cf0 {\listtext	\uc0\u8259 	}SELECT * FROM <myTableName1>\
{\listtext	\uc0\u8259 	}WHERE <myFeatureName1>\
{\listtext	\uc0\u8259 	}LIKE \'91%google%\'92      /*eg, returns all email addresses containing the word google*/\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls123\ilvl1\cf0 {\listtext	\uc0\u8259 	}iLIKE\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls124\ilvl3\cf0  \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\
SAS/SPSS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls125\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Fantastic YouTube Channels\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls125\ilvl1\cf0 {\listtext	\uc0\u8259 	}Research By Design\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls125\ilvl2\cf0 {\listtext	\uc0\u8259 	}Statistical Significance vs. Effect Size ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/playlist?list=PLVI_iGT5ZuRlCryAkbFeRiutKec8ck4Qk"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/playlist?list=PLVI_iGT5ZuRlCryAkbFeRiutKec8ck4Qk}})  # Playlist\
{\listtext	\uc0\u8259 	}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls126\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}One Way Anova Interpretation\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls126\ilvl1\cf0 {\listtext	\uc0\u8259 	}How to do a One-Way ANOVA in SPSS (12-6)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls126\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=rS3k8ONVN-o"}}{\fldrslt https://www.youtube.com/watch?v=rS3k8ONVN-o}}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls126\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls126\ilvl4\cf0 {\listtext	\uc0\u8259 	}Following a series of complaints about wicked witches, the Wizard of Oz conducts a study to determine if certain regions of Oz have more problems with wicked witches than other regions. He randomly surveys five Munchkins from each of four regions and records the number of complaints he received about wicked witchiness from each one. For example, each Munchkin may say he/she received anywhere from 1-5 complaints\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls126\ilvl5\cf0 {\listtext	\uc0\u8259 	}Before we start interpreting our ANOVA results, we should first check the assumptions for the test. One of which is\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls126\ilvl6\cf0 {\listtext	\uc0\u8259 	}Homogeneity of Variances	\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls126\ilvl7\cf0 {\listtext	\uc0\u8259 	}Which is tested by performing a Levene Test (see my other video {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=4mkEZxgxMRA"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=4mkEZxgxMRA}})\
{\listtext	\uc0\u8259 	}Since our results show the the the Significance of the Levene\'92s test is 0.918 this means our group variances are homogenous (not related, this is great)\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls126\ilvl8\cf0 {\listtext	\uc0\u8259 	}This means we will not need the Welch ANOVA test or the Games Howell either.\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls126\ilvl5\cf0 {\listtext	\uc0\u8259 	}Now we can proceed to the ANOVA / F Table\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls126\ilvl6\cf0 {\listtext	\uc0\u8259 	}Note, the outputted table is not in APA format. Never use raw SPSS output in place of APA formatted tables.\
{\listtext	\uc0\u8259 	}Since we used the compare means and not the general linear model option\
{\listtext	\uc0\u8259 	}To interpret and report this ANOVA we would write (see ~07:00)\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls126\ilvl7\cf0 {\listtext	\uc0\u8259 	}F(3,16) = 10.49, p < 0.001, \uc0\u951 ^2_p = 0.66\
\pard\tx5980\tx6480\tx7200\tx7920\tx8640\li6480\fi-6480\pardirnatural\partightenfactor0
\ls126\ilvl8\cf0 {\listtext	\uc0\u8259 	}note \uc0\u951 ^2_p is the \'93effect size\'94 (see my other video for more info {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=RkbmA6WszTo&list=PLVI_iGT5ZuRlCryAkbFeRiutKec8ck4Qk&index=5"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=RkbmA6WszTo&list=PLVI_iGT5ZuRlCryAkbFeRiutKec8ck4Qk&index=5}})\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls126\ilvl5\cf0 {\listtext	\uc0\u8259 	}Moving on, in the generated output, we can look at the Welch\'92s ANOVA \'93Robust Tests of Equality of Means\'94\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls126\ilvl6\cf0 {\listtext	\uc0\u8259 	}Note, we only report Welch\'92s ANOVA if Levene\'92s test is significant so the assumption of equality of variances has been violated.\
{\listtext	\uc0\u8259 	}If Welch\'92s ANOVA shows significance, we can then use Games-Howell post hoc (see my other video that goes over this situation {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=oagLeAOaevk"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=oagLeAOaevk}})\
{\listtext	\uc0\u8259 	}For now, we can ignore the Welch\'92s ANOVA / Robust Tests of Equality of Means because we do not need it\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls126\ilvl5\cf0 {\listtext	\uc0\u8259 	}So one region/case is different than the others, we need to look at the Post Hoc results / multiple comparisons to see which one\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls126\ilvl6\cf0 {\listtext	\uc0\u8259 	}Each region compared to the other regions can easily be seen here\
{\listtext	\uc0\u8259 	}We see the North Region and South Region have a very high significance value (therefore, they are very homogenous/similar)\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls126\ilvl7\cf0 {\listtext	\uc0\u8259 	}But, note that the North Region vs East and West Region(s) show a low significance value (therefore, they are different)\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls126\ilvl6\cf0 {\listtext	\uc0\u8259 	}We could continue on with these comparisons or we could evaluate the  homogenous subsets (eg, North vs South)\
{\listtext	\uc0\u8259 	}SPSS will generate output for homogenous groups (in the output)\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls126\ilvl7\cf0 {\listtext	\uc0\u8259 	}In this different table we see, a mean comparison between homogeneous groups. Note, since North vs South had a high significance value, East vs West will too\
{\listtext	\uc0\u8259 	}Example of how we would write the results in APA format\
{\listtext	\uc0\u8259 	} \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls126\ilvl1\cf0 {\listtext	\uc0\u8259 	}Standard Error (note, the std error can also be thought of as the standard deviation of a bunch of sampled means from a given population (see StatQuickie: Standard Deviation vs Standard Error by Josh Starmer ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=A82brFpdr9g&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=13"}}{\fldrslt https://www.youtube.com/watch?v=A82brFpdr9g&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=13}}))) \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls126\ilvl2\cf0 {\listtext	\uc0\u8259 	}Standard Error by Bozeman Science {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=BwYj69LAQOI"}}{\fldrslt https://www.youtube.com/watch?v=BwYj69LAQOI}}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls126\ilvl3\cf0 {\listtext	\uc0\u8259 	}stdError = stdDev / sqrt(n)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls126\ilvl4\cf0 {\listtext	\uc0\u8259 	}where StdDev and n is the number of observations/measurements\
{\listtext	\uc0\u8259 	}Note, the more observations we have, the lower the standard error will become\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls126\ilvl1\cf0 {\listtext	\uc0\u8259 	}Standard Deviation by Bozeman Science {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=09kiX3p5Vek"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=09kiX3p5Vek}}\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls126\ilvl2\cf0 {\listtext	\uc0\u8259 	}stdDev = sqrt( summation( (x - x_mean)^2 ) / (n -1) ) # Note this is for 
\f2\i sample
\f0\i0  variance, not 
\f2\i population
\f0\i0  variance\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls126\ilvl3\cf0 {\listtext	\uc0\u8259 	}where x is each sample in data set, x_mean is the average of the samples, n is the number of samples in our data\
{\listtext	\uc0\u8259 	}Why we normally divide by n-1 {\field{\*\fldinst{HYPERLINK "https://www.khanacademy.org/math/ap-statistics/summarizing-quantitative-data-ap/more-standard-deviation/v/another-simulation-giving-evidence-that-n-1-gives-us-an-unbiased-estimate-of-variance"}}{\fldrslt https://www.khanacademy.org/math/ap-statistics/summarizing-quantitative-data-ap/more-standard-deviation/v/another-simulation-giving-evidence-that-n-1-gives-us-an-unbiased-estimate-of-variance}}\
{\listtext	\uc0\u8259 	}Variance and Std Deviation | Why divide by n-1 by zedstatistics {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=wpY9o_OyxoQ"}}{\fldrslt https://www.youtube.com/watch?v=wpY9o_OyxoQ}} (great video)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls126\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls126\ilvl5\cf0 {\listtext	\uc0\u8259 	}Objective: Describe the spread of the data\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls126\ilvl6\cf0 {\listtext	\uc0\u8259 	}Out intuition tells us to just subtract the highest and lowest observations to get the range\
{\listtext	\uc0\u8259 	}However, this can be susceptible to outliers. Therefore, we need a way to incorporate the rest of the observations.\
{\listtext	\uc0\u8259 	}First thought: We can solve for the mean and the look at the average deviation from it!\
{\listtext	\uc0\u8259 	}However, this will give us negative deviations for the left side of the mean while the right side will give positive.\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls126\ilvl7\cf0 {\listtext	\uc0\u8259 	}Note, by definition, the sum of the deviations to the left and to the right will be \ul zero\ulnone . Hence, why we square the residuals from the mean.\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls126\ilvl6\cf0 {\listtext	\uc0\u8259 	}Second though: Let\'92s find the average squared deviation from the mean\
\pard\tx5260\tx5760\tx6480\tx7200\tx7920\tx8640\li5760\fi-5760\pardirnatural\partightenfactor0
\ls126\ilvl7\cf0 {\listtext	\uc0\u8259 	}Note, there is a topic called \'93moments\'94 that addresses why it is better to square the differences instead of using the absolute value ~6:00\
{\listtext	\uc0\u8259 	}Also, we would think to get the average squared deviation we will need to divide by just n (not n - some#)\
{\listtext	\uc0\u8259 	}So why is it so common to divide by n-1?\
{\listtext	\uc0\u8259 	}See video around ~6:00 for explanation\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls126\ilvl3\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls126\ilvl2\cf0 {\listtext	\uc0\u8259 	}Std deviation measures the variance in the data\
{\listtext	\uc0\u8259 	}Note, variance = (standard deviation)^2\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\'97\'97\'97\'97\'97\
ARTIFICIAL INTELLIGENCE\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls127\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Path Finding AI\
{\listtext	
\f1 \uc0\u9642 
\f0 	}Cool Projects\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls127\ilvl1\cf0 {\listtext	\uc0\u8259 	}Training Forest To Run Circles\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls127\ilvl2\cf0 {\listtext	\uc0\u8259 	}WRITING MY FIRST MACHINE LEARNING GAME! (1/4) by Jabrils {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=ZX2Hyu5WoFg"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=ZX2Hyu5WoFg}}
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul \
\ls127\ilvl2
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}WRITING MY FIRST MACHINE LEARNING GAME! (2/4) by Jabrils {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=OpodKCR6P-M"}}{\fldrslt https://www.youtube.com/watch?v=OpodKCR6P-M}} \
{\listtext	\uc0\u8259 	}WRITING MY FIRST MACHINE LEARNING GAME! (3/4) by Jabrils {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=GDy45vT1xlA"}}{\fldrslt 
\f3 \cf3 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=GDy45vT1xlA}}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
__________\
ARTIFICIAL INTELLIGENCE vs MACHINE LEARNING vs DATA SCIENCE\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls128\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Artificial Intelligence\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls128\ilvl1\cf0 {\listtext	\uc0\u8259 	}What is Artificial Intelligence (AI)?\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls128\ilvl2\cf0 {\listtext	\uc0\u8259 	}Artificial intelligence refers to the simulation of a human brain function by machines. This is achieved by creating an artificial neural network that can show human intelligence. The primary human functions that an AI machine performs include logical reasoning, learning and self-correction. Artificial intelligence is a wide field with many applications but it also one of the most complicated technology to work on. Machines inherently are not smart and to make them so, we need a lot of computing power and data to empower them to simulate human thinking.\
{\listtext	\uc0\u8259 	}Artificial intelligence is classified into two parts, general Artificial Intelligence and Narrow Artificial Intelligence. General AI refers to making machines intelligent in a wide array of activities that involve thinking and reasoning. Narrow AI, on the other hand, involves the use of artificial intelligence for a very specific task. For instance, general AI would mean an algorithm that is capable of playing all kinds of board game while narrow AI will limit the range of machine capabilities to a specific game like chess or scrabble. Currently, only narrow AI is within the reach of developers and researchers. General AI is just a dream of researchers and perception among the masses that will take a lot of time for the human race to achieve (if ever possible).\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls128\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Machine Learning\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls128\ilvl1\cf0 {\listtext	\uc0\u8259 	}What is Machine Learning (ML)?\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls128\ilvl2\cf0 {\listtext	\uc0\u8259 	}Machine learning is the ability of a computer system to learn from the environment and improve itself from experience without the need for any explicit programming. Machine learning focuses on enabling algorithms to learn from the data provided, gather insights and make predictions on previously unanalyzed data using the information gathered. Machine learning can be performed using multiple approaches. The three basic models of machine learning are supervised, unsupervised and reinforcement learning.\
{\listtext	\uc0\u8259 	}In case of supervised learning, labeled data is used to help machines recognize characteristics and use them for future data. For instance, if you want to classify pictures of cats and dogs then you can feed the data of a few labeled pictures and then the machine will classify all the remaining pictures for you.  On the other hand, in unsupervised learning, we simply put unlabeled data and let machine understand the characteristics and classify it. Reinforcement machine learning algorithms interact with the environment by producing actions and then analyze errors or rewards. For example, to understand a game of chess an ML algorithm will not analyze individual moves but will study the game as a whole.\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls128\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Data Science\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\'97\'97\'97\'97\'97\
TERMINAL \
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls129\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Commonly Used Commands\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls129\ilvl1\cf0 {\listtext	\uc0\u8259 	}find file\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls129\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls129\ilvl3\cf0 {\listtext	\uc0\u8259 	}sudo find / -name <myFile>.<myFileExtension> -print\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls129\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls129\ilvl3\cf0 {\listtext	\uc0\u8259 	}sudo find / -name *.<myFileExtension> -print    # eg, find all files with an extension of .csv\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls129\ilvl1\cf0 {\listtext	\uc0\u8259 	}find directory\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls129\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls129\ilvl3\cf0 {\listtext	\uc0\u8259 	}\cf16 \CocoaLigature0 sudo find / -name <myDirectoryName> -type d -print #eg, mysql (case sensitive)\cf0 \CocoaLigature1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\'97\
IGNORE\
\
\
\
Reading and processing CSV file chunk wise with memory constraint. The trouble I am running into is that currently each line already exceeds the memory constraint. Therefore, how can I process bit-wise information of each line while meeting the memory constraint.\
\
\
First off, I hope the strategy I am using to solve this problem is correct. \
\
**WHAT I HAVE**\
\
```\
list_1 = ["5","1","6","1","2","5"]\
list_2 = ["1","3","9","15","16","16"]\
```\
\
*Please note the duplicates in the lists*\
\
**WHAT I WANT**\
\
\
I am trying to create a dictionary where the uniques values from list 1 are the keys, and the average of the corresponding elements in list 2 are the values.\
\
In other words, the dictionary I am after is the following:\
\
```\
\{"1": 9.0, "2": 16, "5": 8.5, "6": 9.0\}\
```\
\
I am not looking specifically for someone to just give me the answer (although that would be appreciated). Therefore, if someone could just point me in the right direction or give me some topics to look into that would be great.}