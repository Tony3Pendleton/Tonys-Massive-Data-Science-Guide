{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf500
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 LucidaGrande;\f2\froman\fcharset0 Times-Roman;
\f3\fswiss\fcharset0 Helvetica-Bold;\f4\fswiss\fcharset0 Helvetica-Oblique;\f5\fnil\fcharset0 STIXGeneral-Regular;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue233;\red35\green255\blue6;\red0\green0\blue0;
\red0\green0\blue0;\red252\green87\blue8;\red253\green139\blue9;\red255\green255\blue11;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c93333;\cssrgb\c0\c100000\c0;\cssrgb\c0\c1\c1;
\cssrgb\c0\c0\c0;\cssrgb\c100000\c43066\c0;\cssrgb\c100000\c61456\c0;\cssrgb\c100000\c100000\c0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid203\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid204\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid301\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid304\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid305\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid306\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid403\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid404\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid405\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid406\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid407\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid803\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid804\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid805\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid9}
{\list\listtemplateid10\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid10}
{\list\listtemplateid11\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1003\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1004\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1005\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid11}
{\list\listtemplateid12\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid12}
{\list\listtemplateid13\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid13}
{\list\listtemplateid14\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1304\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1305\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1306\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listname ;}\listid14}
{\list\listtemplateid15\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1402\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1403\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1404\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1405\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1406\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1407\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid15}
{\list\listtemplateid16\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1503\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1504\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid16}
{\list\listtemplateid17\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1603\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1604\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid17}
{\list\listtemplateid18\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1704\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid18}
{\list\listtemplateid19\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1803\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid19}
{\list\listtemplateid20\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1903\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1904\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid20}
{\list\listtemplateid21\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid2001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2003\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2004\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2005\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2006\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listname ;}\listid21}
{\list\listtemplateid22\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2104\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid22}
{\list\listtemplateid23\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid23}
{\list\listtemplateid24\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid24}
{\list\listtemplateid25\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2403\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2404\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2405\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2406\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listname ;}\listid25}
{\list\listtemplateid26\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid2501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2503\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2504\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2505\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid26}
{\list\listtemplateid27\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid2601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid27}
{\list\listtemplateid28\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2701\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid28}
{\list\listtemplateid29\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2801\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2803\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2804\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid29}
{\list\listtemplateid30\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid2901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid2903\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid30}
{\list\listtemplateid31\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid3001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3003\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3004\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid31}
{\list\listtemplateid32\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid3101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid32}
{\list\listtemplateid33\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid3201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3203\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3204\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid33}
{\list\listtemplateid34\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid3301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3304\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid34}
{\list\listtemplateid35\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid3401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid35}
{\list\listtemplateid36\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid3501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid36}
{\list\listtemplateid37\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid3601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid37}
{\list\listtemplateid38\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid3701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid38}
{\list\listtemplateid39\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid3801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid39}
{\list\listtemplateid40\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid3901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid3902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid40}
{\list\listtemplateid41\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid4001\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid41}
{\list\listtemplateid42\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4101\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid42}
{\list\listtemplateid43\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid4201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid43}
{\list\listtemplateid44\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid4301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid4302\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid44}
{\list\listtemplateid45\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4401\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4403\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4404\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid45}
{\list\listtemplateid46\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid4501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid46}
{\list\listtemplateid47\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid4601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4603\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4604\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid47}
{\list\listtemplateid48\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid4701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4704\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid48}
{\list\listtemplateid49\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid4801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4803\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4804\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4805\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid49}
{\list\listtemplateid50\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid4901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid4903\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid50}
{\list\listtemplateid51\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid5001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5003\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid51}
{\list\listtemplateid52\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid5101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid52}
{\list\listtemplateid53\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid5201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid53}
{\list\listtemplateid54\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid5301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid54}
{\list\listtemplateid55\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid5401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid55}
{\list\listtemplateid56\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid5501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5503\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid56}
{\list\listtemplateid57\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid5601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5603\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5604\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid57}
{\list\listtemplateid58\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid5701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5704\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5705\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid58}
{\list\listtemplateid59\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid5801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5803\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5804\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid59}
{\list\listtemplateid60\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid5901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid5902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid60}
{\list\listtemplateid61\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6003\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6004\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6005\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid61}
{\list\listtemplateid62\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid62}
{\list\listtemplateid63\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid63}
{\list\listtemplateid64\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid64}
{\list\listtemplateid65\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid65}
{\list\listtemplateid66\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6503\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6504\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6505\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid66}
{\list\listtemplateid67\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6603\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6604\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6605\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid67}
{\list\listtemplateid68\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6704\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6705\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6706\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6707\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid68}
{\list\listtemplateid69\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6803\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6804\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid69}
{\list\listtemplateid70\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid6901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6903\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid6904\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid70}
{\list\listtemplateid71\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7003\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7004\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid71}
{\list\listtemplateid72\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7104\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid72}
{\list\listtemplateid73\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7203\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7204\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid73}
{\list\listtemplateid74\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7304\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid74}
{\list\listtemplateid75\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7403\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7404\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid75}
{\list\listtemplateid76\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7503\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7504\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listname ;}\listid76}
{\list\listtemplateid77\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7602\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7603\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid77}
{\list\listtemplateid78\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7701\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7702\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7703\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listname ;}\listid78}
{\list\listtemplateid79\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7802\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7803\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7804\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7805\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid79}
{\list\listtemplateid80\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid7901\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7902\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7903\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7904\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7905\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7906\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid7907\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid80}
{\list\listtemplateid81\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid8001\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8002\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8003\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8004\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8005\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8006\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8007\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid81}
{\list\listtemplateid82\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid8101\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8102\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8103\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8104\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8105\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8106\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8107\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid82}
{\list\listtemplateid83\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid8201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8202\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8203\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8204\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8205\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8206\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8207\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid83}
{\list\listtemplateid84\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid8301\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8302\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8303\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8304\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8305\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8306\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8307\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid84}
{\list\listtemplateid85\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid8401\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8402\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8403\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8404\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8405\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8406\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8407\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid85}
{\list\listtemplateid86\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid8501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8502\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8503\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8504\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8505\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8506\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li4320\lin4320 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid8507\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li5040\lin5040 }{\listname ;}\listid86}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}{\listoverride\listid10\listoverridecount0\ls10}{\listoverride\listid11\listoverridecount0\ls11}{\listoverride\listid12\listoverridecount0\ls12}{\listoverride\listid13\listoverridecount0\ls13}{\listoverride\listid14\listoverridecount0\ls14}{\listoverride\listid15\listoverridecount0\ls15}{\listoverride\listid16\listoverridecount0\ls16}{\listoverride\listid17\listoverridecount0\ls17}{\listoverride\listid18\listoverridecount0\ls18}{\listoverride\listid19\listoverridecount0\ls19}{\listoverride\listid20\listoverridecount0\ls20}{\listoverride\listid21\listoverridecount0\ls21}{\listoverride\listid22\listoverridecount0\ls22}{\listoverride\listid23\listoverridecount0\ls23}{\listoverride\listid24\listoverridecount0\ls24}{\listoverride\listid25\listoverridecount0\ls25}{\listoverride\listid26\listoverridecount0\ls26}{\listoverride\listid27\listoverridecount0\ls27}{\listoverride\listid28\listoverridecount0\ls28}{\listoverride\listid29\listoverridecount0\ls29}{\listoverride\listid30\listoverridecount0\ls30}{\listoverride\listid31\listoverridecount0\ls31}{\listoverride\listid32\listoverridecount0\ls32}{\listoverride\listid33\listoverridecount0\ls33}{\listoverride\listid34\listoverridecount0\ls34}{\listoverride\listid35\listoverridecount0\ls35}{\listoverride\listid36\listoverridecount0\ls36}{\listoverride\listid37\listoverridecount0\ls37}{\listoverride\listid38\listoverridecount0\ls38}{\listoverride\listid39\listoverridecount0\ls39}{\listoverride\listid40\listoverridecount0\ls40}{\listoverride\listid41\listoverridecount0\ls41}{\listoverride\listid42\listoverridecount0\ls42}{\listoverride\listid43\listoverridecount0\ls43}{\listoverride\listid44\listoverridecount0\ls44}{\listoverride\listid45\listoverridecount0\ls45}{\listoverride\listid46\listoverridecount0\ls46}{\listoverride\listid47\listoverridecount0\ls47}{\listoverride\listid48\listoverridecount0\ls48}{\listoverride\listid49\listoverridecount0\ls49}{\listoverride\listid50\listoverridecount0\ls50}{\listoverride\listid51\listoverridecount0\ls51}{\listoverride\listid52\listoverridecount0\ls52}{\listoverride\listid53\listoverridecount0\ls53}{\listoverride\listid54\listoverridecount0\ls54}{\listoverride\listid55\listoverridecount0\ls55}{\listoverride\listid56\listoverridecount0\ls56}{\listoverride\listid57\listoverridecount0\ls57}{\listoverride\listid58\listoverridecount0\ls58}{\listoverride\listid59\listoverridecount0\ls59}{\listoverride\listid60\listoverridecount0\ls60}{\listoverride\listid61\listoverridecount0\ls61}{\listoverride\listid62\listoverridecount0\ls62}{\listoverride\listid63\listoverridecount0\ls63}{\listoverride\listid64\listoverridecount0\ls64}{\listoverride\listid65\listoverridecount0\ls65}{\listoverride\listid66\listoverridecount0\ls66}{\listoverride\listid67\listoverridecount0\ls67}{\listoverride\listid68\listoverridecount0\ls68}{\listoverride\listid69\listoverridecount0\ls69}{\listoverride\listid70\listoverridecount0\ls70}{\listoverride\listid71\listoverridecount0\ls71}{\listoverride\listid72\listoverridecount0\ls72}{\listoverride\listid73\listoverridecount0\ls73}{\listoverride\listid74\listoverridecount0\ls74}{\listoverride\listid75\listoverridecount0\ls75}{\listoverride\listid76\listoverridecount0\ls76}{\listoverride\listid77\listoverridecount0\ls77}{\listoverride\listid78\listoverridecount0\ls78}{\listoverride\listid79\listoverridecount0\ls79}{\listoverride\listid80\listoverridecount0\ls80}{\listoverride\listid81\listoverridecount0\ls81}{\listoverride\listid82\listoverridecount0\ls82}{\listoverride\listid83\listoverridecount0\ls83}{\listoverride\listid84\listoverridecount0\ls84}{\listoverride\listid85\listoverridecount0\ls85}{\listoverride\listid86\listoverridecount0\ls86}}
\margl1440\margr1440\vieww14400\viewh13220\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \'97\'97\'97\'97\'97\
READ ME\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls1\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}How to use this guide\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls1\ilvl1\cf0 {\listtext	\uc0\u8259 	}Currently the best way to search through this guide is to press CMD+F (if on Mac) or Ctrl+F (if on Windows/Linux) and to search for the following keywords\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls1\ilvl2\cf0 {\listtext	\uc0\u8259 	}snippets, examples, python, plt, sns, statistics, sparsity, ETL, read_csv, get_dummies, probability, keras, tflearn, impute, onehotencod, dummy v, PCA, Hive,  CNN, ensemble, validation, memory, usage, Hadoop, MAE, RSME, confusion, grid, function, matplotlib, plot, seaborn, facet, violin, histogram, bar, box, anomaly, count, label, title, libraries, sklearn. , keras. , tensor, from import, subplotting, regression, regressor, linear, logistic, SVM, clustering, classification, KNN,  k-m, boosting, xgb, decision tree, random forest, bagging, deep, learning, convolution, dense, bias, variance, skew, heat, drop, probability, pandas, preprocessing, skew, correction, subsetting, dataframe, for i, apply, column, row, data engineering,  concatenate, aggregate, terminology, important, feature, index, unique, value, naive bay, scaling, AB, A/B, predict, classifier, model, X_train, X, y_val, y_test, error, deviation, accurate, metric, melt, loc, iloc, .column, NaN, missing, lambda, np.expm1, np.log1p, split, CART, gradient, Series, Index, list, array, convert, lightgbm, xgboost, catboost, reinforcement, rule, selection, gaussian, numba, timeit\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\'97\'97\'97\'97\'97\
COMMON DS QUESTIONS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls2\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Probability\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls3\ilvl1\cf0 {\listtext	\uc0\u8259 	}Probability of rolling three dice without getting a six\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls3\ilvl2\cf0 {\listtext	\uc0\u8259 	}To obtain the probability that at least one 6 is rolled in the three tosses\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls3\ilvl3\cf0 {\listtext	\uc0\u8259 	}1- (5/6)^3   Note, the probability of rolling a 6 with one dice on one roll is 1/6.\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls3\ilvl1\cf0 {\listtext	\uc0\u8259 	}Probability of rolling double sixes twice in a roll (rolling two dices at a time, two times)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls3\ilvl2\cf0 {\listtext	\uc0\u8259 	}Since each dice is being rolled independently\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls3\ilvl3\cf0 {\listtext	\uc0\u8259 	}(1/6)^1 * (1/6)^1 * 1/6)^1 * (1/6)^1 = (1/6)^4 = 0.00077\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls3\ilvl1\cf0 {\listtext	\uc0\u8259 	}Probability of rolling one dice twice and getting a six both times\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls3\ilvl3\cf0 {\listtext	\uc0\u8259 	}(1/6)^1 * (1/6)^1 = 1/36\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls3\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://sciencing.com/calculate-dice-probabilities-5858157.html"}}{\fldrslt https://sciencing.com/calculate-dice-probabilities-5858157.html}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls3\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Statistics\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls3\ilvl1\cf0 {\listtext	\uc0\u8259 	}Multicollinearity (occurs when independent variables in a regression model are correlated.)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls3\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://towardsdatascience.com/multicollinearity-in-data-science-c5f6c0fe6edf"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \ulc2 https://towardsdatascience.com/multicollinearity-in-data-science-c5f6c0fe6edf}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \ulc2 \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls3\ilvl1
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}Bias and Variance Tradeoff (Great)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls3\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=EuBBz3bI-aA"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=EuBBz3bI-aA}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone 	\uc0\u8259 	The first thing we do before start looking bias and variance is to split out data into a training and test set\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\cf0 	\uc0\u8259 	Example\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls4\ilvl5\cf0 {\listtext	\uc0\u8259 	}imagine viewing a scatter plot with data in a log10 shape and showing 80% of the markers/data as blue and the remaining 20% as green. The blue dots are the training set and the green dots are the testing set\
{\listtext	\uc0\u8259 	}\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\cf0 	
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}The inability for a machine learning method (like linear regression) to capture the true relationship is called 
\f3\b \cf3 bias
\f0\b0 \cf0 \
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Think about trying to fit a linear line to a training set with log10 type scatter data\
{\listtext	\uc0\u8259 	}Because the linear line can\'92t be curved like the \'93true\'94 relationship, we say it has a relatively large amount of \cf3 bias\cf0 . In other words, the inability for a machine learning method (like linear regression) to capture the true relationship is called 
\f3\b \cf3 bias\cf0 \
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4
\f0\b0 \cf0 {\listtext	\uc0\u8259 	}Example\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Another machine learning method, may try to fit a squiggly line (zig-zag type shape) to the training set. The squiggly line can handle to the arc of a true relationship with log10 type data so we can it would have a relatively low amount of 
\f3\b \cf3 bias
\f0\b0 \cf0  compared to trying trying to fit a straight line to log10 type data.\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}To quantify this, we can compare the sum of squares between the squiggly line model and straight line model. But note, the squiggly line model fits the scatter plot so well, that the sum of squares will be zero! The squiggly line wins here at modeling the training set data.\
{\listtext	\uc0\u8259 	}Lets now look at the testing set and calculate the sum of squares\
{\listtext	\uc0\u8259 	}The straight line wins here! It\'92s sum of squares is lower than that of the squiggly line fine\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}\cf3 Variance\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}Since the squiggly line model fitted the training set so well, but failed to fit the testing set well, we say it has a relatively large amount of \cf3 variance\cf4 . In other words, the difference in fits between the data sets is called \cf3 variance\cf4 .\
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}In summary, the squiggly line had low \cf3 bias\cf0  with the training and test data since it is flexible and can adapt to the log10 shaped data. But\'85the squiggly line has \cf5 high variability/\cf3 variance\cf0  because it results in a vastly different sums of squares for different data sets. In other words, it\'92s hard to predict how well the squiggly line will perform with future data sets. It might do well sometimes, and other times it might do terribly.\
{\listtext	\uc0\u8259 	}In contrast, the straight line has relatively high balance when trying to capture the true relationship of the log10 shaped data. But\'85. The straight line has relatively low variability/variance because the sums of squares are very similar between different datasets.  In other words, the straight line might only give good predictions, and not 
\f4\i great 
\f0\i0 predictions. But they will be consistently good predictions\
{\listtext	\uc0\u8259 	}\cf6 Terminology alert:\cf0 \
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}If the model fits the training set better than the test set we call this \cf3 overfitting\cf5 .\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}Overall\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls5\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls5\ilvl5\cf0 {\listtext	\uc0\u8259 	}In machine learning, the ideal algorithm has low bias and can accurately model the true relationship (imagine a model fitting a training set that has log10 type shape data). Also, the ideal model will have low variability, by producing consistently predictions across different data sets (imagine a representation of the same model but now with testing data. The model will ideally fit it the same. The sums of squares are about equal between data sets if it has low variability.) An ideal model for your data to create is done by finding the sweet spot between a simple model (think about the straight line fitting the training set, ~high bias) and complex model (think about the squiggly line model fitting the training set perfectly, zero bias)\
{\listtext	\uc0\u8259 	}\cf6 Terminology alert\cf0 :\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls5\ilvl6\cf0 {\listtext	\uc0\u8259 	}Three commonly used methods for finding the sweet spot between simple and complicated models are: 
\f3\b regularization
\f0\b0 , 
\f3\b boosting
\f0\b0 , and 
\f3\b bagging
\f0\b0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls5\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Finance\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls5\ilvl1\cf0 {\listtext	\uc0\u8259 	}Black\'96Scholes model\
{\listtext	\uc0\u8259 	}Definitions:\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls5\ilvl2\cf0 {\listtext	\uc0\u8259 	}Risk\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls5\ilvl3\cf0 {\listtext	\uc0\u8259 	}How do you measure Risk\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls5\ilvl2\cf0 {\listtext	\uc0\u8259 	}Stocks\
{\listtext	\uc0\u8259 	}Portfolio\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls5\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Difference between linear and logistic regression\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls5\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://techdifferences.com/difference-between-linear-and-logistic-regression.html"}}{\fldrslt https://techdifferences.com/difference-between-linear-and-logistic-regression.html}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls5\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls6\ilvl0\cf0 	\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\
SLANG TERMS/TERMINOLOGY\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls7\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Long Data\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls7\ilvl1\cf0 {\listtext	\uc0\u8259 	}Bunch of columns, little rows\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls7\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Wide Data\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls7\ilvl1\cf0 {\listtext	\uc0\u8259 	}Bunch of rows, little columns\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\
MATHEMATICS\
\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls8\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Common Functions\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls8\ilvl1\cf0 {\listtext	\uc0\u8259 	}Sigmoid\
{\listtext	\uc0\u8259 	}Threshold\
{\listtext	\uc0\u8259 	}Rectifier\
{\listtext	\uc0\u8259 	}Hyperbolic tangent\
{\listtext	\uc0\u8259 	}Softmax Function\
{\listtext	\uc0\u8259 	}Cross-Entropy\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls8\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Common Distributions\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls8\ilvl1\cf0 {\listtext	\uc0\u8259 	}Gaussian\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\'97\
RANDOMs\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls9\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Python\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls9\ilvl1\cf0 {\listtext	\uc0\u8259 	}Error Handing!\
{\listtext	\uc0\u8259 	}OpenCV\
{\listtext	\uc0\u8259 	}Looping through files in a directory\
{\listtext	\uc0\u8259 	}sparsity (can come up a lot in NLP, we want to avoid it)\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://realpython.com/python-modules-packages/"}}{\fldrslt https://realpython.com/python-modules-packages/}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls9\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Watch Later\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls9\ilvl1\cf0 {\listtext	\uc0\u8259 	}YouTube\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls9\ilvl2\cf0 {\listtext	\uc0\u8259 	}Image Classifier using VGG16 Model (Great playlist to go through by deeplizard)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls9\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=oDHpqu52soI&list=PLZbbT5o_s2xrwRnXk_yCPtnqqo4_u2YGL&index=13"}}{\fldrslt https://www.youtube.com/watch?v=oDHpqu52soI&list=PLZbbT5o_s2xrwRnXk_yCPtnqqo4_u2YGL&index=13}} \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls9\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Big Data Tools\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls9\ilvl1\cf0 {\listtext	\uc0\u8259 	}Hadoop\
{\listtext	\uc0\u8259 	}Spark\
{\listtext	\uc0\u8259 	}H2O\
{\listtext	\uc0\u8259 	}CDSW\
{\listtext	\uc0\u8259 	}Domino Labs\
{\listtext	\uc0\u8259 	}HIVE\
{\listtext	\uc0\u8259 	}PIG\
{\listtext	\uc0\u8259 	}and unstructured data\
{\listtext	\uc0\u8259 	}etc.\
{\listtext	\uc0\u8259 	}CPLEX, IBM-DOC & AnyLogic  (Bayer mentioned this in a data science job posted)\
{\listtext	\uc0\u8259 	}Developing and/or applying linear, mixed integer, stochastic programming to solve demand planning, supply chain, production scheduling (The Goodyear Tire & Rubber Company)\
{\listtext	\uc0\u8259 	}Discrete Event Simulation, Factor Analysis, Genetic Algorithms, Bayesian Probability Models, Hidden Markov Models and Sensitivity Analysis. (Cotiviti)\
{\listtext	\uc0\u8259 	}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls9\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Matrices\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls9\ilvl1\cf0 {\listtext	\uc0\u8259 	}scipy.sparse matrices vs numpy matrices?\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls9\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Feature Selection\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls9\ilvl1\cf0 {\listtext	\uc0\u8259 	}Heatmaps\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls9\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Leaf-wise vs Level-wise Decision Tree Algorithms\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls9\ilvl1\cf0 {\listtext	\uc0\u8259 	}Leaf-Wise\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls9\ilvl2\cf0 {\listtext	\uc0\u8259 	}lightgbm\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls9\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}XGBoost, Lightgbm, and CatBoost \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls9\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=V5158Oug4W8"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=V5158Oug4W8}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \
\ls9\ilvl1
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db"}}{\fldrslt https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db}}\
{\listtext	\uc0\u8259 	}these are are gradient boosting algorithms for decision trees\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls9\ilvl2\cf0 {\listtext	\uc0\u8259 	}xgboost\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls9\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://xgboost.readthedocs.io/en/latest/tutorials/index.html"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://xgboost.readthedocs.io/en/latest/tutorials/index.html}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \
\ls9\ilvl3
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}loss function term\
{\listtext	\uc0\u8259 	}regularization term\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls9\ilvl4\cf0 {\listtext	\uc0\u8259 	}xgboost has show to work very welling practice \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls9\ilvl2\cf0 {\listtext	\uc0\u8259 	}LightGBM\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls9\ilvl3\cf0 {\listtext	\uc0\u8259 	}written by Microsoft\
{\listtext	\uc0\u8259 	}much faster than xgboost\
{\listtext	\uc0\u8259 	}works about the same\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls9\ilvl2\cf0 {\listtext	\uc0\u8259 	}CatBoost \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls9\ilvl3\cf0 {\listtext	\uc0\u8259 	}great if you have a lot of categorical variables\
{\listtext	\uc0\u8259 	}uses something called mean target encoding ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=V5158Oug4W8"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=V5158Oug4W8}}, see around ~13:00)\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls9\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}GridSearchCV vs random search\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls9\ilvl1\cf0 {\listtext	\uc0\u8259 	}learning rate \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls9\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}\
{\listtext	
\f1 \uc0\u9642 
\f0 	}DecisionTreeRegressor \
{\listtext	
\f1 \uc0\u9642 
\f0 	}BaggingRegressor \
{\listtext	
\f1 \uc0\u9642 
\f0 	}RandomForestRegressor \
{\listtext	
\f1 \uc0\u9642 
\f0 	}ExtraTreesRegressor\
{\listtext	
\f1 \uc0\u9642 
\f0 	}AdaBoostRegressor\
{\listtext	
\f1 \uc0\u9642 
\f0 	}GradientBoostingRegressor\
{\listtext	
\f1 \uc0\u9642 
\f0 	}XGBRegressor\
{\listtext	
\f1 \uc0\u9642 
\f0 	}KerasRegressor\
{\listtext	
\f1 \uc0\u9642 
\f0 	}How to determine how many n_estimators we need for most of the regressors\
{\listtext	
\f1 \uc0\u9642 
\f0 	}Memory Usage Feature in Jupyter Notebook\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls10\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Survival Analysis\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls10\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://stats.idre.ucla.edu/stata/seminars/stata-survival/"}}{\fldrslt https://stats.idre.ucla.edu/stata/seminars/stata-survival/}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls11\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Data Engineering\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls11\ilvl1\cf0 {\listtext	\uc0\u8259 	}Goals of a Data Engineer\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls11\ilvl2\cf0 {\listtext	\uc0\u8259 	}Developing data pipelines (most undergrad courses don\'92t teach you how to do this)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls11\ilvl3\cf0 {\listtext	\uc0\u8259 	}Taking data from an operational system and moving it to something so it can be used by analysts and data scientists \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls11\ilvl2\cf0 {\listtext	\uc0\u8259 	}Manage tables and data sets for analysts and data scientists\
{\listtext	\uc0\u8259 	}Design with the product in mind\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls11\ilvl3\cf0 {\listtext	\uc0\u8259 	}What data is being tracked, what questions are the users going to be asking while they are using the product (for example, dashboards) \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls11\ilvl1\cf0 {\listtext	\uc0\u8259 	}Skills of a Data Engineer\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls11\ilvl2\cf0 {\listtext	\uc0\u8259 	}Data Modeling\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls11\ilvl3\cf0 {\listtext	\uc0\u8259 	}Relational Databases\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls11\ilvl4\cf0 {\listtext	\uc0\u8259 	}Note, databases prefer data in long format\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls11\ilvl3\cf0 {\listtext	\uc0\u8259 	}Data Warehouses\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls11\ilvl2\cf0 {\listtext	\uc0\u8259 	}Automation\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls11\ilvl3\cf0 {\listtext	\uc0\u8259 	}\uc0\u932 iming (isolated task/function at a specific time routinely)\
{\listtext	\uc0\u8259 	}Dependencies (for example, we want to make sure task/table A comes before task/table B)\
{\listtext	\uc0\u8259 	}Failures (you want to make sure you capture your failures like a notification system (email, conbon?)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls11\ilvl2\cf0 {\listtext	\uc0\u8259 	}ETL Development (there is another type called ELT, essentially Data Pipeline, means Extract Transformation Label) \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls11\ilvl3\cf0 {\listtext	\uc0\u8259 	}For Example, taking data from an operational system such as Workday (an HR system) or MySQL (if its a product), and moving this data into a data warehouse. The old school way is to use SQL Server, Oracle databases, etc as the data warehouse. The more modern way is Hadoop, Redshift, etc.\
{\listtext	\uc0\u8259 	}Also in ETL development, data engineers can also be in charged of implementing an analytical data layer on the data to aggregate it so it can be easily fed to dashboards.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls11\ilvl2\cf0 {\listtext	\uc0\u8259 	}Product Understanding (data is our product, we want to understand what the data scientists want)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls11\ilvl3\cf0 {\listtext	\uc0\u8259 	}Dashboards\
{\listtext	\uc0\u8259 	}Datasets\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls11\ilvl2\cf0 {\listtext	\uc0\u8259 	}Data engineers develop data pipelines to\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls11\ilvl3\cf0 {\listtext	\uc0\u8259 	}improve ease of data access\
{\listtext	\uc0\u8259 	}produce analytical layers for dashboards\
{\listtext	\uc0\u8259 	}clean up data (eliminate duplicate data, etc)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls11\ilvl4\cf0 {\listtext	\uc0\u8259 	}we want data scientists to know that every observation they get is that its accurate/valid)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls11\ilvl2\cf0 {\listtext	\uc0\u8259 	}Tools Used\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls11\ilvl3\cf0 {\listtext	\uc0\u8259 	}Pipeline Framework (SSIS (GUI, hard to customize with Python), Infromatica, Airflow (AirBnB\'92s version of a data pipeline management system, easy to customize)), Spark\
{\listtext	\uc0\u8259 	}SQL\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls11\ilvl4\cf0 {\listtext	\uc0\u8259 	}It is easier to hire someone who knows SQL instead of map/reduce in Hadoop, Hive, etc\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls11\ilvl3\cf0 {\listtext	\uc0\u8259 	}Python, Powershell, Linux and/or Java (Java is for map/reduce stuff)\
{\listtext	\uc0\u8259 	}Tableau (heavily used, easiest), D3.js (fun and customizable, but painful if you are not Java script fan), SSRS, etc.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls11\ilvl2\cf0 {\listtext	\uc0\u8259 	}How does a Data Engineer make an impact?\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls11\ilvl3\cf0 {\listtext	\uc0\u8259 	}Creating optimized pipelines\
{\listtext	\uc0\u8259 	}influencing\
{\listtext	\uc0\u8259 	}designing\
{\listtext	\uc0\u8259 	}maintaining\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls11\ilvl2\cf0 {\listtext	\uc0\u8259 	}Note, Business Intelligence and Data Engineering are now more of the same. \
{\listtext	\uc0\u8259 	}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls12\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Undersampling vs Oversampling\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\'97\'97\'97\'97\'97\
GENERAL\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls13\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Overfitting occurs when there is greater accuracy seen with the training set than the test set\
{\listtext	
\f1 \uc0\u9642 
\f0 	}Anything model that is built with X_train and y_train is considered supervised learning. Any model but with just X_train is considered unsupervised learning.\
{\listtext	
\f1 \uc0\u9642 
\f0 	}Features are column-wise data \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls14\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Supervised Learning vs Unsupervised Learning vs Reinforcement Learning (note, I need to un indent this over)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls14\ilvl1\cf0 {\listtext	\uc0\u8259 	}The main difference between the two types is that supervised learning is done using a ground truth, or in other words, we have prior knowledge of what the output values for our samples should be.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls14\ilvl2\cf0 {\listtext	\uc0\u8259 	}Supervised Learning\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls14\ilvl3\cf0 {\listtext	\uc0\u8259 	}the goal of supervised learning is to learn a function that, given a sample of data and desired outputs, best approximates the relationship between input and output observable in the data.\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls14\ilvl4\cf0 {\listtext	\uc0\u8259 	}Classification or Regression\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls14\ilvl5\cf0 {\listtext	\uc0\u8259 	}Classification:\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls15\ilvl5\cf0 {\listtext	\uc0\u8259 	}Regression:\
{\listtext	\uc0\u8259 	}Common Algorithms:\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls15\ilvl6\cf0 {\listtext	\uc0\u8259 	}logistic regression\
{\listtext	\uc0\u8259 	}naive bayes\
{\listtext	\uc0\u8259 	}support vector machines\
{\listtext	\uc0\u8259 	}artificial neural networks\
{\listtext	\uc0\u8259 	}random forests\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls15\ilvl4\cf0 {\listtext	\uc0\u8259 	}In both regression and classification, the goal is to find specific relationships or structure in the input data that allow us to effectively produce correct output data. Note that \'93correct\'94 output is determined entirely from the training data, so while we do have a ground truth that our model will assume is true, it is not to say that data labels are always correct in real-world situations. Noisy, or incorrect, data labels will clearly reduce the effectiveness of your model. When conducting supervised learning, the main considerations are model complexity, and the bias-variance tradeoff. Note that both of these are interrelated.\
{\listtext	\uc0\u8259 	}When conducting supervised learning, the main considerations are model complexity, and the bias-variance tradeoff. Note that both of these are interrelated.\
{\listtext	\uc0\u8259 	}The bias-variance tradeoff also relates to model generalization. In any model, there is a balance between bias, which is the constant error term, and variance, which is the amount by which the error may vary between different training sets. So, high bias and low variance would be a model that is consistently wrong 20% of the time, whereas a low bias and high variance model would be a model that can be wrong anywhere from 5%-50% of the time, depending on the data used to train it. Note that bias and variance typically move in opposite directions of each other; increasing bias will usually lead to lower variance, and vice versa. When making your model, your specific problem and the nature of your data should allow you to make an informed decision on where to fall on the bias-variance spectrum. Generally, increasing bias (and decreasing variance) results in models with relatively guaranteed baseline levels of performance, which may be critical in certain tasks. Additionally, in order to produce models that generalize well, the variance of your model should scale with the size and complexity of your training data \'97 small, simple data-sets should usually be learned with low-variance models, and large, complex data-sets will often require higher-variance models to fully learn the structure of the data.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls15\ilvl4\cf0 {\listtext	\uc0\u8259 	}Unsupervised Learning \
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls15\ilvl4\cf0 {\listtext	\uc0\u8259 	}Does not have labeled outputs, so its goal is to infer the natural structure present within a set of data points\
{\listtext	\uc0\u8259 	}The most common tasks within unsupervised learning are clustering, representation learning, and density estimation. In all of these cases, we wish to learn the inherent structure of our data without using explicitly-provided labels. \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls15\ilvl5\cf0 {\listtext	\uc0\u8259 	}Common Algorithms:\
\pard\tx4540\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li5040\fi-5040\pardirnatural\partightenfactor0
\ls15\ilvl6\cf0 {\listtext	\uc0\u8259 	}K-Means Clustering\
{\listtext	\uc0\u8259 	}Principal Component Analysis (PCA)\
{\listtext	\uc0\u8259 	}Autoencoders\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls15\ilvl5\cf0 {\listtext	\uc0\u8259 	}Some common use-cases for unsupervised learning are exploratory analysis (I thinkk, this is apriori) and dimensionality reduction (PCA, LDA)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\cf0 	\uc0\u8259 	Reinforcement Learning \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls16\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Decision Tree Terminology\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls16\ilvl1\cf0 {\listtext	\uc0\u8259 	}The very top of the tree is called the \'93Root Node\'94 or just \'93The Root\'94. It represents the entire population or sample, and this further gets divided into two or more homogeneous sets.  \
{\listtext	\uc0\u8259 	}\'93Internal/Child Nodes\'94 have arrows pointing to them and away from them\
{\listtext	\uc0\u8259 	}\'93Leaf Nodes\'94 only have arrows pointing to them \
{\listtext	\uc0\u8259 	}\'93Splitting\'94 is dividing the root node/sub node into different parts on the basis of some condition\
{\listtext	\uc0\u8259 	}\'93Pruning\'94 is the opposite of splitting, basically removing unwanted branches from the trees. In other words, pruning is a method of limiting tree depth to reduce overfitting in decision trees. There are two types of pruning:\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls16\ilvl2\cf0 {\listtext	\uc0\u8259 	}Pre-pruning: A decision tree involves setting the parameters of a decision tree before building  it. For example, setting the maximum tree depth, maximum number of terminal nodes, minimum samples for a node split (controls the size of the resultant terminal nodes), maximum number of features \
{\listtext	\uc0\u8259 	}Post-pruning: To post-prune, validate the performance of the model on a test set. Afterwards, cut back splits that seem to result from overfitting noise in the training set. Pruning these splits dampens the noise in the data set.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls16\ilvl3\cf0 {\listtext	\uc0\u8259 	}Post-pruning may result in overfitting the model and is currently not available in Pythons sklearn, but its available in R.\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls16\ilvl1\cf0 {\listtext	\uc0\u8259 	}\'93Branch/SubTree\'94 is formed by splitting the tree/node\
{\listtext	\uc0\u8259 	}\'93Entropy\'94 & \'93Information Gain\'94\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls16\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://homes.cs.washington.edu/~shapiro/EE596/notes/InfoGain.pdf"}}{\fldrslt https://homes.cs.washington.edu/~shapiro/EE596/notes/InfoGain.pdf}}\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=qDcl-FRnwSU&t=235s"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=qDcl-FRnwSU&t=235s}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul  
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone (great example, 27:00, 34:00)\
{\listtext	\uc0\u8259 	}Entropy\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls16\ilvl3\cf0 {\listtext	\uc0\u8259 	}Common way to measure impurity. This value is used in the information gain formula.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls16\ilvl2\cf0 {\listtext	\uc0\u8259 	}Information Gain \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls16\ilvl3\cf0 {\listtext	\uc0\u8259 	}Measures the reduction in entropy. Decides which attribute should be selected as the decision node. We select the attribute with the maximum gain to be the root node.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls16\ilvl2\cf0 {\listtext	\uc0\u8259 	}Also, in other words, which ever feature has the lowest Gini Impurity will be the one that provides the greatest information gain. Therefore, this feature will be selected as the node and so forth for the remaining nodes.\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls16\ilvl1\cf0 {\listtext	\uc0\u8259 	}Important decision trees can be constructed using binary data, continuous data, and categorical/ranked data:\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls16\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example with binary data (3 independent vars, 1 dependent var, all binary data (0,1))\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls16\ilvl3\cf0 {\listtext	\uc0\u8259 	}See @03:25  {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=7VeUPuFGJHk"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=7VeUPuFGJHk}}\
{\listtext	\uc0\u8259 	}To begin a decision tree, you calculate the Gini impurity using each independent var with the dependent var one at a time and keep track of all the true pos, false pos, false neg, true neg. Which ever feature shows the lowest 
\f4\i total
\f0\i0  Gini impurity is the one selected to start the root node. Next,  the 
\f4\i subsection 
\f0\i0 Gini impurity\'92s that were created for the \'93true pos, false pos\'94 block and \'93false neg, true neg\'94 block are the splits for that node. Now let\'92s start with going down the left side of the root node (the true pos, false pos) side.  We must compare the Gini impurity found in this section to the Gini impurities of the other features (note, these features have to fall under the left side of the of the root node too). If a smaller Gini impurity is found in one of the other features, then it becomes the node for this level of the tree. Therefore, in a sense, it overrides the \'93true pos, false pos\'94 section of the root node. \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls17\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example with continuous data (1 independent var (continuous data), 1 dependent var (binary data))\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls17\ilvl3\cf0 {\listtext	\uc0\u8259 	}See @13:57  {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=7VeUPuFGJHk"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=7VeUPuFGJHk}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \
\ls17\ilvl3
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}To begin a decision tree with the independent var as continuous, you first have to sort the column data (eg, 125, 135, 155, and so forth (make sure the dependent vars stay with the appropriate ones)). Then you take an average between each observation (130, 145). From there, you then start calculating the 
\f4\i subsection
\f0\i0  Gini Impurities, and then get the 
\f4\i total
\f0\i0  Gini impurity. See the link directly above.
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls18\ilvl2
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}Example with ranked/categorical data (1 independent var (categorical data), 1 dependent var (binary data))\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls18\ilvl3\cf0 {\listtext	\uc0\u8259 	}See @15:25  {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=7VeUPuFGJHk"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=7VeUPuFGJHk}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \
\ls18\ilvl3
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}Need to revisit\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls19\ilvl1\cf0 {\listtext	\uc0\u8259 	}Advantages: Decision trees are easy to interpret. To build a decision tree requires little data preparation from the user - there is no need to normalize the data\
{\listtext	\uc0\u8259 	}Disadvantages: Decision trees are likely to overfit noisy data. The probability of overfitting on noise increases as a tree gets deeper.\
{\listtext	\uc0\u8259 	}Ensembles\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls19\ilvl2\cf0 {\listtext	\uc0\u8259 	}Creating ensembles involves aggregating the results of different models. Ensemble decision trees are used in bagging and random forests while ensemble regression trees are used in boosting\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls19\ilvl1\cf0 {\listtext	\uc0\u8259 	}Bagging/Bootstrap aggregating\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls19\ilvl2\cf0 {\listtext	\uc0\u8259 	}Bagging involves creating multiple decision trees each trained on a different bootstrap sample of the data. Because bootstrapping involves samples with replacement, some of the data in the sample is left out of each tree.\
{\listtext	\uc0\u8259 	}Consequently, the decision trees created are made using different samples which solves the problem of overfitting to the training sample. Ensembling decision trees in this way helps reduce the total error because variance of the model continues to decrease with each new tree added without an increase in the bias of the ensemble.\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls19\ilvl1\cf0 {\listtext	\uc0\u8259 	}Random Forest\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls19\ilvl2\cf0 {\listtext	\uc0\u8259 	}A bag of decision trees that uses subspace sampling is referred to as a random forest. Only a selection of the features is considered at each node split which decor relates the trees in the forest.\
{\listtext	\uc0\u8259 	}Another advantage of random forests is that they have an in-built validation mechanism. Because only a percentage of the data is used for each model, an out-of-bag error of the moles performance can be calculated using the 37% of the sample left out of each model\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls19\ilvl1\cf0 {\listtext	\uc0\u8259 	}Boosting	\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls19\ilvl2\cf0 {\listtext	\uc0\u8259 	}Boosting involves aggregating a collection of weak learners(regression trees) to form a strong predictor. Ad boosted model is built over time adding a new tree into the model that minimizes the error by previous learners. This is done by fitting the end tree on the residuals of the previous trees.\
{\listtext	\uc0\u8259 	}If it isn\'92t clear this far, for many real-world applications a single decision tree is not a preferable classification as it is likely to overfit and generalize very poorly to new examples. However, an ensemble of decision or regression trees minimizes the overfitting disadvantage and these models become stellar, state of the art classification and regression algorithms.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls20\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Model Selection\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls20\ilvl1\cf0 {\listtext	\uc0\u8259 	}Regression\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls20\ilvl2\cf0 {\listtext	\uc0\u8259 	}Cross-Validation\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls20\ilvl3\cf0 {\listtext	\uc0\u8259 	}MAE\
{\listtext	\uc0\u8259 	}RSME\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls20\ilvl1\cf0 {\listtext	\uc0\u8259 	}Classification\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls20\ilvl2\cf0 {\listtext	\uc0\u8259 	}Confusion Matrix\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls20\ilvl1\cf0 {\listtext	\uc0\u8259 	}Selecting the best model in scikit-learn using cross-validation\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls20\ilvl2\cf0 {\listtext	\uc0\u8259 	}What is the drawback of using the train/test split procedure for model evaluation?\
{\listtext	\uc0\u8259 	}How does K-fold cross-validation overcome this limitation?\
{\listtext	\uc0\u8259 	}How can cross validation be used for selecting tuning parameters, choosing between models, and selecting features?\
{\listtext	\uc0\u8259 	}What are some possible improvements to cross-validation?\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls20\ilvl1\cf0 {\listtext	\uc0\u8259 	}K Fold Validation\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls20\ilvl2\cf0 {\listtext	\uc0\u8259 	}This is used to over come importing just one random state and testing it. For example, if cv=10 is set in the cross_val_score model, 10 increments will be tested as the test set with the remaining data being used as the training set each time.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\'97\'97\'97\'97\'97  \
DATA PREPROCESSING\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls21\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Terminology\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls21\ilvl1\cf0 {\listtext	\uc0\u8259 	}feature scaling/normalization/\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls21\ilvl2\cf0 {\listtext	\uc0\u8259 	}Checking for NaN values in Pandas\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls21\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe}}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls21\ilvl1\cf0 {\listtext	\uc0\u8259 	}Data Transformation\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls21\ilvl2\cf0 {\listtext	\uc0\u8259 	}Why should we transform data when it is already clean?\
{\listtext	\uc0\u8259 	}Different features in the data set may have values in different ranges. For example, in an employee data set, the range of salary feature may lie from thousands to lakhs but the range of values of age feature will be in 20- 60. That means a column is more weighted compared to other.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls21\ilvl3\cf0 {\listtext	\uc0\u8259 	}Two most common methods for normalization:\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls21\ilvl4\cf0 {\listtext	\uc0\u8259 	}Min-Max\
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls21\ilvl5\cf0 {\listtext	\uc0\u8259 	}Min- Max tries to get the values closer to mean. But when there are outliers in the data which are important and we don\'92t want to loose their impact ,we go with Z score normalization.\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls21\ilvl4\cf0 {\listtext	\uc0\u8259 	}Z score\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls21\ilvl3\cf0 {\listtext	\uc0\u8259 	}Skewness of data:\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls21\ilvl4\cf0 {\listtext	\uc0\u8259 	}According to Wikipedia,\'94 In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean.\'94\
{\listtext	\uc0\u8259 	}Generally, if the skewness value lies above +1 or \cf7 BELOW\cf0  -1, data is highly skewed. If it lies between +0.5 to -0.5, it is moderately skewed. If the value is 0, then the data is symmetric\
{\listtext	\uc0\u8259 	}It is also important to make sure the absolute value of the skewness is greater than twice the std error value (04:45, {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=_c3dVTRIK9c"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=_c3dVTRIK9c}})\
{\listtext	\uc0\u8259 	}NOTE, we want to be very careful when sampling data sets if the results are highly skewed. For example, in the fraud analysis I am going through {\field{\*\fldinst{HYPERLINK "https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul  
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone there are 99.83% of cases that are not fraud and 0.17 % of cases that are. Therefore, if we did sampling with this dataset, we may not get any of the fraud cases in our training set.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls21\ilvl3\cf0 {\listtext	\uc0\u8259 	}Heat maps\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls21\ilvl4\cf0 {\listtext	\uc0\u8259 	}are great for seeing correlated variables (<myDataFrameName1>.corr() \'97> with the Pandas dataframe, this gives a print out of all the correlation coefficients).\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls21\ilvl3\cf0 {\listtext	\uc0\u8259 	}Copy a dataframe\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls21\ilvl4\cf0 {\listtext	\uc0\u8259 	}temp3 = <myDataFrameName1>.copy() # our dataframe\
{\listtext	\uc0\u8259 	}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls21\ilvl4\cf0 {\listtext	\uc0\u8259 	}Data Frame Types\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls21\ilvl1\cf0 {\listtext	\uc0\u8259 	}Calling <myDataFrameName1>.info() will show us information related to the Dataframe \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls21\ilvl2\cf0 {\listtext	\uc0\u8259 	}\'93Object\'94 are typically string values\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 	\'95	Imputer\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 	\uc0\u8259 	import Imputes module to handle missing data\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls22\ilvl0\cf0 {\listtext	\uc0\u8226 	}Encoding variables\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls22\ilvl1\cf0 {\listtext	\uc0\u8259 	}Terminology\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls22\ilvl2\cf0 {\listtext	\uc0\u8259 	}Same Thing\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls22\ilvl3\cf0 {\listtext	\uc0\u8259 	}Dummy Encoding (if you are coming from the statistics field) \
{\listtext	\uc0\u8259 	}One Hot Encoding (if you are coming from the computer science or electrical engineering field) \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 	\uc0\u8259 	eg, male and female into 1 and 0\
	\uc0\u8259 	eg, France, Spain, and Germany into 0, 2,and 1 (note, we have to be careful of the dummy variable trap here. We must use OneHotEncoder to create ONLY TWO new columns that show only 1 and 0\'92s). We do not need three columns for each country. We need one minus for some reason. I think this has something to do with the degrees of freedom. We are avoiding what is called the Dummy Variable Trap.\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls23\ilvl0\cf0 {\listtext	\uc0\u8226 	}Feature scaling\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls23\ilvl1\cf0 {\listtext	\uc0\u8259 	}5.3.1.3. Scaling data with outliers\
{\listtext	\uc0\u8259 	}If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use robust_scale and RobustScaler as drop-in replacements instead. They use more robust estimates for the center and range of your data. (see {\field{\*\fldinst{HYPERLINK "https://scikit-learn.org/stable/modules/preprocessing.html#scaling-data-with-outliers"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://scikit-learn.org/stable/modules/preprocessing.html#scaling-data-with-outliers}})\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\tx10256\li1440\fi-1440\pardirnatural\partightenfactor0
\ls24\ilvl1\cf0 {\listtext	\uc0\u8259 	}Feature extraction is very different from Feature selection: the former (feature extraction) consists in transforming arbitrary data, such as text or images, into numerical features usable for machine learning. The latter is a machine learning technique applied on these features. \
{\listtext	\uc0\u8259 	}Examples of Algorithms where Feature Scaling matters \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\tx10256\li2160\fi-2160\pardirnatural\partightenfactor0
\ls24\ilvl2\cf0 {\listtext	\uc0\u8259 	}K-Means uses the Euclidean distance measure here feature scaling matters.\
{\listtext	\uc0\u8259 	}K-Nearest-Neighbours also require feature scaling.\
{\listtext	\uc0\u8259 	}Principal Component Analysis (PCA): Tries to get the feature with maximum variance, here too feature scaling is required.\
{\listtext	\uc0\u8259 	}Gradient Descent: Calculation speed increase as Theta calculation becomes faster after feature scaling.\
{\listtext	\uc0\u8259 	}Note: Naive Bayes, Linear Discriminant Analysis, and Tree-Based models are not affected by feature scaling. In Short, any Algorithm which is Not Distance based is Not affected by Feature Scaling.\
\pard\tx720\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\tx10256\pardirnatural\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls25\ilvl0\cf0 {\listtext	\uc0\u8226 	}Good Reads\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls25\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://stats.stackexchange.com/questions/189652/is-it-a-good-practice-to-always-scale-normalize-data-for-machine-learning"}}{\fldrslt https://stats.stackexchange.com/questions/189652/is-it-a-good-practice-to-always-scale-normalize-data-for-machine-learning}}\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://stats.stackexchange.com/questions/342140/standardization-of-continuous-variables-in-binary-logistic-regression"}}{\fldrslt https://stats.stackexchange.com/questions/342140/standardization-of-continuous-variables-in-binary-logistic-regression}} (eg, 2 regressors are continuous and 2 are binary)\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://datascience.stackexchange.com/questions/20237/why-do-we-convert-skewed-data-into-a-normal-distribution"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://datascience.stackexchange.com/questions/20237/why-do-we-convert-skewed-data-into-a-normal-distribution}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls25\ilvl0\cf0 {\listtext	\uc0\u8226 	}Parametric and Nonparametric: Demystifying the Terms\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls25\ilvl1\cf0 {\listtext	\uc0\u8259 	}Parametric\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls25\ilvl2\cf0 {\listtext	\uc0\u8259 	}We have to normally distributed datasets (not positively or negatively skewed)\
{\listtext	\uc0\u8259 	}Common transformations of this data include square root, cube root, and log10.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls25\ilvl3\cf0 {\listtext	\uc0\u8259 	}The cube root transformation involves converting x to x^(1/3). This is a fairly strong transformation with a substantial effect on distribution shape: but is weaker than the logarithm. It can be applied to negative and zero values too. Negatively skewed data.\
{\listtext	\uc0\u8259 	}The square root transformation, x^2 can only be applied to positive values only. Hence, observe the values of column before applying.\
{\listtext	\uc0\u8259 	}The logarithm transformation , x to log base 10 of x, or x to log base e of x (ln x), or x to log base 2 of x, is a strong transformation and can be used to reduce right skewness (positively skewed).\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls25\ilvl2\cf0 {\listtext	\uc0\u8259 	}If tail is on the right as that of the second image in the figure, it is right skewed data. It is also called positive skewed data.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls25\ilvl3\cf0 {\listtext	\uc0\u8259 	}In order to fix a positively skewed distribution using a log10 distribution, the following assumptions have to be met:\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls25\ilvl4\cf0 {\listtext	\uc0\u8259 	}no negative values, no zeroes, and the positively skewed (if you have negative values or zeros, see around ~08:00 mins, {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=_c3dVTRIK9c"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=_c3dVTRIK9c}})\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls25\ilvl2\cf0 {\listtext	\uc0\u8259 	} If the tail is to the left of data, then it is called left skewed data. It is also called negatively skewed data.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls25\ilvl3\cf0 {\listtext	\uc0\u8259 	}Similarly, to fix a negatively skewed distribution using a log10 distribution, the same conditions have to be met:\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls25\ilvl4\cf0 {\listtext	\uc0\u8259 	}but this time we call the log10 function as \
\pard\tx3820\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li4320\fi-4320\pardirnatural\partightenfactor0
\ls25\ilvl5\cf0 {\listtext	\uc0\u8259 	}log10(max(x) - 1 + x)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls25\ilvl2\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls25\ilvl0\cf0 {\listtext	\uc0\u8226 	}Resolving outliers\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls25\ilvl1\cf0 {\listtext	\uc0\u8259 	}Anomaly Detection (best seen from boxplot)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls25\ilvl2\cf0 {\listtext	\uc0\u8259 	}Interquatile Range Method\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls25\ilvl0\cf0 {\listtext	\uc0\u8226 	}Aggregation of data\
\pard\tx720\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\tx10256\pardirnatural\partightenfactor0
\cf0 \
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\
CLASSIFICATION\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls26\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Decision Tree Classification\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls26\ilvl1\cf0 {\listtext	\uc0\u8259 	}graphical representation of all the possible solutions to a decision\
{\listtext	\uc0\u8259 	}decisions are based on some conditions \
{\listtext	\uc0\u8259 	}decisions made can be easily explained\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls26\ilvl2\cf0 {\listtext	\uc0\u8259 	}\uc0\u917 xample\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls26\ilvl3\cf0 {\listtext	\uc0\u8259 	}Independent variables: \'93Am I hungry?\'94     \'93Do I have $25?\'94\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls26\ilvl4\cf0 {\listtext	\uc0\u8259 	}Data is in binary form (yes or no)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls26\ilvl3\cf0 {\listtext	\uc0\u8259 	}Dependent variable: \'93Outcome\'94\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls26\ilvl4\cf0 {\listtext	\uc0\u8259 	}Data is \'93Go to restaurant\'94 \'93Buy a hamburger\'94 \'93Go to sleep\'94 (this is a multiple classification problem)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls26\ilvl3\cf0 {\listtext	\uc0\u8259 	}NOTE: When building a decision tree you first have to individually compare each independent variable to the dependent variable in order to determine which independent variable will begin the decision tree (classification problem, yes or no for all results for the independent and dependent variable, you keep track of which independent variables that has the lowest \'93Gini Impurity\'94 (aka the one that is most pure) and you begin with that one at the top of the tree.. See {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=7VeUPuFGJHk&list=PLsCzljdLz2iR66dIIpz9E9veNj7wijPS7"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=7VeUPuFGJHk&list=PLsCzljdLz2iR66dIIpz9E9veNj7wijPS7 }}. I thinkkk you can also look at the true pos, false pos, true neg, false neg. \
{\listtext	\uc0\u8259 	}Important: Decision trees for classification use Gini Impurity and/or Information Gain. Decision trees for regression use Standard Deviation Reduction (see {\field{\*\fldinst{HYPERLINK "http://saedsayad.com/decision_tree_reg.htm"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul http://saedsayad.com/decision_tree_reg.htm}}).\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls27\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Random Forest Classification\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls27\ilvl1\cf0 {\listtext	\uc0\u8259 	}Builds multiple decision trees and merges them together\
{\listtext	\uc0\u8259 	}more accurate and stable prediction\
{\listtext	\uc0\u8259 	}random decision forests correct for decision trees\'92 habit of overfitting to their training set\
{\listtext	\uc0\u8259 	}trained with the \'93bagging\'94 method\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 	
\f1 \uc0\u9642 
\f0 	Na\'efve Bayes Classification\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls28\ilvl1\cf0 {\listtext	\uc0\u8259 	}Classification technique based on Bayes\'92 Theorem\
{\listtext	\uc0\u8259 	}Assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0  \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 	
\f1 \uc0\u9642 
\f0 	K Nearest Neighbors Classifier\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls29\ilvl1\cf0 {\listtext	\uc0\u8259 	}Stores all the available cases and classifies new cases based on a similarity measure\
{\listtext	\uc0\u8259 	}The \'93K\'94 is the KNN algorithm is the nearest neighbors we wish to take a vote from\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls29\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example:\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls29\ilvl3\cf0 {\listtext	\uc0\u8259 	}Imagine a scatter plot with real training data on it representing 5 classes. Each dot on the scatter plot is a color. Generally, the color dots in this training set data are grouped by similar color dots. So visually, you can see five classes. Now, I think the KNN algorithm builds the model by looking at super small segments of the plot and starts a computational radius going outward from each segment. So if K is specified to be \'933.\'94 The computational radius will stop expanding for each segment when it hits three dots of the same color and will give that segment of the plot the appropriate color.\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls29\ilvl0\cf0 {\listtext	\uc0\u8259 	}from lightgbm import LightGBMClassifier\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\'97\
REGRESSION\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls30\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Decision Tree Regression\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls30\ilvl1\cf0 {\listtext	\uc0\u8259 	}For example, when visualizing two independent variables on a scatter plot, a decision tree algorithm \'93splits\'94 your dataset based off of information entropy (need to look up more) and based off of these splits is how the tree is made.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls30\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.saedsayad.com/decision_tree_reg.htm"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.saedsayad.com/decision_tree_reg.htm }}(example)
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul  \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls30\ilvl1
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}Important: Decision trees for classification use Gini Impurity and/or Information Gain. Decision trees for regression use Standard Deviation Reduction (see {\field{\*\fldinst{HYPERLINK "http://saedsayad.com/decision_tree_reg.htm"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul http://saedsayad.com/decision_tree_reg.htm}}).\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls31\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Random Forest Regression\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls31\ilvl1\cf0 {\listtext	\uc0\u8259 	}a form of ensemble learning, ensemble learning is when you take the same algorithm multiple times to make your model much more powerful than the original. Gradient boosting is a form of ensemble learning.\
{\listtext	\uc0\u8259 	}Not very sensitive to hyperparameters (need to verify still)\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls31\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Support Vector Regression\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls31\ilvl1\cf0 {\listtext	\uc0\u8259 	}Great video \'97> {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=Y6RRHw9uN9o&t=366s"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=Y6RRHw9uN9o&t=366s}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul  \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls31\ilvl2
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}Dog and cat example. X axis there is snout length and on the Y axis there is ear length. SVR algorithm looks at the outliers within the data set and applies a linear linear / nonlinear line according to them. These outliers are none as support vectors. Note, in the example in the video the data is separable between the cat and dog classes.\
{\listtext	\uc0\u8259 	}When using Support Vector algorithms it is often helpful to map your vectors to a higher dimension in order to fully (better?) separate the data. However, this is often computationally expensive.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls31\ilvl3\cf0 {\listtext	\uc0\u8259 	}See Kernel function or Kernel Trick. The dot product can be computed to project the vectors into a higher dimension.\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls32\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Logistic Regression\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls32\ilvl1\cf0 {\listtext	\uc0\u8259 	}Note, logistic regression is probably better placed under CLASSIFICATION.\
{\listtext	\uc0\u8259 	}Great explanation of how to interpret the coefficients in a logistic or linear regression model (see ~02:48:00, {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=5rNu16O3YNE&list=PLsCzljdLz2iR9pnDlZkTqSvbUZqZenhcK&index=5"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=5rNu16O3YNE&list=PLsCzljdLz2iR9pnDlZkTqSvbUZqZenhcK&index=5}})\
{\listtext	\uc0\u8259 	}Has to do only with probability. Think about example of where a bank decides they should allow you to get a loan or not depending on your credit score, income, age, etc..\
{\listtext	\uc0\u8259 	}Or think about an image data set of digits where the target variable is 0-9. You can do classification binary or multi class problems with LogisticRegression. \
{\listtext	\uc0\u8259 	}Not very sensitive to hyperparameters (need to verify still)\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls32\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Linear Regression\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls32\ilvl1\cf0 {\listtext	\uc0\u8259 	}Great explanation of how to interpret the coefficients in a logistic or linear regression model (see ~02:48:00, {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=5rNu16O3YNE&list=PLsCzljdLz2iR9pnDlZkTqSvbUZqZenhcK&index=5"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=5rNu16O3YNE&list=PLsCzljdLz2iR9pnDlZkTqSvbUZqZenhcK&index=5}})\
{\listtext	\uc0\u8259 	}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\'97\'97\'97\'97\'97\
CLUSTERING:\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls33\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Hierarchial Clustering\
{\listtext	
\f1 \uc0\u9642 
\f0 	}Clustering\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls33\ilvl1\cf0 {\listtext	\uc0\u8259 	}K-Means\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls33\ilvl2\cf0 {\listtext	\uc0\u8259 	}Very good for discovering categories or groups in your data that you may of not been able to recognize yourself.\
{\listtext	\uc0\u8259 	}Can work for multiple dimension problems\
{\listtext	\uc0\u8259 	}How it works (imagine a 2D scatter plot with a bunch of gray markers and we want to separate our data into 3 clusters (red, green, and blue)):\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls33\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.udemy.com/course/machinelearning/learn/lecture/5714416#overview"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.udemy.com/course/machinelearning/learn/lecture/5714416#overview}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul   
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone (great example)\
{\listtext	\uc0\u8259 	}Step 1. Choose the number K of clusters\
{\listtext	\uc0\u8259 	}Step 2. Select 
\f3\b at random
\f0\b0  K points.  The centroids (they don\'92t not necessarily have to be from or around your dataset).\
{\listtext	\uc0\u8259 	}Step 3. Assign each data point on the plot to the closest centroid that will in result form K clusters (generally in the form of Euclidean distance (there are other types, need to look up))\
{\listtext	\uc0\u8259 	}Step 4. Compute and place the new centroid of each cluster.\
{\listtext	\uc0\u8259 	}Step 5. Reassign each data point to the new closest centroid. If any reassignment took place, go back to Step 4, otherwise go to finish.\
{\listtext	\uc0\u8259 	}This is an iterative process.\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls33\ilvl2\cf0 {\listtext	\uc0\u8259 	}The algorithm runs quickly by drawing a straight line connecting to each centroid and then drawing a perpendicular line from that line easily separate the data set and see which data points are closest (well, Im not sure if that\'92s how the algorithm actually runs, but that is a great visual way to think about it, this would be really good to show on a technical presentation).\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\
\
MACHINE LEARNING ALGORITHMS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls34\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}A/B Testing\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls34\ilvl1\cf0 {\listtext	\uc0\u8259 	}Example, comparing which landing page of a website performs best.\
{\listtext	\uc0\u8259 	}A/A tests should be conducted first to make sure there\'92s nothing wrong on the backend such as if the data is messy, sampling problem because not randomizing properly, or too much noise.\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls34\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}KNeighborsRegressor (aka KNN)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls34\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html#sphx-glr-auto-examples-neighbors-plot-regression-py"}}{\fldrslt https://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html#sphx-glr-auto-examples-neighbors-plot-regression-py}}\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"}}{\fldrslt https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm}} (great explanation of the weights parameter)\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls34\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}DecisionTreeRegressor (aka CART)\
{\listtext	
\f1 \uc0\u9642 
\f0 	}SVM\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls34\ilvl1\cf0 {\listtext	\uc0\u8259 	}C-Parameter:\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls34\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel"}}{\fldrslt https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel}}     see Kent Munthe Caspersen answer\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls34\ilvl3\cf0 {\listtext	\uc0\u8259 	}\'93In general, having few training instances and many attributes make it easier to make a linear separation of the data.\'94\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls34\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}xgboost\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls34\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://xgboost.readthedocs.io/en/latest/tutorials/index.html"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://xgboost.readthedocs.io/en/latest/tutorials/index.html}}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\'97\'97\'97\'97\'97\
ASSOCIATED RULE LEARNING\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls35\ilvl0\cf0 {\listtext	\uc0\u8226 	}Apriori Algorithm (think of grocery cart example \'97> diapers and beer relationship)\
{\listtext	\uc0\u8226 	}FP Growth Model\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\'97\
REINFORCEMENT LEARNING\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls36\ilvl0\cf0 {\listtext	\uc0\u8226 	}Upper Confidence Bound Algorithm (\cf8 REVISIT 2019-08-10 21:33:37, DONT UNDERSTAND FULLY\cf0 )\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls36\ilvl1\cf0 {\listtext	\uc0\u8259 	}Multi Armed Bandit Problem\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls37\ilvl0\cf0 {\listtext	\uc0\u8226 	}Thompson Sampling\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls37\ilvl1\cf0 {\listtext	\uc0\u8259 	}Multi Armed Bandit Problem\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 	\'95	Important Terminology\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 	\uc0\u8259 	Explore vs. Exploitation\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\
METRICS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls38\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Use the sklearn.metrics to create a confusion matrix to analyze true negatives, false negatives, true positives, and false positives\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls39\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}See the section Model Selection & Boosting below for a better way to look at the performance of your models (GridSearchCV)\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls40\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}{\field{\*\fldinst{HYPERLINK "https://www.theanalysisfactor.com/assessing-the-fit-of-regression-models/"}}{\fldrslt https://www.theanalysisfactor.com/assessing-the-fit-of-regression-models/}}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls40\ilvl1\cf0 {\listtext	\uc0\u8259 	}Lower values of RMSE indicate better fit.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\
NATURAL LANGUAGE PROCESSING\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls41\ilvl0\cf4 \
{\listtext	\uc0\u8226 	}rule learning\
\ls41\ilvl0{\listtext	\uc0\u8226 	}sparsity\
{\listtext	\uc0\u8226 	}sparse matrices\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls41\ilvl0\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\'97\
DEEP LEARNING\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 	\'95	Important Terminology\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 	\uc0\u8259 	Output values of the neural network can either be continuous (eg, price), binary (eg, yes/no), or categorical\
	\uc0\u8259 	weighted sum, activation function (sigmoid, threshold, rectifier, hyperbolic tangent)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls42\ilvl1\cf0 {\listtext	\uc0\u8259 	}Think about churn modeling problem (trying to predict which customers will leave the bank or not)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 	\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\
ARTIFICIAL NEURAL NETWORKS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls43\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}NEURAL NETWORKS 	\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls43\ilvl1\cf0 {\listtext	\uc0\u8259 	}Additional Reading\
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\sl280\partightenfactor0
\ls44\ilvl2
\f2 \cf2 {\listtext	
\f1 \uc0\u8259 
\f2 	}{\field{\*\fldinst{HYPERLINK "https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications"}}{\fldrslt \expnd0\expndtw0\kerning0
\ul https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications}}
\f0 \cf0 \
\pard\tx720\tx1440\tx2160\pardeftab720\sl280\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 		\'95	How do neural networks learn?\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 		\uc0\u8259 	You can either hard code them or let them learn on their own. We are always 	trying to let the models learn on their own.\
		\uc0\u8259 	Basically the only thing we have control over in the programming are the weights\
		\uc0\u8259 	Batch gradient descent (think of the smooth parabolic cost function curve with the 	ball bouncing back and forth trying to find the minimum)\
		\uc0\u8259 	Stochastic gradient descent (this method is used to find the global minimum of a 	rough shaped parabolic cost function curve) - this method is faster than batch 	(there is also a mini-batch method)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls45\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://iamtrask.github.io/2015/07/27/python-network-part2/"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://iamtrask.github.io/2015/07/27/python-network-part2/}}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 		\'95	Important Terminology\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 		\uc0\u8259 	Back propagation (the process of optimizing the cost function (aka adjusting the 	weights to find optimal value to agree with the known value))\
		\uc0\u8259 	Normally the rectifier function is used for the development of the hidden layers 	and for the output layer the sigmoid function is used.\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls46\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}CONVOLUTIONAL NEURAL NETWORKS \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls47\ilvl1\cf0 {\listtext	\uc0\u8259 	}Think about the following images: duck or rabbit,  man looking at you or away from you, and image of the girl with duplicate facial features (eyes, nose, mouth, eyebrows)\
{\listtext	\uc0\u8259 	}Steps \'97> Convolution, Max Pooling, Flattening, and Full Connection\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls47\ilvl2\cf0 {\listtext	\uc0\u8259 	}\ul Convolution\ulnone : The key take away of what convolution is, is to find features in your image by using a feature detector (think of a feature detector as a specific feature that distinguishes an image from the other images) and putting them into a feature map. Then from the feature map, it preserves the spatial relationships between pixels\
{\listtext	\uc0\u8259 	}Convolution (Cont.): ReLU Layer \'97> we introduce this Rectifier Linear Units algorithm to break up linearity.\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls47\ilvl3\cf0 {\listtext	\uc0\u8259 	}Good read on convolution filters:  {\field{\*\fldinst{HYPERLINK "http://www.roborealm.com/help/Convolution.php"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul http://www.roborealm.com/help/Convolution.php}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul   
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls47\ilvl2\cf0 {\listtext	\uc0\u8259 	}\ul Max Pooling\ulnone : Looking for a specific feature of an image. For example, the tears/marks on a cheetah. It is one feature that makes the cheetah very distinct. Note, there are other forms of pooling (mean pooling, sum pooling, etc). In the example I was following on Udemy, the instructor mentioned that max pooling eliminates the need of unnecessary features. In the cheetah example, we were able to get rid of 75% of the information. Max pooling allows us to instantly see specific distinct regions. In summary, we are able to preserve distinct features, inducing spatial variance, and reducing the size of information and parameters (this helps us prevent over fitting)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls47\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "http://scs.ryerson.ca/~aharley/vis/conv/flat.html"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul http://scs.ryerson.ca/~aharley/vis/conv/flat.html}}\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \
\ls47\ilvl3
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "http://scs.ryerson.ca/~aharley/vis/conv/"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul http://scs.ryerson.ca/~aharley/vis/conv/}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \
\ls47\ilvl3
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "http://www.cs.cmu.edu/~aharley/"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul http://www.cs.cmu.edu/~aharley/}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone 	\uc0\u8259 	\ul Flattening\ulnone : For example, making a 3x3 feature map display vertically 1-9\
	\uc0\u8259 	\ul Full Connection\ulnone : The middle/inner hidden layers (these develop attributes that describe each output). The final inner layer nodes each pass on a vote/probability from 0.0 to 1.0 on whether the image presented is a cat or dog (for example)\
	\uc0\u8259 	\ul SUMMARY\ulnone : {\field{\*\fldinst{HYPERLINK "https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html"}}{\fldrslt https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html\
}}\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls48\ilvl1\cf0 {\listtext	\uc0\u8259 	}Softmax & Cross-Entropy (Loss Function)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls48\ilvl2\cf0 {\listtext	\uc0\u8259 	}These functions behave similar to how the the mean square error (MSE) / cost function works when linear fitting\
{\listtext	\uc0\u8259 	}Classification Error, Mean Squared Error, Cross-Entropy\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls48\ilvl3\cf0 {\listtext	\uc0\u8259 	}Cross-Entropy is typically the best value to look at to evaluate your neural network (note, cross-entropy involves a logarithmic function. Also, cross-entropy is only the best for classification problems)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 	\uc0\u8259 	Image Preprocessing\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\cf0 	\uc0\u8259 	{\field{\*\fldinst{HYPERLINK "https://keras.io/preprocessing/image/"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://keras.io/preprocessing/image/}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \

\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone 	\uc0\u8259 	To avoid overfitting, data augmentation is used.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\'97\
DIMENSIONALITY REDUCTION\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls49\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Two Types\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls49\ilvl1\cf0 {\listtext	\uc0\u8259 	}Feature Selection\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls49\ilvl2\cf0 {\listtext	\uc0\u8259 	}Backward elimination, forward selection, bidirectional elimination, score comparison, and more (these topics were covered in the Regression lecture)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls49\ilvl1\cf0 {\listtext	\uc0\u8259 	}Feature Extraction\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls49\ilvl2\cf0 {\listtext	\uc0\u8259 	}Principal Component Analysis (PCA)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls49\ilvl3\cf0 {\listtext	\uc0\u8259 	}see \'97> {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=_UVHneBUBW0"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=_UVHneBUBW0}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul  
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone and {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=FgakZw6K1QQ"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=FgakZw6K1QQ}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul  
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone (great) and {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=HMOI_lkzW08"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=HMOI_lkzW08}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul   
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone (NOTE: I think the cells are considered the classes/dependent variables and the genes are considered as the independent variables)\
{\listtext	\uc0\u8259 	}PCA is a form of data preprocessing I thinkk. It has nothing to do with the model/classifier. PCA is a method of compressing a lot of data into something that captures the essence of the original data. PCA reduces dimensions by focusing on the genes with the most variation.The business example I followed in the Udemy course involved analyzing 178 data observations (12 independent variables, 1 dependent variable (3 classes, each class represents a different wine)). The business was trying to predict based off the ingredients what type of wine the drink should be classified as. PCA was applied to reduce the independent variables to the ones that show the most variance. This also allows us to visualize the data in 2D or 3D better.  \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls49\ilvl2\cf0 {\listtext	\uc0\u8259 	}Linear Discriminant Analysis (LDA)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls49\ilvl3\cf0 {\listtext	\uc0\u8259 	}see \'97> {\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=azXCzI57Yfc"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=azXCzI57Yfc}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul  
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone (great)\
{\listtext	\uc0\u8259 	}LDA is very similar to PCA. It is also used in the preprocessing step for pattern classification. Its goal is to project a dataset onto a lower-dimensional space. However, LDA differs because in addition to finding the component axises with LDA. In other words, we are interested in maximizing the separability between the two (or more) groups/categories so we can make the best decisions. We are interested in the axes that maximize the separation between classes (think of the diagram that shows gaussian distributions on both the x-axis and y-axis). The goal of LDA is to project a feature space (a dataset n-dimensional samples) onto a small subspace k(where k is less than or equal to n-1) while maintaining the class-discriminatory information.\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls49\ilvl4\cf0 {\listtext	\uc0\u8259 	}Both PCA and LDA are linear transformation techniques used for dimensional reduction. PCA is described as unsupervised but LDA is supervised because of the relation to the dependent variable. \
{\listtext	\uc0\u8259 	}PCA: Component axes that maximize the variance\
{\listtext	\uc0\u8259 	}LDA: Maximizing the component axes for class-separation.\
\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls49\ilvl2\cf0 {\listtext	\uc0\u8259 	}Kernel PCA (this is a nonlinear reduction strategy)\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls49\ilvl3\cf0 {\listtext	\uc0\u8259 	}This is for nonlinear problems. Note, the same linear model can be used with the normal PCA object but this time we must use an additional argument "rbf.\'92 This accounts for the nonlinearity.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\'97\
MODEL SELECTION & BOOSTING\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls50\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Two Types \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls50\ilvl1\cf0 {\listtext	\uc0\u8259 	}Cross Vaidation (multiple methods)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls50\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=L_dQrZZjGDg"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=L_dQrZZjGDg}}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls50\ilvl1\cf0 {\listtext	\uc0\u8259 	}k-Fold Cross Validation\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls50\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=gJo0uNL-5Qw"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=gJo0uNL-5Qw}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul  
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls50\ilvl1\cf0 {\listtext	\uc0\u8259 	}Grid Search\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls50\ilvl2\cf0 {\listtext	\uc0\u8259 	}This technique involves grid search to find optimal hyperparameters. Note, 
\fs22 hyperparameters
\fs24  are the parameters you specify when you build your model.\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls51\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Boosting\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls51\ilvl1\cf0 {\listtext	\uc0\u8259 	}XGBoost\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls51\ilvl2\cf0 {\listtext	\uc0\u8259 	}It is the most powerful implementation of gradient boosting\
{\listtext	\uc0\u8259 	}When using XGBoost, you don\'92t have to use feature scaling,\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls52\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Mean Absolute Error (MAE) vs Root mean squared error (RMSE)        \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls52\ilvl1\cf0 {\listtext	\uc0\u8259 	}For continuous dependent variables only\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls52\ilvl2\cf0 {\listtext	\uc0\u8259 	}great read\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d}}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\'97\'97\'97\'97\'97\
PLOTS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls53\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Violin plots\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls53\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=M6Nu59Fsyyw"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=M6Nu59Fsyyw}}\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls54\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Line plot\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls54\ilvl1\cf0 {\listtext	\uc0\u8259 	}ax = sns.lineplot(x="pickup_hour", y="Trip_distance", legend="full" , data=summary_wdays_avg_duration, estimator=None, hue="day_of_week")\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\'97\
STATISTICS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls55\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Pearson Coefficient\
{\listtext	
\f1 \uc0\u9642 
\f0 	}Skewness\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls55\ilvl1\cf0 {\listtext	\uc0\u8259 	}measures the symmetry of a distribution\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls55\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Kurtosis \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls55\ilvl1\cf0 {\listtext	\uc0\u8259 	}measures how peaked or flat a distribution is\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls55\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Standard Error\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls55\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.investopedia.com/terms/s/standard-error.asp"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.investopedia.com/terms/s/standard-error.asp}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls56\ilvl0
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	
\f1 \uc0\u9642 
\f0 	}Standard Deviation\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls56\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.investopedia.com/terms/s/standarddeviation.asp"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.investopedia.com/terms/s/standarddeviation.asp}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls56\ilvl0
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	
\f1 \uc0\u9642 
\f0 	}Homoscedasticity\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls56\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://datascience.stackexchange.com/questions/20237/why-do-we-convert-skewed-data-into-a-normal-distribution"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://datascience.stackexchange.com/questions/20237/why-do-we-convert-skewed-data-into-a-normal-distribution}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls56\ilvl2
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}The errors your model commits should have the same variance, i.e. you should ensure the linear regression does not make small errors for low values of 
\f5 \uc0\u55349 \u56395 
\f0  and big errors for higher values of 
\f5 \uc0\u55349 \u56395 
\f0 . In other words, the difference between what you predict 
\f5 \uc0\u55349 \u56396 \u770 
\f0   and the true values 
\f5 \uc0\u55349 \u56396 
\f0  should be constant. You can ensure that by making sure that 
\f5 \uc0\u55349 \u56396 
\f0  follows a Gaussian distribution. (The proof is highly mathematical.)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\'97\'97\'97\'97\'97\
PYTHON\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls57\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Commonly used functions\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls57\ilvl1\cf0 {\listtext	\uc0\u8259 	}list()\
{\listtext	\uc0\u8259 	}set()\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls57\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://realpython.com/python-sets/"}}{\fldrslt https://realpython.com/python-sets/}}\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls57\ilvl3\cf0 {\listtext	\uc0\u8259 	}see the \'93Operators vs Methods\'94 section (the or operator)\
{\listtext	\uc0\u8259 	}that structure comes up a lot while getting unique elements for encoding with both a training and test set\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls58\ilvl1\cf0 {\listtext	\uc0\u8259 	}len()\
{\listtext	\uc0\u8259 	}range()\
{\listtext	\uc0\u8259 	}append()\
{\listtext	\uc0\u8259 	}apply()\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls58\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Syntax\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls58\ilvl1\cf0 {\listtext	\uc0\u8259 	}ebola_long["variable"].str.split("_", expand=True)         \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls58\ilvl2\cf0 {\listtext	\uc0\u8259 	}to view the syntax better, you can rewrite as \
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls58\ilvl4\cf0 {\listtext	\uc0\u8259 	}(ebola_long["variable\'92]\
{\listtext	\uc0\u8259 	}.str\
{\listtext	\uc0\u8259 	}.split("_", expand=True)\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls58\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Functions\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls59\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Libraries \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls59\ilvl1\cf0 {\listtext	\uc0\u8259 	}tflearn\
{\listtext	\uc0\u8259 	}keras \
{\listtext	\uc0\u8259 	}numba \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls59\ilvl2\cf0 {\listtext	\uc0\u8259 	}Tries to make all the computations numpy does really fast\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls59\ilvl1\cf0 {\listtext	\uc0\u8259 	}seaborn\
{\listtext	\uc0\u8259 	}matplotlib.pyplot\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls59\ilvl2\cf0 {\listtext	\uc0\u8259 	}Anatomy of a figure\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls59\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://matplotlib.org/3.1.1/gallery/showcase/anatomy.html"}}{\fldrslt https://matplotlib.org/3.1.1/gallery/showcase/anatomy.html}}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'97\'97\'97\'97\
NUMPY\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls60\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Common functions\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls60\ilvl1\cf0 {\listtext	\uc0\u8259 	}np.expm1()\
{\listtext	\uc0\u8259 	}np.linspace\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\'97\'97\'97\'97\'97\
PANDAS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\cf0 	
\f1 \uc0\u9642 
\f0 	Dataframe\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls61\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Dataframe\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls61\ilvl1\cf0 {\listtext	\uc0\u8259 	}Terminology\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls61\ilvl2\cf0 {\listtext	\uc0\u8259 	}Mean the same thing\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls61\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li3600\fi-3600\pardirnatural\partightenfactor0
\ls61\ilvl4\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>[\'93<myFeatureName1>\'94] and <myDataFrameName1>.<myFeatureName1>\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls61\ilvl1\cf0 {\listtext	\uc0\u8259 	}There are three parts to a dataframe\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls61\ilvl2\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.columns       (feature names)\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.index           (row names)\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.values          (body values, this is good for extracting data from the Dataframe to use the data in another library that may just use numpy arrays)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls61\ilvl1\cf0 {\listtext	\uc0\u8259 	}Dataframe information\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls61\ilvl2\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.info()\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls61\ilvl3\cf0 {\listtext	\uc0\u8259 	}\'93object\'94 are typically string values\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2160\fi-2160\pardirnatural\partightenfactor0
\ls61\ilvl2\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.types\
{\listtext	\uc0\u8259 	}If you extract a single column of data from a DataFrame set that new data  then becomes a Series object\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li2880\fi-2880\pardirnatural\partightenfactor0
\ls61\ilvl3\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls61\ilvl1\cf0 {\listtext	\uc0\u8259 	}aggregate()\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\'97\'97\'97\'97\'97\
Q&A STACKEXCHANGE\
\
\pard\pardeftab720\sl280\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://datascience.stackexchange.com/questions/26776/how-to-calculate-ideal-decision-tree-depth-without-overfitting"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://datascience.stackexchange.com/questions/26776/how-to-calculate-ideal-decision-tree-depth-without-overfitting}}\
\
\pard\pardeftab720\sl280\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://stackoverflow.com/questions/34162443/why-do-many-examples-use-fig-ax-plt-subplots-in-matplotlib-pyplot-python"}}{\fldrslt \cf0 https://stackoverflow.com/questions/34162443/why-do-many-examples-use-fig-ax-plt-subplots-in-matplotlib-pyplot-python}}\
\
\pard\pardeftab720\sl280\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://stackoverflow.com/questions/3584805/in-matplotlib-what-does-the-argument-mean-in-fig-add-subplot111"}}{\fldrslt \cf0 https://stackoverflow.com/questions/3584805/in-matplotlib-what-does-the-argument-mean-in-fig-add-subplot111}}\
\
\pard\pardeftab720\sl280\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://www.geeksforgeeks.org/python-pandas-dataframe-ix/"}}{\fldrslt \cf0 https://www.geeksforgeeks.org/python-pandas-dataframe-ix/}}\
\
\pard\pardeftab720\sl280\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://stackoverflow.com/questions/31593201/how-are-iloc-ix-and-loc-different"}}{\fldrslt \cf0 https://stackoverflow.com/questions/31593201/how-are-iloc-ix-and-loc-different}}\
\
\pard\pardeftab720\sl280\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://stats.stackexchange.com/questions/56302/what-are-good-rmse-values"}}{\fldrslt \cf0 https://stats.stackexchange.com/questions/56302/what-are-good-rmse-values}}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\'97\'97 \
GREAT YOUTUBE CHANNELS\
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls62\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Data School ({\field{\*\fldinst{HYPERLINK "https://www.youtube.com/channel/UCnVzApLJE2ljPZSeQylSEyg"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/channel/UCnVzApLJE2ljPZSeQylSEyg}})\
{\listtext	
\f1 \uc0\u9642 
\f0 	}codebasics\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li1440\fi-1440\pardirnatural\partightenfactor0
\ls62\ilvl1\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=gJo0uNL-5Qw"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.youtube.com/watch?v=gJo0uNL-5Qw}}          (Machine Learning Tutorial Python 12 - K Fold Cross Validation)\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://github.com/codebasics/py"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://github.com/codebasics/py}}            (see all repositories)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
---\'97\'97\'97\'97\
MEDIUM \
\
\pard\tx220\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\li720\fi-720\pardirnatural\partightenfactor0
\ls63\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}{\field{\*\fldinst{HYPERLINK "https://towardsdatascience.com/understanding-input-and-output-shapes-in-convolution-network-keras-f143923d56ca"}}{\fldrslt https://towardsdatascience.com/understanding-input-and-output-shapes-in-convolution-network-keras-f143923d56ca}}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\'97\'97\'97\'97\'97\
INTERVIEW\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\'97\'97\'97\
PROBLEM SOLVING TIPS\
\
\
\pard\tx220\tx720\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls64\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Classification Problem\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls64\ilvl1\cf0 {\listtext	\uc0\u8259 	}Typically its good to test logistic regression vs random forest classifier to see which one performs best. If the random forest classifier performs best, then we should look into boosting with either xgboost, lightgbm, or catboost\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\
\
Overall process (from Allstate Severity Example):\
\
\pard\tx220\tx720\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls65\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Data statistics\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls65\ilvl1\cf0 {\listtext	\uc0\u8259 	}Shape\
{\listtext	\uc0\u8259 	}Peek\
{\listtext	\uc0\u8259 	}Description\
{\listtext	\uc0\u8259 	}Skew\
\pard\tx220\tx720\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls65\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Transformation\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls65\ilvl1\cf0 {\listtext	\uc0\u8259 	}Correction of skew\
\pard\tx220\tx720\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls65\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Data Interaction\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls65\ilvl1\cf0 {\listtext	\uc0\u8259 	}Correlation\
{\listtext	\uc0\u8259 	}Scatter plot\
\pard\tx220\tx720\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls65\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Data Visualization\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls65\ilvl1\cf0 {\listtext	\uc0\u8259 	}Box and density plots\
{\listtext	\uc0\u8259 	}Grouping of one hot encoded attributes\
\pard\tx220\tx720\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls65\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Data Preparation\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls65\ilvl1\cf0 {\listtext	\uc0\u8259 	}One hot encoding of categorical data\
{\listtext	\uc0\u8259 	}Train-test split\
\pard\tx220\tx720\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls65\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Evaluation, Prediction, and Analysis\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls65\ilvl1\cf0 {\listtext	\uc0\u8259 	}Linear Regression (Linear algo)\
{\listtext	\uc0\u8259 	}Ridge Regression (Linear algo)\
{\listtext	\uc0\u8259 	}LASSO Linear Regression (Linear algo)\
{\listtext	\uc0\u8259 	}Elastic Net Regression (Linear algo)\
{\listtext	\uc0\u8259 	}KNN (non-linear algo)\
{\listtext	\uc0\u8259 	}CART (non-linear algo)\
{\listtext	\uc0\u8259 	}SVM (Non-linear algo)\
{\listtext	\uc0\u8259 	}Bagged Decision Trees (Bagging)\
{\listtext	\uc0\u8259 	}Random Forest (Bagging)\
{\listtext	\uc0\u8259 	}Extra Trees (Bagging)\
{\listtext	\uc0\u8259 	}AdaBoost (Boosting)\
{\listtext	\uc0\u8259 	}Stochastic Gradient Boosting (Boosting)\
{\listtext	\uc0\u8259 	}MLP (Deep Learning)\
{\listtext	\uc0\u8259 	}XGBoost\
{\listtext	\uc0\u8259 	}Make Predictions\
\pard\tx720\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\
######################\
CODE SNIPPETS\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls66\ilvl2\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx220\tx720\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls66\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Jupyter Notebook / Markdown\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls66\ilvl1\cf0 {\listtext	\uc0\u8259 	}Github\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls66\ilvl2\cf0 {\listtext	\uc0\u8259 	}This is a self-exercise notebook that was created while following along the Natural Language Processing section in the following Udemy course by SuperDataScience:\
{\listtext	\uc0\u8259 	}This is a self-exercise notebook that was created while following along in the following YouTube video by codebasics:\
\pard\tx220\tx720\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\partightenfactor0
\ls66\ilvl0\cf0 {\listtext	
\f1 \uc0\u9642 
\f0 	}Python\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls66\ilvl1\cf0 {\listtext	\uc0\u8259 	}Important\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls66\ilvl2\cf0 {\listtext	\uc0\u8259 	}There seems to be a lot of issues with copying snippets over from TextEdit to Jupyter. It\'92s something to do with single quotes. Change them to double quotes.\
{\listtext	\uc0\u8259 	}Snippet Template Names\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls66\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>\
{\listtext	\uc0\u8259 	}<>\
{\listtext	\uc0\u8259 	}<myFeatureName1>\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls66\ilvl4\cf0 {\listtext	\uc0\u8259 	}<myFeatureName1_categorical>\
{\listtext	\uc0\u8259 	}<myFeatureName1_continuous>\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls66\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myVarName1> \
{\listtext	\uc0\u8259 	}<myModelName1> \
{\listtext	\uc0\u8259 	}<myXTestName1>\
{\listtext	\uc0\u8259 	}<myFunctionName1> \
{\listtext	\uc0\u8259 	}<myInt1>\
{\listtext	\uc0\u8259 	}<myIndexName1>\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls66\ilvl1\cf0 {\listtext	\uc0\u8259 	}Pandas\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls66\ilvl2\cf0 {\listtext	\uc0\u8259 	}Info \
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls66\ilvl3\cf0 {\listtext	\uc0\u8259 	}pandas.__version__       # this is really useful when trying to get help from Google or StackOverflow\
{\listtext	\uc0\u8259 	}[]   # single brackets are for specifying if you are in rows or columns\
{\listtext	\uc0\u8259 	}[[]]  # inner brackets are typically used when you want to implement a list/multiple things\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls66\ilvl4\cf0 {\listtext	\uc0\u8259 	}[["year\'92, "date\'92]]        # example\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls66\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>[\'93<myFeatureName1>\'94] is the same thing as <myDataFrameName1>.<myFeatureName1>\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls66\ilvl2\cf0 {\listtext	\uc0\u8259 	}Set Options\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls66\ilvl3\cf0 {\listtext	\uc0\u8259 	}pd.set_option('display.max_columns', 999) # allows us to see all columns in Jupyter notebook\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls66\ilvl2\cf0 {\listtext	\uc0\u8259 	}Converting Data Types\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls66\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>[<myFeatureNameList1>] = <myDataFrameName1>[<myFeatureNameList1>].apply(pd.to_numeric, errors='coerce') \
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls66\ilvl1\cf0 {\listtext	\uc0\u8259 	}Commonly used\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls66\ilvl2\cf0 {\listtext	\uc0\u8259 	}timeit example\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls66\ilvl3\cf0 {\listtext	\uc0\u8259 	}%%timeit                                                                   # line 1 in Jupyter cell\
{\listtext	\uc0\u8259 	}<myFunctionName1>(<myDataFrameName1>[\'93<myFeatureName1>\'94].values, <myDataFrameName1>[\'93<myFeatureName2>\'94].values)    # line 2 in Jupyter cell\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls66\ilvl2\cf0 {\listtext	\uc0\u8259 	}print(train_X.shape)\
{\listtext	\uc0\u8259 	}print(type(train_X.shape))\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls66\ilvl1\cf0 {\listtext	\uc0\u8259 	}Commonly used libraries\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls66\ilvl2\cf0 {\listtext	\uc0\u8259 	}import pandas as pd\
{\listtext	\uc0\u8259 	}import numpy as np\
{\listtext	\uc0\u8259 	}import matplotlib.pyplot as plt\
{\listtext	\uc0\u8259 	}import seaborn as sns\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}import warnings\
{\listtext	\uc0\u8259 	}warnings.filterwarnings('ignore')\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}import tensorflow as tf \
{\listtext	\uc0\u8259 	}tf.logging.set_verbosity(tf.logging.ERROR)     # To surpress any TensorFlow warnings.  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}from sklearn.preprocessing import LabelEncoder, OneHotEncoder\
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.linear_model import LinearRegression \
{\listtext	\uc0\u8259 	}from sklearn.neighbors import KNeighborsRegressor      # KNN (Non-linear Algo) \
{\listtext	\uc0\u8259 	}from sklearn.tree import DecisionTreeRegressor  # CART\
{\listtext	\uc0\u8259 	}from sklearn.svm import SVR\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import BaggingRegressor\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import RandomForestRegressor\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import ExtraTreesRegressor\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import AdaBoostRegressor\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import GradientBoostingRegressor\
{\listtext	\uc0\u8259 	}from xgboost import XGBRegressor\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}import re # used often for NLP, removes and replaces characters \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}  \
{\listtext	\uc0\u8259 	}from keras.models import Sequential \
{\listtext	\uc0\u8259 	}from keras.layers import Convolution2D\
{\listtext	\uc0\u8259 	}from keras.layers import MaxPooling2D\
{\listtext	\uc0\u8259 	}from keras.layers import Flatten\
{\listtext	\uc0\u8259 	}from keras.layers import Dense\
{\listtext	\uc0\u8259 	}from keras.preprocessing.image import ImageDataGenerator\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls66\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://keras.io/preprocessing/image/"}}{\fldrslt https://keras.io/preprocessing/image/}}\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls66\ilvl2\cf0 {\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import libraries for deep learning\
{\listtext	\uc0\u8259 	}from keras.wrappers.scikit_learn import KerasRegressor\
{\listtext	\uc0\u8259 	}from keras.models import Sequential\
{\listtext	\uc0\u8259 	}from keras.layers import Dense\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Optimize using dropout and decay\
{\listtext	\uc0\u8259 	}from keras.optimizers import SGD \
{\listtext	\uc0\u8259 	}from keras.layers import Dropout \
{\listtext	\uc0\u8259 	}from keras.constraints import maxnorm \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls66\ilvl1\cf0 {\listtext	\uc0\u8259 	}Encoding\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls66\ilvl2\cf0 {\listtext	\uc0\u8259 	}pandas\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls66\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls66\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}<myDataFrameName1> = sns.load_dataset("tips")\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.head()\
{\listtext	\uc0\u8259 	}<myDataFrameName2> = pd.get_dummies(<myDataFrameName1>, drop_first=True) \
{\listtext	\uc0\u8259 	}<myDataFrameName2>.head() \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls66\ilvl2\cf0 {\listtext	\uc0\u8259 	}sklearn\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls66\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls67\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.preprocessing import LabelEncoder, OneHotEncoder\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}<myDataFrameName1> = pd.read_csv("train.csv") \
{\listtext	\uc0\u8259 	}<myDataFrameName2> = pd.read_csv("test.csv") \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}cols = <myDataFrameName1>.columns\
{\listtext	\uc0\u8259 	}split = <myIntOfCategoricalColumnsToEncode> \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}labels = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Making sure we account for all of the unique variables that show up in both the training and test set provided. \
{\listtext	\uc0\u8259 	}# For instance, this ensures we dont run into any unforeseen variables when going from the training set to test set.\
{\listtext	\uc0\u8259 	} for i in range(0, split):\
{\listtext	\uc0\u8259 	}     train = <myDataFrameName1>[cols[i]].unique()\
{\listtext	\uc0\u8259 	}     test = <myDataFrameName2>[cols[i]].unique()\
{\listtext	\uc0\u8259 	}     labels.append(list(set(train) | set(test))) #note the OR operator!  (See how sets work {\field{\*\fldinst{HYPERLINK "https://realpython.com/python-sets/"}}{\fldrslt https://realpython.com/python-sets/}})\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} print("labels %s" % labels)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}cats = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for i in range(0, split):\
{\listtext	\uc0\u8259 	}    label_encoder = LabelEncoder()  # Label Encode\
{\listtext	\uc0\u8259 	}    label_encoder.fit(labels[i])\
{\listtext	\uc0\u8259 	}    feature = label_encoder.transform(dataset_train.iloc[:,i])\
{\listtext	\uc0\u8259 	}    feature = feature.reshape(dataset_train.shape[0], 1)\
{\listtext	\uc0\u8259 	}    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))         # One Hot Encode\
{\listtext	\uc0\u8259 	}    feature = onehot_encoder.fit_transform(feature)\
{\listtext	\uc0\u8259 	}    cats.append(feature)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}print("################")\
{\listtext	\uc0\u8259 	}print("List of 1D array of cats--> %s" % cats)\
{\listtext	\uc0\u8259 	}print("################")\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Make a 2D array from a list of 1D arrays\
{\listtext	\uc0\u8259 	}encoded_cats = np.column_stack(cats) \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}print("################")\
{\listtext	\uc0\u8259 	}print("List of 2D array of cats--> %s" % encoded_cats)\
{\listtext	\uc0\u8259 	}print("################")\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}print("################")  \
{\listtext	\uc0\u8259 	}print(encoded_cats.shape)  # These are the categorical variables we just encoded. Now, we just need to concatenate it with the continuous vars.\
{\listtext	\uc0\u8259 	}print("################")\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Concatenate encoded attributes with continuous attributes\
{\listtext	\uc0\u8259 	}<myDataFrameName3> = np.concatenate((encoded_cats,dataset_train.iloc[:,split:].values),axis=1) \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}del cats\
{\listtext	\uc0\u8259 	}del dataset_train\
{\listtext	\uc0\u8259 	}del encoded_cats\
{\listtext	\uc0\u8259 	} \
\
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Data Set Details\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.info() # data info. ex: datatypes, size etc.\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.dtypes\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.describe()\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.skew()      # Values close to 0 show less skew.\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.shape\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}Column Details\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}<myVarName1> = <myDataFrameName1>["<myFeatureName1>\'92].min() # minimum trip distance, same for max()\
{\listtext	\uc0\u8259 	}<myVarName2> = <myDataFrameName1>["<myFeatureName1>\'92].value_counts()      # counts unique occurrences \
{\listtext	\uc0\u8259 	}<myVarName2>.head(10) \
\pard\tx1660\tx2160\tx2880\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}Searching for NaN Values\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe}}\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls68\ilvl5\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.isnull().sum()                (searches for which features contain NaN values)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}Renaming columns\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1> = <myDataFrameName1>.rename(columns=\{"<myOldName1>\'92: "<myNewName1>\'92, "<myOldName2>\'92: "<myNewName2>\'92\})\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}To see a random sample of data\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.sample(30) \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}Return names of columns where a certain character shows up\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.columns[<myDataFrameName1>.isin(['?']).any()] \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Shuffling the DataFrame\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1> = <myDataFrameName1>.sample(frac=1).reset_index(drop=True)\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Reading In Data\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}CSV\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}df = pd.read_csv("green_tripdata_2015-09.csv", low_memory=False, parse_dates=["lpep_pickup_datetime"])                 # the parse_dates argument is turning "lpep_pickup_datetime" from an object type into a datetime64 type \
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}TSV\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}dataset = pd.read_csv("Restaurant_Reviews.tsv", delimiter="\\t")\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Converting data types\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}series and numpy array\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>[\'93<myFeatureName1>\'94]                 # this gives a series\
{\listtext	\uc0\u8259 	}<myDataFrameName1>[\'93<myFeatureName1>\'94].values      # this gives a numpy array\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Creating a DataFrame\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}Manually\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1> = pd.DataFrame(\{\
{\listtext	\uc0\u8259 	}  "<myFeatureName1>": [10, 20, 30],\
{\listtext	\uc0\u8259 	}  "<myFeatureName2>": [20, 30, 40]\
{\listtext	\uc0\u8259 	}\})\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Operations on a DataFrame\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}By Feature (this is called Broadcasting)\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>[\'93<myFeatureName1>\'94] ** 2     # this squares every row\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}Adding rows\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>["<myFeatureName1>"] + <myDataFrameName1>["<myFeatureName2>"]\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Copy a data frame\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName2> = <myDataFrameName1>.copy()                                  # our dataframe\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Subsetting a data frame\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}By columns\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}Into a series\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName2> = <myDataFrameName1>["<myFeatureName1>\'92]\
{\listtext	\uc0\u8259 	}<myDataFrameName2>.head()\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}Into a dataframe\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName2> = <myDataFrameName1>[["<myFeatureName1>", "<myFeatureName2>", "<myFeatureName3>"]]\
{\listtext	\uc0\u8259 	}<myDataFrameName2>.head()     # note if we did a single column it would be a series\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}By rows\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.loc[[2,0]]           # label based indexing, note the algorithm isn\'92t actually looking for the index. It is doing string matching instead.\
{\listtext	\uc0\u8259 	}loc is typically used \
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.iloc[[0, 1, -1]]    # another example of positional indexing, note this prints the first two rows and the last row of the data frame\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}Range Selection\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}By Columns\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls68\ilvl5\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>[[\'93<myFeatureName1>\'94,\'94<myFeatureName2>\'94]] \
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls68\ilvl5\cf0 {\listtext	\uc0\u8259 	}# Get the column names\
{\listtext	\uc0\u8259 	}<myColumnNames1>  = <myDataFrameName1>.columns[<myIndexInt1>:<myIndexInt2>]\
{\listtext	\uc0\u8259 	}# Then get the data columns \
{\listtext	\uc0\u8259 	} <myDataFrameName1>[<myColumnNames1>]\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}By Rows\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls68\ilvl5\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.iloc[<myIndexInt1>:<myIndexInt2>]\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}By Columns and Rows\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls68\ilvl5\cf0 {\listtext	\uc0\u8259 	}<myIndexName1>  = <myDataFrameName1>.columns[<myInt1>:<myInt2>]  # returns an Index of column names\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls68\ilvl5\cf0 {\listtext	\uc0\u8259 	}<myDataFrameSeriesName1>  = <myDataFrameName1>[<myIndexName1>] # returns a data frame series\
{\listtext	\uc0\u8259 	}<myDataFrameName2> =  <myDataFrameSeriesName1>.iloc[<myInt3>:<myInt4>] # returns data frame subsetted by rows and columns\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls68\ilvl5\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName2> = <myDataFrameName1>[<myDataFrameName1>.columns[<myInt1>:<myInt2>]].iloc[<myInt3>:<myInt4>]\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Filtering dataframe\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}df.loc[(df[\'93smoker"]=="No\'94)&(df["total_bill"] >= 10)]                                  # Filter rows by smoker == "No" and total_bill >= 10\
{\listtext	\uc0\u8259 	}df (["smoker","day","time"])["total_bill"].mean()                                            # What is the average total_bill for each value of smoker, day, and time?\
{\listtext	\uc0\u8259 	}df.groupby(["smoker","day","time"])["total_bill"].mean().reset_index()          # To get back to a Dataframe \
{\listtext	\uc0\u8259 	}df.loc[df["year"]==1967,["year", "pop"]]\
{\listtext	\uc0\u8259 	}df.loc[(df["year"]==1967) & (df["pop"] > 1_000_000),["year", "pop"]]         # Note: Python has a neat feature where it ignores underscores in integers to help visualize.\
{\listtext	\uc0\u8259 	}df.loc[(df["year"]==1967) | (df["pop"] > 1_000_000),["year", "pop"]]\
{\listtext	\uc0\u8259 	}df[billboard_melt["track"] == "Loser\'94]\
{\listtext	\uc0\u8259 	}df.groupby(["year"])["lifeExp"].mean() \
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}The aggregate function in the numpy library can do the same thing\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}df.groupby(["year"])["lifeExp"].agg(np.mean)\
{\listtext	\uc0\u8259 	}df.groupby(["year"])["lifeExp"].agg(np.std)\
{\listtext	\uc0\u8259 	}df.groupby(["year", "continent"])[["lifeExp", "gdpPercap"]].agg(np.mean)      # note, the created Dataframe becomes wonky when using a list\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Fixing Skewed Data\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}#log1p function applies log(1+x) to all elements of the column\
{\listtext	\uc0\u8259 	}<myDataFrame1>[\'93<myFeatureName1>\'94] = np.log1p(<myDataFrame1>[\'93<myFeatureName1>\'94])\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Unskewing Skewed Data\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}<myVarName1> = np.expm1(<myModelName1>.predict(<myXTest1>)) \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Tidy data (cleaning data)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "http://vita.had.co.nz/papers/tidy-data.pdf"}}{\fldrslt http://vita.had.co.nz/papers/tidy-data.pdf}} (first three sections are a good read)\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}Criteria 1 (what we don"t want): Column headers are values, not variable names\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}aka going from wide data to long data\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls68\ilvl5\cf0 {\listtext	\uc0\u8259 	}pew_long = pd.melt(pew, id_vars="religion")       # doesn\'92t touch the religion column, and condenses the rest of the data frame into a \'93variable\'94 and \'93value\'94 column\
{\listtext	\uc0\u8259 	}pew_long = pd.melt(pew, id_vars="religion", var_name="income", value_name="count")   # renames the variable and value heading. \
{\listtext	\uc0\u8259 	}billboard_melt = pd.melt(billboard, id_vars=["year","artist","track","time","date.entered"], var_name=\'93week\'94, value_name=\'93rating\'94)        # many column example\
{\listtext	\uc0\u8259 	}billboard_melt = pd.melt(billboard, id_vars=["year","artist","track","time","date.entered"], var_name=\'93week\'94, value_name=\'93rating\'94).groupby("artist")["rating\'92].mean() \
{\listtext	\uc0\u8259 	}billboard_melt = pd.melt(billboard, id_vars=["year","artist","track","time","date.entered"], var_name=\'93week\'94, value_name=\'93rating\'94).groupby("artist")["rating\'92].mean().reset_index() \
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}Criteria 2 (what we don"t want): Multiple variables are stored in one column. #random keywords: parse\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}variable_split = ebola_long["variable"].str.split("_")        # Note, the \'93.str.\'94 is called an accessor. There is a commonly used \'93.dt.\'94 accessor too.\
{\listtext	\uc0\u8259 	}ebola_long["status"] = variable_split.str.get(0)\
{\listtext	\uc0\u8259 	}ebola_long["country"] = variable_split.str.get(1) \
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls68\ilvl5\cf0 {\listtext	\uc0\u8259 	}or, you can do the above code all in one line with\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls68\ilvl6\cf0 {\listtext	\uc0\u8259 	}ebola_long[["status", "country"]] = (ebola_long["cd_country"].str.split("_", expand=True))             #this is also an example of concatenating\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}Criteria 3 (what we don"t want): Variables are stored in both rows and columns\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}Note, # if we see a lot of rows with repeated values, this is a symptom of this type of problem. \
{\listtext	\uc0\u8259 	}weather_melt = pd.melt(weather, id_vars=["id","year", "month", "element"], var_name="day", value_name="temp") \
{\listtext	\uc0\u8259 	}weather_tidy = weather_melt.pivot_table(index=["id","year", "month","day"], columns="element",values="temp")        # note, pivot_table is the opposite of melt. Also, by default, this drops NaN values.\
{\listtext	\uc0\u8259 	}weather_tidy\
{\listtext	\uc0\u8259 	}weather_tidy.reset_index()\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}Criteria 4 (what we don"t want): Multiple types of observational units are stored in the same table  / This is also called "normalization"\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}billboard_melt.loc[billboard_melt["track"] == "Loser"]\
{\listtext	\uc0\u8259 	}billboard_songs = billboard_melt[["year", "artist", "track", "time"]]\
{\listtext	\uc0\u8259 	}billboard_songs = billboard_songs.drop_duplicates()\
{\listtext	\uc0\u8259 	}billboard_songs["id"] = range(len(billboard_songs))\
{\listtext	\uc0\u8259 	}billboard_ratings = billboard_melt.merge(billboard_songs, on = ["year", "artist", "track", "time"]) # merges billboard_melt (left) and billboard_songs (right)\
{\listtext	\uc0\u8259 	}billboard_ratings = billboard_ratings[["id", "date.entered","week","rating"]]\
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}Criteria 5 (what we don\'92t want): A single observational unit is stored in multiple tables\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Joining Tables/Datasets\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}billboard_songs["id"] = range(len(billboard_songs)) \
{\listtext	\uc0\u8259 	}billboard_ratings = billboard_melt.merge(billboard_songs, on = ["year", "artist", "track", "time\'92])    # merges billboard_melt (left) and billboard_songs (right)\
{\listtext	\uc0\u8259 	}billboard_ratings = billboard_ratings[["id", "date.entered","week","rating"]]\
{\listtext	\uc0\u8259 	}billboard_ratings.head()\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Saving to csv\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}billboard_songs.to_csv("billboard_songs.csv", index=False) \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Concatenating (similar to joining)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}X = np.concatenate((X_train,X_test), axis=0)   \
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Functions\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}We use what is called an assert statement to test a function. This is the basis of unit testing.\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}assert my_sq(4) == 16              # if you run this code nothing happens, but if you change \'9316\'94 to \'9315\'94 the code will crash\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}Broadcasting\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}df[\'93a\'94] ** 2     # this squares every row in the column labeled a\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}With a data series (specific to one feature/column)\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls68\ilvl5\cf0 {\listtext	\uc0\u8259 	}def my_sq(x)\
{\listtext	\uc0\u8259 	}    return x ** 2\
{\listtext	\uc0\u8259 	}df[\'93a\'94].apply(my_sq)\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls68\ilvl5\cf0 {\listtext	\uc0\u8259 	}def my_sq(x, e)\
{\listtext	\uc0\u8259 	}    return x ** e\
{\listtext	\uc0\u8259 	}df[\'93a\'94].apply(my_sq, e)\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}With a data frame (applies a function to each entire column of a dataframe)\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 1   # the apply statement here will print all data out in each column of the dataframe\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls68\ilvl5\cf0 {\listtext	\uc0\u8259 	}def print_me(x):\
{\listtext	\uc0\u8259 	}    return x\
{\listtext	\uc0\u8259 	}df.apply(print_me)  \
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 2  #  Get the mean of every column\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls68\ilvl5\cf0 {\listtext	\uc0\u8259 	}def avg_apply(col):\
{\listtext	\uc0\u8259 	}    return np.mean(col)\
{\listtext	\uc0\u8259 	}df.apply(avg_apply)\
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 3 # Get the mean of every column \
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls68\ilvl5\cf0 {\listtext	\uc0\u8259 	}def avg_apply(col):\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls68\ilvl5\cf0 {\listtext	\uc0\u8259 	}    x = col[0]\
{\listtext	\uc0\u8259 	}    y = col[1]\
{\listtext	\uc0\u8259 	}    z = col[2]\
{\listtext	\uc0\u8259 	}    return (x+y+z)/3\
{\listtext	\uc0\u8259 	}df.apply(avg_apply)       # or we could do df.apply(avg_apply, axis=\'93columns\'94) to get the average of each row\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 4 # A more realistic example\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls68\ilvl5\cf0 {\listtext	\uc0\u8259 	}@np.vectorize              # Important step. We have to vectorize our functions so it can pass rows one at a time. We write this at the top right before the function.\
{\listtext	\uc0\u8259 	}def avg_2_mod(x, y):\
{\listtext	\uc0\u8259 	}    if (x==20):\
{\listtext	\uc0\u8259 	}        return np.NaN\
{\listtext	\uc0\u8259 	}    else:\
{\listtext	\uc0\u8259 	}        return (x + y) / 2\
{\listtext	\uc0\u8259 	}avg_2_mod(df["a"], df["b"])        #if we use number we have to do avg_2_mod(df[\'93a\'94].values, df[\'93b\'94].values)\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls68\ilvl4\cf0 {\listtext	\uc0\u8259 	}Example 5\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls68\ilvl5\cf0 {\listtext	\uc0\u8259 	}def extract_population(rate):\
{\listtext	\uc0\u8259 	}    population = rate.split("/")[1]\
{\listtext	\uc0\u8259 	}    return population                                            # if we want to return population as an integer we can just return it as int(population)\
{\listtext	\uc0\u8259 	}assert extract_population("123/456") == "456\'94   # quick way to check to make sure the function won\'92t fail       # random keywords: unit testing\
{\listtext	\uc0\u8259 	}tbl3["population"] = tbl3["rate"].apply(extract_population)                  # to apply the function to the column and create a new column called \'93population\'94\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls68\ilvl1\cf0 {\listtext	\uc0\u8259 	}Splitting into train and test set\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}IMPORTANT: Split into X_train, X_test, y_train, y_test like this (MAKE sure X and y are data frames! Not np.ndarray\'92s! It keeps X_train, etc. as data frames so you can still run X_train.head() after splitting ) \
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}X = df_model                             # entire data frame\
{\listtext	\uc0\u8259 	}y = df_model.tip_percent           # tip_percent (dependent / target var)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(\
{\listtext	\uc0\u8259 	}    X, y, test_size=0.2, random_state=seed)\
\pard\tx1660\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls68\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls68\ilvl3\cf0 {\listtext	\uc0\u8259 	}  \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	} r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)]\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del <myDataFrameName1>\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0 \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Splitting the data\
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split \
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split( \
{\listtext	\uc0\u8259 	}    X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls69\ilvl3\cf0 {\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls70\ilvl3\cf0 {\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls71\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls72\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls73\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls74\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls75\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
\pard\tx2380\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls76\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\pard\tx940\tx1440\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls77\ilvl1\cf0 {\listtext	\uc0\u8259 	}Grabbing specific feature values\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls77\ilvl2\cf0 {\listtext	\uc0\u8259 	}df["Trip_distance"][df["Trip_distance"]<50]\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls77\ilvl1\cf0 {\listtext	\uc0\u8259 	}Duplicating time column, then striping hour field of it, and putting the hour value into a new column\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls77\ilvl2\cf0 {\listtext	\uc0\u8259 	}df["pickup"] = df["lpep_pickup_datetime"].apply(lambda x: datetime.strptime(x, "%Y-%m-%d %H:%M:%S"))\
{\listtext	\uc0\u8259 	}df["pickup_hour"] = df["pickup"].apply(lambda x: x.hour) \
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls77\ilvl1\cf0 {\listtext	\uc0\u8259 	}Creating a new feature from an existing date time column (datetime64 type):\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls77\ilvl2\cf0 {\listtext	\uc0\u8259 	}X_train["pickup_day"] = X_train["pickup_datetime"].dt.day\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls77\ilvl1\cf0 {\listtext	\uc0\u8259 	}This puts, the average trip distance of every pickup hour into a new column\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls77\ilvl2\cf0 {\listtext	\uc0\u8259 	}df["avg_trip_dist_per_pickup_hour"] = df[["Trip_distance","pickup_hour"]].groupby("pickup_hour").mean()\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls77\ilvl1\cf0 {\listtext	\uc0\u8259 	}List Unique Values In A pandas Column\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls77\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://chrisalbon.com/python/data_wrangling/pandas_list_unique_values_in_column/"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://chrisalbon.com/python/data_wrangling/pandas_list_unique_values_in_column/}}\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls78\ilvl1\cf0 {\listtext	\uc0\u8259 	}select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls78\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/"}}{\fldrslt 
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/}}
\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls79\ilvl1
\f0 \cf0 \kerning1\expnd0\expndtw0 \ulnone {\listtext	\uc0\u8259 	}Groups two features, and provides the average of a third feature, and stores it into a dataframe\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls79\ilvl2\cf0 {\listtext	\uc0\u8259 	}summary_wdays_avg_duration = pd.DataFrame(df.groupby(["day_of_week","pickup_hour"])["Trip_distance"].mean())     (line 1)\
{\listtext	\uc0\u8259 	}summary_wdays_avg_duration              (line 2)\
{\listtext	\uc0\u8259 	}summary_wdays_avg_duration.reset_index(inplace = True)                 (resets the index for the data frame / makes one?)\
{\listtext	\uc0\u8259 	}summary_wdays_avg_duration["unit"]=1                (makes a new column with all one\'92s)\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls79\ilvl1\cf0 {\listtext	\uc0\u8259 	}Drop row or drop column\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls79\ilvl2\cf0 {\listtext	\uc0\u8259 	}train_X = X_train.drop(["Tip_amount", "tip_percent", "log_tip_percent"], axis=1)               # how to drop multiple columns\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/"}}{\fldrslt https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/}}\
{\listtext	\uc0\u8259 	}df = df[df["Tip_amount"] > 0]                                                     # removes all instances in dataframe where Tip_amount <= 0\
{\listtext	\uc0\u8259 	}Calculations with two columns \
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls79\ilvl3\cf0 {\listtext	\uc0\u8259 	}df["tip_percent"] = df["Tip_amount"]/df["Total_amount"] # calculate tip percentage \
{\listtext	\uc0\u8259 	}df["tip_percent"] = df["tip_percent"].apply(lambda x: x * 100) # multiply by 100 to get the %\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls79\ilvl2\cf0 {\listtext	\uc0\u8259 	}Python Pandas : Drop columns in DataFrame by label Names or by Index Positions\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls79\ilvl3\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://thispointer.com/python-pandas-drop-columns-in-dataframe-by-label-names-or-by-index-positions/"}}{\fldrslt https://thispointer.com/python-pandas-drop-columns-in-dataframe-by-label-names-or-by-index-positions/}}\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls79\ilvl2\cf0 {\listtext	\uc0\u8259 	}Drop Column\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls79\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls79\ilvl4\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.drop(\'91<myFeatureName1>', axis=1, inplace=True)\
\pard\tx1660\tx2160\tx2880\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls79\ilvl2\cf0 {\listtext	\uc0\u8259 	}Drop Index\
\pard\tx2380\tx2880\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls79\ilvl3\cf0 {\listtext	\uc0\u8259 	}See \'93Drop Column\'94\
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls79\ilvl1\cf0 {\listtext	\uc0\u8259 	}Good To Know\'92s When Plotting\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls79\ilvl2\cf0 {\listtext	\uc0\u8259 	}Always pay attention to what your objects are returning while looking at the documentation for matplotlib, seaborn, etc.\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls79\ilvl1\cf0 {\listtext	\uc0\u8259 	}Preparing data to plot with matplotlib, etc (note all features that go into the plot arguments have the size of \'93(some#, )\'94    )\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls79\ilvl2\cf0 {\listtext	\uc0\u8259 	}grouped_df = pd.DataFrame(df.groupby(["pickup_hour", "Airport"])["tip_percent"].aggregate(np.mean).reset_index())\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls79\ilvl1\cf0 {\listtext	\uc0\u8259 	}Quick plotting with Pandas\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls79\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls79\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.<myFeatureName1_categorical>.plot(kind=\'93bar\'94)\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls79\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls79\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.<myFeatureName1_continuous>.plot(kind=\'93hist\'94)\
\pard\tx940\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls79\ilvl1\cf0 {\listtext	\uc0\u8259 	}Heatmaps\
\pard\tx1660\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls79\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html"}}{\fldrslt https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html}}\
{\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://docs.scipy.org/doc/numpy/reference/generated/numpy.triu_indices.html#numpy.triu_indices"}}{\fldrslt https://docs.scipy.org/doc/numpy/reference/generated/numpy.triu_indices.html#numpy.triu_indices}}\
{\listtext	\uc0\u8259 	}Example 1 (shows one heat map)\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls79\ilvl3\cf0 {\listtext	\uc0\u8259 	}sns.set(style="white")\
{\listtext	\uc0\u8259 	}# Generate a large random dataset\
{\listtext	\uc0\u8259 	}df_model_copy = <myDataFrameName1>.copy() # our dataframe\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Compute the correlation matrix\
{\listtext	\uc0\u8259 	}corr = df_model_copy.corr() # corr calculation\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Generate a mask for the upper triangle. Note, we only want to show the relevant part\
{\listtext	\uc0\u8259 	}# of the heat map\
{\listtext	\uc0\u8259 	}mask = np.zeros_like(corr, dtype=np.bool)\
{\listtext	\uc0\u8259 	}mask[np.triu_indices_from(mask)] = True\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Generate a custom diverging colormap\
{\listtext	\uc0\u8259 	}cmap = sns.diverging_palette(220, 10, as_cmap=True)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Draw the heatmap with the mask and correct aspect ratio\
{\listtext	\uc0\u8259 	}sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\
{\listtext	\uc0\u8259 	}            square=True, linewidths=.5, cbar_kws=\{"shrink": .5\})\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}plt.show()\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls79\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2 (shows two different heat maps, one before data skewing, and one after)\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls79\ilvl3\cf0 {\listtext	\uc0\u8259 	}# Make sure we use the subsample in our correlation\
{\listtext	\uc0\u8259 	}f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Entire DataFrame\
{\listtext	\uc0\u8259 	}corr = <myDataFrameName1>.corr()\
{\listtext	\uc0\u8259 	}sns.heatmap(corr, cmap="coolwarm_r", annot_kws=\{"size":20\}, ax=ax1)\
{\listtext	\uc0\u8259 	}ax1.set_title("Imbalanced Correlation Matrix \\n (don"t use for reference)", fontsize=14)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}sub_sample_corr = <myDataFrameName2_withSkewCorrection>.corr()\
{\listtext	\uc0\u8259 	}sns.heatmap(sub_sample_corr, cmap="coolwarm_r", annot_kws=\{"size":20\}, ax=ax2)\
{\listtext	\uc0\u8259 	}ax2.set_title("SubSample Correlation Matrix \\n (use for reference)", fontsize=14)\
{\listtext	\uc0\u8259 	}plt.show()\
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls79\ilvl1\cf0 {\listtext	\uc0\u8259 	}Violin Plots\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls79\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls79\ilvl3\cf0 {\listtext	\uc0\u8259 	}# We will visualize all the continuous attributes using Violin Plot - a combination of box and density plots\
{\listtext	\uc0\u8259 	}# Creating a dataframe with only continuous features\
{\listtext	\uc0\u8259 	}data = <myDataFrameSetForContinuousVarOnly>.iloc[:, 116:] # Creating a dataframe with only continuous features\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}cols=data.columns \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Plot violin for all attributes in a 7x2 facetgrid \
{\listtext	\uc0\u8259 	}n_cols = 2\
{\listtext	\uc0\u8259 	}n_rows = 7\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(n_rows):\
{\listtext	\uc0\u8259 	}    fg, ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(12,8))\
{\listtext	\uc0\u8259 	}    for j in range(n_cols):\
{\listtext	\uc0\u8259 	}        sns.violinplot(y=cols[i*n_cols+j], data=dataset_train, ax=ax[j])\
{\listtext	\uc0\u8259 	} \
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls79\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls79\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Single Violin Plot\
{\listtext	\uc0\u8259 	}sns.violinplot(data=<myDataFrameName1>, y=\'93<myFeatureName1_continuous>\'93) \
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls79\ilvl1\cf0 {\listtext	\uc0\u8259 	}Subplotting (subplots)\
\pard\tx720\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\
\
\
\
\
\
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls80\ilvl1\cf0 {\listtext	\uc0\u8259 	}Pair Plots \
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls80\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1  (sns.pairplot)\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls80\ilvl3\cf0 {\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}data = <myDataFrameSetForContinuousVarOnly>.iloc[:, 116:] # Creating a dataframe with only continuous features\
{\listtext	\uc0\u8259 	}cols=data.columns                                                            # Getting the names of all the columns\
{\listtext	\uc0\u8259 	}data_corr = data.corr()                                                      # Calculating Pearson coefficient for all combinations\
{\listtext	\uc0\u8259 	}threshold = <myThresholdValue>                                     # Setting the threshold to select only highly correlated attributes. Eg, 0.5. \
{\listtext	\uc0\u8259 	}corr_list = []                                                                       # List of pairs along with correlation above threshold\
{\listtext	\uc0\u8259 	}size = <myIntegerOfFeaturesToBeConsidered>                          # Eg, 15. \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Searching for the highly correlated pairs\
{\listtext	\uc0\u8259 	}for i in range(0, size):                                                       #for continuous features\
{\listtext	\uc0\u8259 	}    for j in range(i+1, size):\
{\listtext	\uc0\u8259 	}        if (data_corr.iloc[i,j] >= threshold and data_corr.iloc[i,j] < 1) or (data_corr.iloc[i,j] < 0 and data_corr.iloc[i,j] <= -threshold):\
{\listtext	\uc0\u8259 	}            corr_list.append([data_corr.iloc[i,j],i,j])                   # stores coefficient and appropriate column indexes\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Sorting to show higher ones first            \
{\listtext	\uc0\u8259 	}s_corr_list = sorted(corr_list,key=lambda x: -abs(x[0])).    # See key function, https://docs.python.org/3/howto/sorting.html\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for v, i, j in s_corr_list: \
{\listtext	\uc0\u8259 	}    print("%s and %s = %.2f" % (cols[i],cols[j],v))\
{\listtext	\uc0\u8259 	}                    \
{\listtext	\uc0\u8259 	}# Scatter plot of all the highly correlated pairs\
{\listtext	\uc0\u8259 	}for v, i, j in s_corr_list:\
{\listtext	\uc0\u8259 	}    sns.pairplot(dataset_train, size=6, x_vars=cols[i], y_vars=cols[j])\
{\listtext	\uc0\u8259 	}    plt.show\
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls80\ilvl1\cf0 {\listtext	\uc0\u8259 	}Boxplots\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls80\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1 (box plots with hours)\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls80\ilvl3\cf0 {\listtext	\uc0\u8259 	}f, axes = plt.subplots(ncols=4, figsize=(20,4))\
{\listtext	\uc0\u8259 	}# Negative Correlations with our <myFeatureName1_categorical> (The lower our feature value the more likely it will be a fraud transaction)\
{\listtext	\uc0\u8259 	}sns.boxplot(x=\'93<myFeatureName1_categorical>\'94, y="<myFeatureName2_continuous>", data=<myDataFrameName1>, palette=colors, ax=axes[0])\
{\listtext	\uc0\u8259 	}axes[0].set_title("<myFeatureName2_continuous> vs <myFeatureName1_categorical> Negative Correlation")\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}sns.boxplot(x="<myFeatureName1_categorical>", y="<myFeatureName3_continuous>", data=<myDataFrameName1>, palette=colors, ax=axes[1])\
{\listtext	\uc0\u8259 	}axes[1].set_title("<myFeatureName3_continuous> vs <myFeatureName1_categorical> Negative Correlation")\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}sns.boxplot(x="<myFeatureName1_categorical>", y="<myFeatureName4_continuous>", data=<myDataFrameName1>, palette=colors, ax=axes[2])\
{\listtext	\uc0\u8259 	}axes[2].set_title("<myFeatureName4_continuous> vs <myFeatureName1_categorical> Negative Correlation")\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}sns.boxplot(x="<myFeatureName1_categorical>", y="<myFeatureName5_continuous>", data=<myDataFrameName1>, palette=colors, ax=axes[3])\
{\listtext	\uc0\u8259 	}axes[3].set_title("<myFeatureName5_continuous> vs <myFeatureName1_categorical> Negative Correlation")\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}plt.show()\
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls80\ilvl1\cf0 {\listtext	\uc0\u8259 	}Histograms \
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls80\ilvl2\cf0 {\listtext	\uc0\u8259 	}{\field{\*\fldinst{HYPERLINK "https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0"}}{\fldrslt https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0}}\
{\listtext	\uc0\u8259 	}Example 1  # a quick way to generate a histogram within Pandas    (plot)\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls80\ilvl3\cf0 {\listtext	\uc0\u8259 	}<myDataFrameName1>.<myFeatureName1_continuous>.plot(kind="hist")            \
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls80\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2   # with seaborn, good for continuous variables (distplot)\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls80\ilvl3\cf0 {\listtext	\uc0\u8259 	}sns.distplot(<myDataFrameName1>.<myFeatureName1_continuous>)       # this plots a histogram plot along with the kernel density shape\
{\listtext	\uc0\u8259 	}sns.distplot(<myDataFrameName1>.<myFeatureName1_continuous>, kde=False)       # plots only a histogram\
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls80\ilvl1\cf0 {\listtext	\uc0\u8259 	}Scatter Plot (with Linear fits)\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls80\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1 # using seaborn, this shows a scatter plot and applies a linear fit\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls80\ilvl3\cf0 {\listtext	\uc0\u8259 	}sns.lmplot(x=\'93<myFeatureName1_continuous>\'94, y=\'93<myFeatureName2_continuous>\'94, data=<myDataFrameName1>)      # note, sns.lmplot returns back an entire figure whereas sns.regplot can be used for subplotting/in axes form (still a little confused, but overall I think sns.regplot is typically used when subplotting)\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls80\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2 # using seaborn, this shows the data as a scatter plot and colors the scattered data based on categories in myFeatureName3 (note, there are two linear fits applied to this plot if <myFeatureName3> is binary). \
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls80\ilvl3\cf0 {\listtext	\uc0\u8259 	}sns.lmplot(x=\'93<myFeatureName1_continuous>\'94, y=\'93<myFeatureName2_continuous>\'94, data=<myDataFrameName1>, hue=\'93<myFeatureName3_categorical>\'94)\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls80\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 3 # using seaborn, this shows the data as a scatter plot and colors the scattered data based on categories in myFeatureName3 (note, there are two linear fits applied to this plot if <myFeatureName3> is binary).  The addition of the col argument <myFeatureName4> separates the data into two plots. This categorical var must match the categorical var in <myFeatureName3>\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls80\ilvl3\cf0 {\listtext	\uc0\u8259 	}sns.lmplot(x=\'93<myFeatureName1_continuous>\'94, y=\'93<myFeatureName2_continuous>\'94, data=<myDataFrameName1>, hue=\'93<myFeatureName3_categorical>\'94, col=\'93<myFeatureName4_categorical>\'94)\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls80\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 4 # similar to above, but including the row argument turns this into a FacetGrid type plot (think of an 8x2 grid, etc). This is a great way to plot using multiple vars. Note <myFeatureName3>, <myFeatureName4>, <myFeatureName5> can be binary, binary, multi categorical, respectively. \
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls80\ilvl3\cf0 {\listtext	\uc0\u8259 	}sns.lmplot(x=\'93<myFeatureName1_continuous>\'94, y=\'93<myFeatureName2_continuous>\'94, data=<myDataFrameName1>, hue=\'93<myFeatureName3_categorical>\'94, col=\'93<myFeatureName4_categorical>\'94, row=\'93<myFeatureName5_categorical>\'94)\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls80\ilvl2\cf0 {\listtext	\uc0\u8259 	}Exampe 5 \
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls80\ilvl3\cf0 {\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls80\ilvl1\cf0 {\listtext	\uc0\u8259 	}FacetGrid / Subplotting\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls80\ilvl2\cf0 {\listtext	\uc0\u8259 	}Scatter Plot\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls80\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls80\ilvl4\cf0 {\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}<someVar1> = sns.FacetGrid(<myDataFrameName1>, col="<myFeatureName1_categorical>", row=\'93<myFeatureName2_categorical>\'94, hue="<myFeatureName3_categorical>\'94)\
{\listtext	\uc0\u8259 	}<someVar1>.map(plt.scatter, \'93<myFeatureName4_continuous>\'94, \'93<myFeatureName5_continuous>\'94)       #think of "map()" similar to how "apply()" works. Here, map() provides/applies the data to each subplot in the facet grid\
\pard\tx1660\tx2160\tx2880\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls80\ilvl2\cf0 {\listtext	\uc0\u8259 	}Histogram and Scatter Plot (with linear fit)\
\pard\tx2380\tx2880\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls80\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls80\ilvl4\cf0 {\listtext	\uc0\u8259 	}fig, (ax1, ax2) = plt.subplots(1,2)\
{\listtext	\uc0\u8259 	}sns.distplot(<myDataFrameName1>.<myFeatureName1_continuous>, ax=ax1) \
{\listtext	\uc0\u8259 	}sns.regplot(x="<myFeatureName2_continuous>",y="<myFeatureName1_continuous>", data=<myDataFrameName1>, ax=ax2)    # note, this seems very similar to sns.lmplot\
{\listtext	\uc0\u8259 	}\
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls80\ilvl1\cf0 {\listtext	\uc0\u8259 	}Bar Graph / Count Plot\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls80\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 1 # a quick way to generate a bar plot within Pandas\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls80\ilvl3\cf0 {\listtext	\uc0\u8259 	}cts = <myDataFrameName1>.<myFeatureName1_categorical>.value_counts\
{\listtext	\uc0\u8259 	}cts.plot(kind=\'93bar\'94)\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls80\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 2 # with seaborn\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls80\ilvl3\cf0 {\listtext	\uc0\u8259 	}sns.countplot(x=\'93<myFeatureName1_categorical>\'94, data = <myDataFrameName1>)\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls80\ilvl2\cf0 {\listtext	\uc0\u8259 	}Example 3 # FacetGrid type with seaborn\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls80\ilvl3\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}n_rows = <myIntOfRows>\
{\listtext	\uc0\u8259 	}n_cols = <myIntOfColumns>\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(n_rows):\
{\listtext	\uc0\u8259 	}    fg, ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(16,8))\
{\listtext	\uc0\u8259 	}    for j in range(n_cols):\
{\listtext	\uc0\u8259 	}        sns.countplot(x=cols[i*n_cols+j], data=<myDataFrameName1_categorical>, ax=ax[j])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
\pard\tx940\tx1440\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li1440\fi-1440\pardirnatural\partightenfactor0
\ls80\ilvl1\cf0 {\listtext	\uc0\u8259 	}Modeling\
\pard\tx1660\tx2160\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls80\ilvl2\cf0 {\listtext	\uc0\u8259 	}Regression\
\pard\tx2380\tx2880\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls80\ilvl3\cf0 {\listtext	\uc0\u8259 	}Linear \
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls80\ilvl4\cf0 {\listtext	\uc0\u8259 	}Linear Regression\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls80\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls80\ilvl6\cf0 {\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}from sklearn.linear_model import LinearRegression\
{\listtext	\uc0\u8259 	}<myDataFrameName1> = sns.load_dataset("tips") \
{\listtext	\uc0\u8259 	}<myDataFrameName1>.head() \
{\listtext	\uc0\u8259 	}<myModel1> = LinearRegression() \
{\listtext	\uc0\u8259 	}<myModel1>.fit(X=<myDataFrameName1>[["<myFeatureName1>","<myFeatureName2>"]], y=<myDataFrameName1>["<myFeatureName3>"]) \
{\listtext	\uc0\u8259 	}<myModel1>.coef_  \
{\listtext	\uc0\u8259 	}<myModel1>.intercept_ \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# LEARNING:\
{\listtext	\uc0\u8259 	}# For every one dollar increase in total_bill, given that\
{\listtext	\uc0\u8259 	}# the size remains the same, the tip increases by 9 cents.\
{\listtext	\uc0\u8259 	}\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls80\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 2\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls80\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}	\
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	} r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# LINEAR REGRESSION (Linear Algo) \
{\listtext	\uc0\u8259 	}from sklearn.linear_model import LinearRegression\
{\listtext	\uc0\u8259 	}lin_reg = LinearRegression(n_jobs=-1) # using all processors \
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls81\ilvl6\cf0 {\listtext	\uc0\u8259 	}algo = "LR" \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}for name, i_cols_list in X_all: \
{\listtext	\uc0\u8259 	}    print(name)\
{\listtext	\uc0\u8259 	}    lin_reg.fit(X_train[:, i_cols_list], y_train) #fitting all features to the target column\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls82\ilvl6\cf0 {\listtext	\uc0\u8259 	}    result = mean_absolute_error(np.expm1(y_test), np.expm1(lin_reg.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}    mae.append(result)\
{\listtext	\uc0\u8259 	}    print(name + " %s" % result) \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}comb.append(algo) \
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls83\ilvl6\cf0 {\listtext	\uc0\u8259 	}print(comb) \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# #Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}# fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}# plt.plot(mae)\
{\listtext	\uc0\u8259 	}# #Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}# ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}# ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}# #Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}# plt.show() \
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls84\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls85\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\pard\tx3100\tx3600\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls86\ilvl4\cf0 {\listtext	\uc0\u8259 	}Logistic Regression\
\pard\tx3820\tx4320\tx4480\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls86\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls86\ilvl6\cf0 {\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}from sklearn.linear_model import LogisticRegression\
{\listtext	\uc0\u8259 	}<myDataFrameName1> = sns.load_dataset("titanic")\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.head()\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.info()\
{\listtext	\uc0\u8259 	}<myDataFrameName1>.drop([\'93<myFeatureName1>\'94], axis=1).head()\
{\listtext	\uc0\u8259 	}<myDataFrameName2> = <myDataFrameName1>[["<myFeatureName2>", "<myFeatureName3>", "<myFeatureName4>"]] \
{\listtext	\uc0\u8259 	}<myDataFrameName2>.head()\
{\listtext	\uc0\u8259 	}<myDataFrameName3> = pd.get_dummies(<myDataFrameName2>, drop_first=True)\
{\listtext	\uc0\u8259 	}<myDataFrameName3>.head()\
{\listtext	\uc0\u8259 	}<myXTrain1> = <myDataFrameName3>.iloc[:,1:]\
{\listtext	\uc0\u8259 	}<myXTrain1>.head()\
{\listtext	\uc0\u8259 	}<myYTrain1> = <myDataFrameName3>.iloc[:,0]\
{\listtext	\uc0\u8259 	}<myYTrain1>.head()\
{\listtext	\uc0\u8259 	}<myModel1> = LogisticRegression(random_state=0, solver="lbfgs", multi_class="multinomial") \
{\listtext	\uc0\u8259 	}<myModel1>.fit(<myXTrain1>, <myYTrain1>)\
{\listtext	\uc0\u8259 	}<myModel1>.coef_\
{\listtext	\uc0\u8259 	} \
\pard\tx2380\tx2880\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls86\ilvl3\cf0 {\listtext	\uc0\u8259 	}Non-Linear\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls86\ilvl4\cf0 {\listtext	\uc0\u8259 	}KNN\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls86\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls86\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	} r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# KNN (Non-linear Algo)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Evaluation of various combinations of KNN\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Fitting Classifier to the Training set\
{\listtext	\uc0\u8259 	}from sklearn.neighbors import KNeighborsRegressor     \
{\listtext	\uc0\u8259 	}# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Add the N value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}n_list = np.array([]) #note, when the list is empty, the algo doesnt run\
{\listtext	\uc0\u8259 	}'''\
{\listtext	\uc0\u8259 	}With n_list = np.array([5])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}All 1434.788795356784\
{\listtext	\uc0\u8259 	}['LR', 'KNN 5']\
{\listtext	\uc0\u8259 	}'''\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}'''\
{\listtext	\uc0\u8259 	}With n_list = np.array([2])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}All 1526.7442802258656\
{\listtext	\uc0\u8259 	}['LR', 'KNN 2']\
{\listtext	\uc0\u8259 	}'''\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# we can use multiple values into n_list if we want to search for the optimal n_neighbors. However, xgboost is usally the best for parameter tuning.\
{\listtext	\uc0\u8259 	}for n_neighbors in n_list:\
{\listtext	\uc0\u8259 	}    # Setting the base model\
{\listtext	\uc0\u8259 	}    regressor = KNeighborsRegressor(n_neighbors=n_neighbors,n_jobs=-1)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "KNN"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name, i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        regressor.fit(X_train[:, i_cols_list], y_train) #fitting all features to the target column\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(regressor.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % n_neighbors)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}print(comb)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# since we know the outcome, we can skip the algorithm and append the result\
{\listtext	\uc0\u8259 	}if (len(n_list)==0):\
{\listtext	\uc0\u8259 	}    mae.append(1527)\
{\listtext	\uc0\u8259 	}    comb.append("KNN" + " %s" % 2 )\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Set figure size, this figure compares mae for all of the algorithms ran\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Very high computation time\
{\listtext	\uc0\u8259 	}#Best estimated performance is 1745 for n=1\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# LEARNING:\
{\listtext	\uc0\u8259 	}# KNN 5 performed the best. Lowest MAE. \
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls86\ilvl4\cf0 {\listtext	\uc0\u8259 	}Decision Tree (CART)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls86\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls86\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	} r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# CART (Non-linear Algo)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	} #Evaluation of various combinations of CART\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import the library\
{\listtext	\uc0\u8259 	}from sklearn.tree import DecisionTreeRegressor\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Add the max_depth value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}d_list = np.array([])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for max_depth in d_list:\
{\listtext	\uc0\u8259 	}    #Set the base model\
{\listtext	\uc0\u8259 	}    model = DecisionTreeRegressor(max_depth=max_depth,random_state=seed)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "CART"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % max_depth )\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}# since we know the outcome, we can skip the algorithm and append the result\
{\listtext	\uc0\u8259 	}if (len(d_list)==0):\
{\listtext	\uc0\u8259 	}    mae.append(1741)\
{\listtext	\uc0\u8259 	}    comb.append("CART" + " %s" % 5 )    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#High computation time\
{\listtext	\uc0\u8259 	}#Best estimated performance is 1741 for depth=5 \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls86\ilvl4\cf0 {\listtext	\uc0\u8259 	}Support Vector Machine (SVM, SVR)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls86\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1  \
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls86\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	} r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	} # SVM (Non-linear Algo)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import the library\
{\listtext	\uc0\u8259 	}from sklearn.svm import SVR\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Add the C value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}c_list = np.array([])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for C in c_list:\
{\listtext	\uc0\u8259 	}    #Set the base model\
{\listtext	\uc0\u8259 	}    model = SVR(C=C)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "SVM"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % C )\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#very very high computation time, not running\
{\listtext	\uc0\u8259 	} \
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls86\ilvl4\cf0 {\listtext	\uc0\u8259 	}Bagged Decision Trees (Bagging)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls86\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls86\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	}r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} # Bagged Decision Trees (Bagging)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Evaluation of various combinations of Bagged Decision Trees\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import the library\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import BaggingRegressor\
{\listtext	\uc0\u8259 	}#from sklearn.tree import DecisionTreeRegressor\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Add the n_estimators value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}n_list = np.array([])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for n_estimators in n_list:\
{\listtext	\uc0\u8259 	}    #Setting the base model\
{\listtext	\uc0\u8259 	}    model = BaggingRegressor(n_jobs=-1,n_estimators=n_estimators)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "Bag"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % n_estimators )\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#very high computation time, not running\
{\listtext	\uc0\u8259 	} \
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls86\ilvl4\cf0 {\listtext	\uc0\u8259 	}Random Forest (Bagging) # note, we use bagging to find the sweet spot between a simple and  complex model (see bias vs variance tradeoff)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls86\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls86\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	} r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Random Forest (Bagging)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Evaluation of various combinations of RandomForest\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import the library\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import RandomForestRegressor\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Add the n_estimators value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}n_list = np.array([])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for n_estimators in n_list:\
{\listtext	\uc0\u8259 	}    #Set the base model\
{\listtext	\uc0\u8259 	}    model = RandomForestRegressor(n_jobs=-1,n_estimators=n_estimators,random_state=seed)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "RF"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_testl[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % n_estimators )\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}# since we know the outcome, we can skip the algorithm and append the result\
{\listtext	\uc0\u8259 	}if (len(n_list)==0):\
{\listtext	\uc0\u8259 	}    mae.append(1213)\
{\listtext	\uc0\u8259 	}    comb.append("RF" + " %s" % 50 )    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Best estimated performance is 1213 when the number of estimators is 50\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls86\ilvl4\cf0 {\listtext	\uc0\u8259 	}Extra Trees (Bagging) # note, we use bagging to find the sweet spot between a simple and  complex model (see bias vs variance tradeoff)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls86\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls86\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}  \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	}r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	}  \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Extra Trees (Bagging)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Evaluation of various combinations of ExtraTrees\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import the library\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import ExtraTreesRegressor\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Add the n_estimators value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}n_list = np.array([])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for n_estimators in n_list:\
{\listtext	\uc0\u8259 	}    #Set the base model\
{\listtext	\uc0\u8259 	}    model = ExtraTreesRegressor(n_jobs=-1,n_estimators=n_estimators,random_state=seed)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "ET"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % n_estimators )\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}# since we know the outcome, we can skip the algorithm and append the result\
{\listtext	\uc0\u8259 	}if (len(n_list)==0):\
{\listtext	\uc0\u8259 	}    mae.append(1254)\
{\listtext	\uc0\u8259 	}    comb.append("ET" + " %s" % 100 )    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Best estimated performance is 1254 for 100 estimators \
{\listtext	\uc0\u8259 	}\
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls86\ilvl4\cf0 {\listtext	\uc0\u8259 	}Adaboost # note, we use bagging to find the sweet spot between a simple and complex model (see bias vs variance tradeoff)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls86\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls86\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	}r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Evaluation of various combinations of AdaBoost\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import the library\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import AdaBoostRegressor\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Add the n_estimators value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}n_list = np.array([])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for n_estimators in n_list:\
{\listtext	\uc0\u8259 	}    #Set the base model\
{\listtext	\uc0\u8259 	}    model = AdaBoostRegressor(n_estimators=n_estimators,random_state=seed)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "Ada"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % n_estimators )\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}# since we know the outcome, we can skip the algorithm and append the result\
{\listtext	\uc0\u8259 	}if (len(n_list)==0):\
{\listtext	\uc0\u8259 	}    mae.append(1678)\
{\listtext	\uc0\u8259 	}    comb.append("Ada" + " %s" % 100 )    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Best estimated performance is 1678 with n=100 \
{\listtext	\uc0\u8259 	} \
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls86\ilvl4\cf0 {\listtext	\uc0\u8259 	}Stochastic Gradient Boosting (Boosting) # note, we use bagging to find the sweet spot between a simple and  complex model (see bias vs variance tradeoff)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls86\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls86\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	}r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Evaluation of various combinations of SGB\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import the library\
{\listtext	\uc0\u8259 	}from sklearn.ensemble import GradientBoostingRegressor\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Add the n_estimators value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}n_list = np.array([])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for n_estimators in n_list:\
{\listtext	\uc0\u8259 	}    #Set the base model\
{\listtext	\uc0\u8259 	}    model = GradientBoostingRegressor(n_estimators=n_estimators,random_state=seed)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "SGB"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_testl[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % n_estimators )\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# since we know the outcome, we can skip the algorithm and append the result\
{\listtext	\uc0\u8259 	}if (len(n_list)==0):\
{\listtext	\uc0\u8259 	}    mae.append(1278)\
{\listtext	\uc0\u8259 	}    comb.append("SGB" + " %s" % 50 )    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Best estimated performance is ?\
{\listtext	\uc0\u8259 	} \
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls86\ilvl4\cf0 {\listtext	\uc0\u8259 	}XGBoost # note, we use bagging to find the sweet spot between a simple and  complex model (see bias vs variance tradeoff)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls86\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls86\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} # Getting the number of rows and columns\
{\listtext	\uc0\u8259 	}r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} #XGBoost\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Evaluation of various combinations of XGB\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import the library\
{\listtext	\uc0\u8259 	}from xgboost import XGBRegressor\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Add the n_estimators value to the below list if you want to run the algo\
{\listtext	\uc0\u8259 	}n_list = np.array([])\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for n_estimators in n_list:\
{\listtext	\uc0\u8259 	}    #Set the base model\
{\listtext	\uc0\u8259 	}    model = XGBRegressor(n_estimators=n_estimators,seed=seed)\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    algo = "XGB"\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for name,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo + " %s" % n_estimators )\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# since we know the outcome, we can skip the algorithm and append the result\
{\listtext	\uc0\u8259 	}if (len(n_list)==0):\
{\listtext	\uc0\u8259 	}    mae.append(1169)\
{\listtext	\uc0\u8259 	}    comb.append("XGB" + " %s" % 1000 )    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}#plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}##Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}#fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}#plt.plot(mae)\
{\listtext	\uc0\u8259 	}##Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}#ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}#ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}##Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}#plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Best estimated performance is 1169 with n=1000\
{\listtext	\uc0\u8259 	} \
\pard\tx3100\tx3600\tx4320\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls86\ilvl4\cf0 {\listtext	\uc0\u8259 	}Multi-layer Perceptrons (Deep Learning)\
\pard\tx3820\tx4320\tx5040\tx5600\tx6160\tx6720\li4320\fi-4320\pardirnatural\partightenfactor0
\ls86\ilvl5\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx4540\tx5040\tx5600\tx6160\tx6720\li5040\fi-5040\pardirnatural\partightenfactor0
\ls86\ilvl6\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Getting the number of rows and columns\
{\listtext	\uc0\u8259 	}r, c = <myDataFrameName1>.shape\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Creating an array which has indexes of columns\
{\listtext	\uc0\u8259 	}i_cols = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}for i in range(0, c-1):\
{\listtext	\uc0\u8259 	}    i_cols.append(i)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Y is the target column, X has the rest\
{\listtext	\uc0\u8259 	}X = <myDataFrameName1>[:, 0:(c-1)]\
{\listtext	\uc0\u8259 	}y = <myDataFrameName1>[:, (c-1)] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Validation chunk size\
{\listtext	\uc0\u8259 	}val_size = 0.1\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Using a common seed in all experiments so that same chunk is used for validation \
{\listtext	\uc0\u8259 	}seed = 0  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}from sklearn.model_selection import train_test_split\
{\listtext	\uc0\u8259 	}X_train, X_test, y_train, y_test = train_test_split(  \
{\listtext	\uc0\u8259 	}     X, y, test_size=val_size, random_state=seed)\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del y \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# All features \
{\listtext	\uc0\u8259 	}X_all = []\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# List of combinations \
{\listtext	\uc0\u8259 	}comb = []\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}# Dictionary to store the Mean Absolute Error for all algorithms \
{\listtext	\uc0\u8259 	}mae = [] \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Scoring parameter \
{\listtext	\uc0\u8259 	}from sklearn.metrics import mean_absolute_error\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#Add this version of X to the list\
{\listtext	\uc0\u8259 	}n = \'93All\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}X_all.append([n, i_cols])\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print(X_all)     # A list of all the columns along with dummy vars  \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}#MLP (Deep Learning)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Evaluation of various combinations of multi-layer perceptrons\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Import libraries for deep learning\
{\listtext	\uc0\u8259 	}from keras.wrappers.scikit_learn import KerasRegressor\
{\listtext	\uc0\u8259 	}from keras.models import Sequential\
{\listtext	\uc0\u8259 	}from keras.layers import Dense\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# define baseline model\
{\listtext	\uc0\u8259 	}def baseline(v):\
{\listtext	\uc0\u8259 	}     # create model\
{\listtext	\uc0\u8259 	}     model = Sequential()\
{\listtext	\uc0\u8259 	}     model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	}     model.add(Dense(1, init='normal'))\
{\listtext	\uc0\u8259 	}     # Compile model\
{\listtext	\uc0\u8259 	}     model.compile(loss='mean_absolute_error', optimizer='adam')\
{\listtext	\uc0\u8259 	}     return model\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# define smaller model\
{\listtext	\uc0\u8259 	}def smaller(v):\
{\listtext	\uc0\u8259 	}     # create model\
{\listtext	\uc0\u8259 	}     model = Sequential()\
{\listtext	\uc0\u8259 	}     model.add(Dense(v*(c-1)/2, input_dim=v*(c-1), init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	}     model.add(Dense(1, init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	}     # Compile model\
{\listtext	\uc0\u8259 	}     model.compile(loss='mean_absolute_error', optimizer='adam')\
{\listtext	\uc0\u8259 	}     return model\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# define deeper model\
{\listtext	\uc0\u8259 	}def deeper(v):\
{\listtext	\uc0\u8259 	} # create model\
{\listtext	\uc0\u8259 	} model = Sequential()\
{\listtext	\uc0\u8259 	} model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	} model.add(Dense(v*(c-1)/2, init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	} model.add(Dense(1, init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	} # Compile model\
{\listtext	\uc0\u8259 	} model.compile(loss='mean_absolute_error', optimizer='adam')\
{\listtext	\uc0\u8259 	} return model\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Optimize using dropout and decay\
{\listtext	\uc0\u8259 	}from keras.optimizers import SGD\
{\listtext	\uc0\u8259 	}from keras.layers import Dropout\
{\listtext	\uc0\u8259 	}from keras.constraints import maxnorm\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}def dropout(v):\
{\listtext	\uc0\u8259 	}    #create model\
{\listtext	\uc0\u8259 	}    model = Sequential()\
{\listtext	\uc0\u8259 	}    model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu',W_constraint=maxnorm(3)))\
{\listtext	\uc0\u8259 	}    model.add(Dropout(0.2))\
{\listtext	\uc0\u8259 	}    model.add(Dense(v*(c-1)/2, init='normal', activation='relu', W_constraint=maxnorm(3)))\
{\listtext	\uc0\u8259 	}    model.add(Dropout(0.2))\
{\listtext	\uc0\u8259 	}    model.add(Dense(1, init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	}    # Compile model\
{\listtext	\uc0\u8259 	}    sgd = SGD(lr=0.1,momentum=0.9,decay=0.0,nesterov=False)\
{\listtext	\uc0\u8259 	}    model.compile(loss='mean_absolute_error', optimizer=sgd)\
{\listtext	\uc0\u8259 	}    return model\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# define decay model\
{\listtext	\uc0\u8259 	}def decay(v):\
{\listtext	\uc0\u8259 	}    # create model\
{\listtext	\uc0\u8259 	}    model = Sequential()\
{\listtext	\uc0\u8259 	}    model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	}    model.add(Dense(1, init='normal', activation='relu'))\
{\listtext	\uc0\u8259 	}    # Compile model\
{\listtext	\uc0\u8259 	}    sgd = SGD(lr=0.1,momentum=0.8,decay=0.01,nesterov=False)\
{\listtext	\uc0\u8259 	}    model.compile(loss='mean_absolute_error', optimizer=sgd)\
{\listtext	\uc0\u8259 	}    return model\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}est_list = []\
{\listtext	\uc0\u8259 	}#uncomment the below if you want to run the algo\
{\listtext	\uc0\u8259 	}#est_list = [('MLP',baseline),('smaller',smaller),('deeper',deeper),('dropout',dropout),('decay',decay)]\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}for name, est in est_list:\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}    algo = name\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}    #Accuracy of the model using all features\
{\listtext	\uc0\u8259 	}    for m,i_cols_list in X_all:\
{\listtext	\uc0\u8259 	}        model = KerasRegressor(build_fn=est, v=1, nb_epoch=10, verbose=0)\
{\listtext	\uc0\u8259 	}        model.fit(X_train[:,i_cols_list],Y_train)\
{\listtext	\uc0\u8259 	}        result = mean_absolute_error(np.expm1(y_test), np.expm1(model.predict(X_test[:,i_cols_list])))\
{\listtext	\uc0\u8259 	}        mae.append(result)\
{\listtext	\uc0\u8259 	}        print(name + " %s" % result)\
{\listtext	\uc0\u8259 	}        \
{\listtext	\uc0\u8259 	}    comb.append(algo )\
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}# since we know the outcome, we can skip the algorithm and append the result\
{\listtext	\uc0\u8259 	}if (len(est_list)==0):\
{\listtext	\uc0\u8259 	}    mae.append(1168)\
{\listtext	\uc0\u8259 	}    comb.append("MLP" + " baseline" )    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}    \
{\listtext	\uc0\u8259 	}print("mae--> %s" % mae)\
{\listtext	\uc0\u8259 	}print("comb--> %s" % comb)\
{\listtext	\uc0\u8259 	}##Set figure size\
{\listtext	\uc0\u8259 	}plt.rc("figure", figsize=(25, 10))\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Plot the MAE of all combinations\
{\listtext	\uc0\u8259 	}fig, ax = plt.subplots()\
{\listtext	\uc0\u8259 	}plt.plot(mae)\
{\listtext	\uc0\u8259 	}#Set the tick names to names of combinations\
{\listtext	\uc0\u8259 	}ax.set_xticks(range(len(comb)))\
{\listtext	\uc0\u8259 	}ax.set_xticklabels(comb,rotation='vertical')\
{\listtext	\uc0\u8259 	}#Plot the accuracy for all combinations\
{\listtext	\uc0\u8259 	}plt.show()    \
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Best estimated performance is MLP=1168\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx1660\tx2160\tx2880\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2160\fi-2160\pardirnatural\partightenfactor0
\ls86\ilvl2\cf0 {\listtext	\uc0\u8259 	}Classification\
{\listtext	\uc0\u8259 	}Model Selection\
\pard\tx2380\tx2880\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li2880\fi-2880\pardirnatural\partightenfactor0
\ls86\ilvl3\cf0 {\listtext	\uc0\u8259 	}Example 1\
\pard\tx3100\tx3600\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li3600\fi-3600\pardirnatural\partightenfactor0
\ls86\ilvl4\cf0 {\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\'93\'94\'94\
{\listtext	\uc0\u8259 	}Since XGBRegressor is showing the best performance, we can select it as our best model. Therefore, we now need to finalize the model with all of the available data.\
{\listtext	\uc0\u8259 	}\'93\'94\'94\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# note, X_train and X_test are both coming from the training set CSV. axis=0 is stacking rows on top of one another.\
{\listtext	\uc0\u8259 	}X = np.concatenate((X_train,X_test), axis=0) \
{\listtext	\uc0\u8259 	}del X_train\
{\listtext	\uc0\u8259 	}del X_test\
{\listtext	\uc0\u8259 	}Y = np.concatenate((y_train,y_test),axis=0)\
{\listtext	\uc0\u8259 	}del y_train\
{\listtext	\uc0\u8259 	}del y_test\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print("I am here 0 - debug")\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}n_estimators = 1000\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#Best model definition\
{\listtext	\uc0\u8259 	}best_model = XGBRegressor(n_estimators=n_estimators,seed=seed)\
{\listtext	\uc0\u8259 	}print("I am here 0.0 - debug")\
{\listtext	\uc0\u8259 	}best_model.fit(X,Y)\
{\listtext	\uc0\u8259 	}print("I am here 0.1 - debug")\
{\listtext	\uc0\u8259 	}del X\
{\listtext	\uc0\u8259 	}del Y\
{\listtext	\uc0\u8259 	}#Read test dataset\
{\listtext	\uc0\u8259 	}dataset_test = pd.read_csv("test.csv")\
{\listtext	\uc0\u8259 	}print("I am here 0.2 - debug")\
{\listtext	\uc0\u8259 	}#Drop unnecessary columns\
{\listtext	\uc0\u8259 	}ID = dataset_test['id']\
{\listtext	\uc0\u8259 	}dataset_test.drop('id',axis=1,inplace=True)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}#One hot encode all categorical attributes\
{\listtext	\uc0\u8259 	}cats = []\
{\listtext	\uc0\u8259 	}print("I am here 1 - debug")\
{\listtext	\uc0\u8259 	}for i in range(0, split):\
{\listtext	\uc0\u8259 	}    # label encoding\
{\listtext	\uc0\u8259 	}    label_encoder = LabelEncoder()\
{\listtext	\uc0\u8259 	}    label_encoder.fit(labels[i])\
{\listtext	\uc0\u8259 	}    feature = label_encoder.transform(dataset_test.iloc[:,i])\
{\listtext	\uc0\u8259 	}    feature = feature.reshape(dataset_test.shape[0], 1)\
{\listtext	\uc0\u8259 	}    #One hot encoding\
{\listtext	\uc0\u8259 	}    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))\
{\listtext	\uc0\u8259 	}    feature = onehot_encoder.fit_transform(feature)\
{\listtext	\uc0\u8259 	}    cats.append(feature)\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}print("I am here 2 - debug")\
{\listtext	\uc0\u8259 	}# Making a 2D array from a list of 1D arrays\
{\listtext	\uc0\u8259 	}encoded_cats = np.column_stack(cats)\
{\listtext	\uc0\u8259 	}del cats\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Concatenating encoded attributes with continous attributes\
{\listtext	\uc0\u8259 	}X_test = np.concatenate((encoded_cats, dataset_test.iloc[:,split:].values), axis=1)\
{\listtext	\uc0\u8259 	}print("I am here 3 - debug")\
{\listtext	\uc0\u8259 	}del encoded_cats\
{\listtext	\uc0\u8259 	}del dataset_test\
{\listtext	\uc0\u8259 	}\
{\listtext	\uc0\u8259 	}# Making predictions using the best model now\
{\listtext	\uc0\u8259 	}predictions = np.expm1(best_model.predict(X_test))\
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	} \
{\listtext	\uc0\u8259 	}\
\pard\tx720\tx1440\tx2160\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2 \cf2 \expnd0\expndtw0\kerning0
\ul \ulc2 \
}